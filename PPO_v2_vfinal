
import os
import random
import numpy as np
import matplotlib.pyplot as plt
import imageio.v2 as imageio
import time

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.distributions import Categorical

from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import dijkstra

from env_midterm import Environment

# ============================================================
# Reproducibility
# ============================================================

def set_global_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ============================================================
# Action masks
# ============================================================

def valid_action_mask(env, n_actions=5):
    mask = np.zeros(n_actions, dtype=np.bool_)
    mask[0] = True  # idle always allowed

    r, c = env.agent_loc
    H, W = env.vertical_cell_count, env.horizontal_cell_count

    moves = [
        (1, -1, 0),  # up
        (2, 0, 1),   # right
        (3, 1, 0),   # down
        (4, 0, -1),  # left
    ]
    for act, dr, dc in moves:
        nr, nc = r + dr, c + dc
        if 0 <= nr < H and 0 <= nc < W and (nr, nc) in env.eligible_cells:
            mask[act] = True
    return mask

def valid_action_mask_from_obs_batch(obs_batch, n_actions=5):
    """
    obs_batch: torch tensor [B, obs_dim] with channels-first encoding (C,H,W) flattened.
    Uses:
      channel 0: agent position one-hot
      channel 6: eligible mask
    Returns: torch.bool tensor [B, n_actions]
    """
    device = obs_batch.device
    B = obs_batch.shape[0]

    grid = obs_batch.view(B, CHANNELS, HEIGHT, WIDTH)
    agent = grid[:, 0]                 # [B,H,W]
    eligible = grid[:, 6] > 0.5        # [B,H,W] bool

    mask = torch.zeros((B, n_actions), dtype=torch.bool, device=device)
    mask[:, 0] = True  # idle always valid

    pos = agent.view(B, -1).argmax(dim=1)
    ar = (pos // WIDTH).long()
    ac = (pos % WIDTH).long()

    b_idx = torch.arange(B, device=device)

    # UP (1)
    ok = (ar > 0)
    if ok.any():
        b = b_idx[ok]
        r = ar[ok] - 1
        c = ac[ok]
        ok2 = eligible[b, r, c]
        mask[b[ok2], 1] = True

    # RIGHT (2)
    ok = (ac < WIDTH - 1)
    if ok.any():
        b = b_idx[ok]
        r = ar[ok]
        c = ac[ok] + 1
        ok2 = eligible[b, r, c]
        mask[b[ok2], 2] = True

    # DOWN (3)
    ok = (ar < HEIGHT - 1)
    if ok.any():
        b = b_idx[ok]
        r = ar[ok] + 1
        c = ac[ok]
        ok2 = eligible[b, r, c]
        mask[b[ok2], 3] = True

    # LEFT (4)
    ok = (ac > 0)
    if ok.any():
        b = b_idx[ok]
        r = ar[ok]
        c = ac[ok] - 1
        ok2 = eligible[b, r, c]
        mask[b[ok2], 4] = True

    return mask

# ============================================================
# CNN SHAPE
# ============================================================

CHANNELS = 10
HEIGHT = 5
WIDTH = 5

# ============================================================
# Distance structures (robust mapping for v2)
# ============================================================

def _build_distance_structs(env):
    """
    Builds:
      env._dist_matrix              [N,N] shortest path distances among eligible nodes
      env._dist_from_idx_grids      list of grids: for each node idx, grid[r,c]=dist(idx->cell) (only eligible filled)
      env._dist_base_grid           grid distances to base
      env._max_dist
      env._mapping, env._coord_to_idx
    """
    if hasattr(env, "_dist_ready") and env._dist_ready:
        return

    H, W = 5, 5
    variant = env.variant

    if variant in (0, 1):
        # Full 5x5 grid
        neighbor_matrix = np.zeros((25, 25), dtype=int)
        for i in range(25):
            for j in range(i + 1, 25):
                i_r, i_c = i // 5, i % 5
                j_r, j_c = j // 5, j % 5
                if (j_r - i_r == 0 and j_c - i_c == 1) or (j_r - i_r == 1 and j_c - i_c == 0):
                    neighbor_matrix[i, j] = 1
                    neighbor_matrix[j, i] = 1

        graph = csr_matrix(neighbor_matrix)
        dist_matrix, _ = dijkstra(csgraph=graph, directed=False, return_predecessors=True, unweighted=True)

        env._mapping = None
        env._coord_to_idx = None
        env._dist_matrix = dist_matrix

        dist_grids = []
        for idx in range(25):
            grid = np.zeros((H, W), dtype=np.float32)
            for j in range(25):
                r = j // 5
                c = j % 5
                grid[r, c] = dist_matrix[idx, j]
            dist_grids.append(grid)
        env._dist_from_idx_grids = dist_grids

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        idx_base = base_coord[0] * 5 + base_coord[1]
        env._dist_base_grid = dist_grids[idx_base]

        finite = dist_matrix[dist_matrix < np.inf]
        env._max_dist = float(np.max(finite)) if finite.size > 0 else 1.0

    else:
        # Variant 2: derive mapping from env.eligible_cells to avoid mismatches
        mapping = sorted(list(env.eligible_cells))  # list of (r,c)
        env._mapping = mapping
        env._coord_to_idx = {coord: i for i, coord in enumerate(mapping)}

        n = len(mapping)
        neighbor_matrix = np.zeros((n, n), dtype=int)

        def link(a, b):
            neighbor_matrix[a, b] = 1
            neighbor_matrix[b, a] = 1

        # connect manhattan-adjacent eligible cells
        for i, (r1, c1) in enumerate(mapping):
            for j in range(i + 1, n):
                r2, c2 = mapping[j]
                if abs(r1 - r2) + abs(c1 - c2) == 1:
                    link(i, j)

        graph = csr_matrix(neighbor_matrix)
        dist_matrix, _ = dijkstra(csgraph=graph, directed=False, return_predecessors=True, unweighted=True)
        env._dist_matrix = dist_matrix

        dist_grids = []
        for idx in range(n):
            grid = np.zeros((H, W), dtype=np.float32)
            for j, (r, c) in enumerate(mapping):
                grid[r, c] = dist_matrix[idx, j]
            dist_grids.append(grid)
        env._dist_from_idx_grids = dist_grids

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        idx_base = env._coord_to_idx[base_coord]
        env._dist_base_grid = dist_grids[idx_base]

        finite = dist_matrix[dist_matrix < np.inf]
        env._max_dist = float(np.max(finite)) if finite.size > 0 else 1.0

    env._dist_ready = True

def _coord_to_idx(env, coord):
    if env.variant in (0, 1):
        return coord[0] * 5 + coord[1]
    return env._coord_to_idx[coord]

# ============================================================
# Observation encoding (Phi)
# ============================================================

def encode_obs(obs, env, feature_mode="full"):
    step, agent_loc, agent_load, item_locs, item_times = obs
    _build_distance_structs(env)

    H, W, C = HEIGHT, WIDTH, CHANNELS
    state = np.zeros((C, H, W), dtype=np.float32)

    eligible_set = set(env.eligible_cells)

    # 6: eligible mask
    for r in range(H):
        for c in range(W):
            state[6, r, c] = 1.0 if (r, c) in eligible_set else 0.0

    # 0: agent position
    ar, ac = agent_loc
    state[0, ar, ac] = 1.0

    # 1,2,5: items
    for (loc, age) in zip(item_locs, item_times):
        r, c = loc
        state[1, r, c] = 1.0
        state[2, r, c] = age / float(env.max_response_time)
        state[5, r, c] = (env.max_response_time - age) / float(env.max_response_time)

    # 3/4: distances normalized
    agent_idx = _coord_to_idx(env, agent_loc)
    dist_agent_grid = env._dist_from_idx_grids[agent_idx]

    max_dist = float(getattr(env, "_max_dist", 1.0))
    if max_dist <= 0:
        max_dist = 1.0

    state[3, :, :] = dist_agent_grid / max_dist
    state[4, :, :] = env._dist_base_grid / max_dist

    # 7: step norm
    state[7, :, :] = step / float(env.episode_steps)

    # 8: load norm
    state[8, :, :] = agent_load / float(env.agent_capacity)

    # 9: Phi plane
    phi = np.zeros((H, W), dtype=np.float32)

    if agent_load > 0:
        base_pref = 1.0 - (env._dist_base_grid / max_dist)  # base=1
        phi = np.clip(1.8 * base_pref - 1.0, -1.0, 1.0)
    else:
        if len(item_locs) > 0:
            best = np.full((H, W), -np.inf, dtype=np.float32)

            base_coord = env.target_loc
            base_idx = _coord_to_idx(env, base_coord)

            ALPHA = 1.0   # cell to item
            BETA  = 1.6   # time to live (TTL)
            KAPPA = 1.0   # item to base

            for (loc, age) in zip(item_locs, item_times):
                ttl = (env.max_response_time - age) / float(env.max_response_time)
                item_idx = _coord_to_idx(env, loc)

                dist_to_item_grid = env._dist_from_idx_grids[item_idx] / max_dist
                d_item_base = float(env._dist_matrix[item_idx, base_idx]) / max_dist

                cand = (BETA * ttl) - (ALPHA * dist_to_item_grid) - (KAPPA * d_item_base)
                best = np.maximum(best, cand.astype(np.float32))

            phi = np.clip(best, -1.0, 1.0)
        else:
            phi[:, :] = 0.0

    # zero-out holes
    for r in range(H):
        for c in range(W):
            if (r, c) not in eligible_set:
                phi[r, c] = 0.0

    state[9, :, :] = phi

    # feature_mode toggles
    fm = feature_mode
    if fm == "full":
        pass
    elif fm == "reduced_full":
        state[2, :, :] = 0.0
        state[7, :, :] = 0.0
    elif fm == "no_distance":
        state[3, :, :] = 0.0
        state[4, :, :] = 0.0
    elif fm == "expiry_focused":
        state[2, :, :] = 0.0
        threshold = 4
        for (loc, age) in zip(item_locs, item_times):
            r, c = loc
            if env.max_response_time - age <= threshold:
                state[2, r, c] = 1.0
    elif fm == "minimal":
        keep = {0, 1, 5, 8}
        for ch in range(C):
            if ch not in keep:
                state[ch, :, :] = 0.0
    else:
        raise ValueError(f"Invalid feature_mode: {fm}")

    return state.reshape(-1)

# ============================================================
# Networks
# ============================================================

class PolicyNet(nn.Module):
    def __init__(self, obs_dim, n_actions, hidden_dim=256):
        super().__init__()
        assert obs_dim == CHANNELS * HEIGHT * WIDTH

        self.conv1 = nn.Conv2d(CHANNELS, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)

        conv_out_dim = 64 * HEIGHT * WIDTH

        self.fc1 = nn.Linear(conv_out_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        self.logits = nn.Linear(hidden_dim, n_actions)

    def forward(self, x):
        x = x.view(-1, CHANNELS, HEIGHT, WIDTH)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.ln1(self.fc1(x)))
        x = F.relu(self.ln2(self.fc2(x)))
        return self.logits(x)

class ValueNet(nn.Module):
    def __init__(self, obs_dim, hidden_dim=256):
        super().__init__()
        assert obs_dim == CHANNELS * HEIGHT * WIDTH

        self.conv1 = nn.Conv2d(CHANNELS, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)

        conv_out_dim = 64 * HEIGHT * WIDTH

        self.fc1 = nn.Linear(conv_out_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        self.v = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = x.view(-1, CHANNELS, HEIGHT, WIDTH)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.ln1(self.fc1(x)))
        x = F.relu(self.ln2(self.fc2(x)))
        return self.v(x).squeeze(-1)

# ============================================================
# PPO Buffer
# ============================================================

class PPOBuffer:
    def __init__(self, obs_dim, device):
        self.obs_dim = obs_dim
        self.device = device
        self.reset()

    def reset(self):
        self.obs = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.log_probs = []
        self.values = []
        self.next_values = []

    def store(self, obs, action, reward, done, log_prob, value, next_value):
        self.obs.append(np.array(obs, dtype=np.float32))
        self.actions.append(int(action))
        self.rewards.append(float(reward))
        self.dones.append(float(done))
        self.log_probs.append(float(log_prob))
        self.values.append(float(value))
        self.next_values.append(float(next_value))

    def compute_advantages(self, gamma, lam, reward_norm=True):
        rewards = torch.tensor(self.rewards, dtype=torch.float32, device=self.device)
        values = torch.tensor(self.values, dtype=torch.float32, device=self.device)
        next_values = torch.tensor(self.next_values, dtype=torch.float32, device=self.device)
        dones = torch.tensor(self.dones, dtype=torch.float32, device=self.device)

        if reward_norm and rewards.numel() > 1:
            r_std = rewards.std(unbiased=False) + 1e-8
            rewards = (rewards - rewards.mean()) / r_std

        T = rewards.shape[0]
        advantages = torch.zeros(T, dtype=torch.float32, device=self.device)

        gae = 0.0
        for t in reversed(range(T)):
            if dones[t] > 0.5:
                gae = 0.0
            delta = rewards[t] + gamma * next_values[t] - values[t]
            gae = delta + gamma * lam * gae
            advantages[t] = gae

        self.advantages = advantages
        self.returns = advantages + values

    def get(self):
        obs = torch.as_tensor(np.asarray(self.obs, dtype=np.float32), device=self.device)
        actions = torch.as_tensor(np.asarray(self.actions, dtype=np.int64), device=self.device)
        old_log_probs = torch.as_tensor(np.asarray(self.log_probs, dtype=np.float32), device=self.device)
        old_values = torch.as_tensor(np.asarray(self.values, dtype=np.float32), device=self.device)
        return obs, actions, old_log_probs, old_values, self.advantages, self.returns

# ============================================================
# PPO Agent
# ============================================================

class PPOAgent:
    def __init__(
        self,
        obs_dim,
        n_actions,
        gamma=0.99,
        lam=0.90,
        clip_ratio=0.08,
        pi_lr=3e-4,
        vf_lr=2e-4,
        train_epochs=10,
        minibatch_size=64,
        entropy_start=0.018,
        entropy_end=0.004,
        device="cpu",
        target_kl=0.015,
        max_grad_norm=0.5,
        vf_clip_ratio=0.2,
        total_updates=1,
        eps_mix=0.0,  # <<< epsilon-mix exploration probability (train only)
    ):
        self.obs_dim = obs_dim
        self.n_actions = n_actions
        self.gamma = float(gamma)
        self.lam = float(lam)
        self.clip_ratio = float(clip_ratio)
        self.train_epochs = int(train_epochs)
        self.minibatch_size = int(minibatch_size)
        self.device = torch.device(device)

        self.target_kl = float(target_kl)
        self.max_grad_norm = float(max_grad_norm)
        self.vf_clip_ratio = float(vf_clip_ratio)

        self.entropy_start = float(entropy_start)
        self.entropy_end = float(entropy_end)

        self.eps_mix = float(eps_mix)

        self.policy = PolicyNet(obs_dim, n_actions).to(self.device)
        self.value_fn = ValueNet(obs_dim).to(self.device)

        self.pi_optimizer = Adam(self.policy.parameters(), lr=float(pi_lr), eps=1e-5)
        self.vf_optimizer = Adam(self.value_fn.parameters(), lr=float(vf_lr), eps=1e-5)

        self.pi_lr_init = float(pi_lr)
        self.vf_lr_init = float(vf_lr)

        self.total_updates = max(1, int(total_updates))
        self.update_count = 0

    def set_conv_trainable(self, trainable: bool):
        """Freeze/unfreeze conv stacks for transfer learning."""
        for m in [self.policy.conv1, self.policy.conv2, self.policy.conv3,
                  self.value_fn.conv1, self.value_fn.conv2, self.value_fn.conv3]:
            for p in m.parameters():
                p.requires_grad = bool(trainable)

    def _anneal(self):
        frac = min(1.0, self.update_count / float(self.total_updates))

        lr_mult = 1.0 - 0.7 * frac
        for pg in self.pi_optimizer.param_groups:
            pg["lr"] = self.pi_lr_init * lr_mult
        for pg in self.vf_optimizer.param_groups:
            pg["lr"] = self.vf_lr_init * lr_mult

        entropy_coef = self.entropy_start + frac * (self.entropy_end - self.entropy_start)
        return float(entropy_coef)

    @torch.no_grad()
    def select_action(self, obs_vec, env=None, eval_mode=False):
        if eval_mode:
            self.policy.eval()
            self.value_fn.eval()
        else:
            self.policy.train()
            self.value_fn.train()

        obs_t = torch.as_tensor(obs_vec, dtype=torch.float32, device=self.device).unsqueeze(0)
        logits = self.policy(obs_t)

        if env is not None:
            mask_np = valid_action_mask(env, self.n_actions)
            mask_t = torch.as_tensor(mask_np, device=self.device).unsqueeze(0)
            logits = logits.masked_fill(~mask_t, -1e9)

        dist = Categorical(logits=logits)

        # --- epsilon-mix exploration (TRAIN ONLY) ---
        if (not eval_mode) and (env is not None) and (self.eps_mix > 0.0):
            if random.random() < self.eps_mix:
                valid = np.where(mask_np)[0] if 'mask_np' in locals() else np.where(valid_action_mask(env, self.n_actions))[0]
                a = int(np.random.choice(valid))
                a_t = torch.as_tensor([a], dtype=torch.int64, device=self.device)
                log_prob = dist.log_prob(a_t)
                value = self.value_fn(obs_t)
                return a, float(log_prob.item()), float(value.item())
        # -------------------------------------------

        action = torch.argmax(dist.probs, dim=-1) if eval_mode else dist.sample()
        log_prob = dist.log_prob(action)
        value = self.value_fn(obs_t)
        return int(action.item()), float(log_prob.item()), float(value.item())

    def update(self, buffer: PPOBuffer):
        self.policy.train()
        self.value_fn.train()

        entropy_coef = self._anneal()

        obs, actions, old_log_probs, old_values, advantages, returns = buffer.get()
        advantages = (advantages - advantages.mean()) / (advantages.std(unbiased=False) + 1e-8)

        N = obs.size(0)
        idxs = np.arange(N)

        for _ in range(self.train_epochs):
            np.random.shuffle(idxs)
            approx_kls = []

            for start in range(0, N, self.minibatch_size):
                mb_idx = idxs[start:start + self.minibatch_size]
                mb_idx_t = torch.as_tensor(mb_idx, dtype=torch.long, device=self.device)

                mb_obs = obs[mb_idx_t]
                mb_actions = actions[mb_idx_t]
                mb_old_logp = old_log_probs[mb_idx_t]
                mb_old_v = old_values[mb_idx_t]
                mb_adv = advantages[mb_idx_t]
                mb_returns = returns[mb_idx_t]

                logits = self.policy(mb_obs)
                mask = valid_action_mask_from_obs_batch(mb_obs, self.n_actions)
                logits = logits.masked_fill(~mask, -1e9)

                dist = Categorical(logits=logits)
                new_logp = dist.log_prob(mb_actions)
                entropy = dist.entropy().mean()

                ratio = torch.exp(new_logp - mb_old_logp)
                unclipped = ratio * mb_adv
                clipped = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * mb_adv
                policy_loss = -torch.min(unclipped, clipped).mean()

                v_pred = self.value_fn(mb_obs)
                v_pred_clipped = mb_old_v + torch.clamp(v_pred - mb_old_v, -self.vf_clip_ratio, self.vf_clip_ratio)
                v_loss_unclipped = F.smooth_l1_loss(v_pred, mb_returns)
                v_loss_clipped = F.smooth_l1_loss(v_pred_clipped, mb_returns)
                value_loss = torch.max(v_loss_unclipped, v_loss_clipped)

                self.pi_optimizer.zero_grad(set_to_none=True)
                (policy_loss - entropy_coef * entropy).backward()
                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
                self.pi_optimizer.step()

                self.vf_optimizer.zero_grad(set_to_none=True)
                value_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.value_fn.parameters(), self.max_grad_norm)
                self.vf_optimizer.step()

                approx_kls.append((mb_old_logp - new_logp).mean().item())

            if len(approx_kls) > 0 and float(np.mean(approx_kls)) > 1.5 * self.target_kl:
                break

        self.update_count += 1

# ============================================================
# Evaluation
# ============================================================

def run_eval(agent, data_dir, variant, feature_mode, mode, episodes):
    if mode not in ("validation", "testing"):
        raise ValueError(f"mode must be 'validation' or 'testing', got: {mode}")

    env = Environment(variant=variant, data_dir=data_dir)
    returns = []

    if mode == "validation":
        max_eps = min(episodes, len(env.validation_episodes))
    else:
        max_eps = min(episodes, len(env.test_episodes))

    for _ in range(max_eps):
        raw = env.reset(mode=mode)
        obs_vec = encode_obs(raw, env, feature_mode=feature_mode)

        done = False
        ep_ret = 0.0
        while not done:
            action, _, _ = agent.select_action(obs_vec, env=env, eval_mode=True)
            rew, raw_next, done_flag = env.step(action)
            obs_vec = encode_obs(raw_next, env, feature_mode=feature_mode)
            ep_ret += rew
            done = bool(done_flag)

        returns.append(ep_ret)

    return returns

def summarize_returns(name, returns):
    arr = np.asarray(returns, dtype=np.float32)
    n = len(arr)
    print(f"\n=== {name} statistics ===")
    if n == 0:
        print("No episodes recorded.")
        print("================================\n")
        return
    print(f"Episodes total:              {n}")
    print(f"Mean return:                 {float(arr.mean()):.2f} ± {float(arr.std()):.2f}")
    print(f"Best episode return:         {float(arr.max()):.2f}")
    print("================================\n")

# ============================================================
# Visualization (live)
# ============================================================

def visualize_episode(env, agent, feature_mode="full", mode="testing", max_steps=200, pause=0.06, title_prefix=""):
    raw = env.reset(mode=mode)
    obs_vec = encode_obs(raw, env, feature_mode=feature_mode)

    H, W = env.vertical_cell_count, env.horizontal_cell_count
    base = env.target_loc
    eligible = set(env.eligible_cells)

    fig, ax = plt.subplots(figsize=(5, 5))
    fig.canvas.draw()

    ax.set_xlim(-0.5, W - 0.5)
    ax.set_ylim(H - 0.5, -0.5)
    ax.set_xticks(range(W))
    ax.set_yticks(range(H))
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    ax.grid(True)

    for r in range(H):
        for c in range(W):
            if (r, c) not in eligible:
                ax.add_patch(plt.Rectangle((c - 0.5, r - 0.5), 1, 1, color="lightgray", alpha=0.85))

    ax.scatter([base[1]], [base[0]], marker="s", s=500, edgecolors="black", linewidths=1.5)
    ax.text(base[1], base[0], "BASE", ha="center", va="center", fontsize=9, weight="bold")

    agent_sc = ax.scatter([], [], marker="o", s=500, edgecolors="black", linewidths=1.5)
    items_sc = ax.scatter([], [], marker="*", s=350, edgecolors="black", linewidths=1.0)

    info = ax.text(0.02, 1.02, "", transform=ax.transAxes, ha="left", va="bottom", fontsize=10)

    total_rew = 0.0
    done = False
    steps = 0

    while (not done) and (steps < max_steps):
        step_count, agent_loc, agent_load, item_locs, item_times = env.get_obs()

        agent_sc.set_offsets(np.array([[agent_loc[1], agent_loc[0]]], dtype=np.float32))

        if len(item_locs) > 0:
            xs = [loc[1] for loc in item_locs]
            ys = [loc[0] for loc in item_locs]
            items_sc.set_offsets(np.array(list(zip(xs, ys)), dtype=np.float32))
        else:
            items_sc.set_offsets(np.zeros((0, 2), dtype=np.float32))

        action, _, _ = agent.select_action(obs_vec, env=env, eval_mode=True)
        rew, raw_next, done_flag = env.step(action)
        obs_vec = encode_obs(raw_next, env, feature_mode=feature_mode)

        total_rew += rew
        done = bool(done_flag)
        steps += 1

        act_name = {0:"IDLE", 1:"UP", 2:"RIGHT", 3:"DOWN", 4:"LEFT"}.get(action, str(action))
        if len(item_times) > 0:
            ttls = [(env.max_response_time - t) for t in item_times]
            ttl_str = f"TTL min/max: {min(ttls)}/{max(ttls)}"
        else:
            ttl_str = "TTL: -"

        info.set_text(
            f"{title_prefix}{mode.upper()} step={step_count:3d} act={act_name:>5s} "
            f"rew={rew:5.1f} total={total_rew:7.1f} load={agent_load}/{env.agent_capacity} "
            f"items={len(item_locs)} {ttl_str}"
        )

        plt.pause(pause)

    plt.show()

# ============================================================
# Animation saving (macOS Retina safe)
# ============================================================

def save_episode_animation(env, agent, feature_mode="full", mode="testing",
                           max_steps=200, out_path="episode.gif", fps=12):
    raw = env.reset(mode=mode)
    obs_vec = encode_obs(raw, env, feature_mode=feature_mode)

    H, W = env.vertical_cell_count, env.horizontal_cell_count
    base = env.target_loc
    eligible = set(env.eligible_cells)

    fig, ax = plt.subplots(figsize=(5, 5))
    fig.canvas.draw()  # important on macOS

    ax.set_xlim(-0.5, W - 0.5)
    ax.set_ylim(H - 0.5, -0.5)
    ax.set_xticks(range(W))
    ax.set_yticks(range(H))
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    ax.grid(True)

    for r in range(H):
        for c in range(W):
            if (r, c) not in eligible:
                ax.add_patch(plt.Rectangle((c - 0.5, r - 0.5), 1, 1, color="lightgray", alpha=0.8))

    ax.scatter([base[1]], [base[0]], marker="s", s=500, edgecolors="black", linewidths=1.5)
    ax.text(base[1], base[0], "BASE", ha="center", va="center", fontsize=9, weight="bold")

    agent_sc = ax.scatter([], [], marker="o", s=500, edgecolors="black", linewidths=1.5)
    items_sc = ax.scatter([], [], marker="*", s=350, edgecolors="black", linewidths=1.0)
    info = ax.text(0.02, 1.02, "", transform=ax.transAxes, ha="left", va="bottom", fontsize=10)

    frames = []
    total_rew = 0.0
    done = False
    step = 0

    def grab_frame():
        fig.canvas.draw()

        # robust HiDPI path
        try:
            rgba = np.asarray(fig.canvas.buffer_rgba())  # (H,W,4) REAL pixels
            frames.append(rgba[..., :3].copy())
            return
        except Exception:
            pass

        # fallback
        buf = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)
        w0, h0 = fig.canvas.get_width_height()
        n_pix = buf.size // 4
        scale = int(round((n_pix / (w0 * h0)) ** 0.5))
        w = w0 * max(scale, 1)
        h = h0 * max(scale, 1)
        buf = buf.reshape(h, w, 4)
        frames.append(buf[:, :, [1, 2, 3]].copy())

    while (not done) and (step < max_steps):
        step_count, agent_loc, agent_load, item_locs, item_times = env.get_obs()

        agent_sc.set_offsets(np.array([[agent_loc[1], agent_loc[0]]], dtype=np.float32))

        if len(item_locs) > 0:
            xs = [loc[1] for loc in item_locs]
            ys = [loc[0] for loc in item_locs]
            items_sc.set_offsets(np.array(list(zip(xs, ys)), dtype=np.float32))
        else:
            items_sc.set_offsets(np.zeros((0, 2), dtype=np.float32))

        action, _, _ = agent.select_action(obs_vec, env=env, eval_mode=True)
        rew, raw_next, done_flag = env.step(action)
        obs_vec = encode_obs(raw_next, env, feature_mode=feature_mode)

        total_rew += rew
        done = bool(done_flag)
        step += 1

        act_name = {0:"IDLE", 1:"UP", 2:"RIGHT", 3:"DOWN", 4:"LEFT"}.get(action, str(action))
        if len(item_times) > 0:
            ttls = [(env.max_response_time - t) for t in item_times]
            ttl_str = f"TTL min/max: {min(ttls)}/{max(ttls)}"
        else:
            ttl_str = "TTL: -"

        info.set_text(
            f"{mode.upper()} step={step_count:3d} act={act_name:>5s} "
            f"rew={rew:5.1f} total={total_rew:7.1f} load={agent_load}/{env.agent_capacity} "
            f"items={len(item_locs)} {ttl_str}"
        )

        grab_frame()

    plt.close(fig)

    out_path = str(out_path)
    if out_path.lower().endswith(".gif"):
        imageio.mimsave(out_path, frames, fps=fps)

    elif out_path.lower().endswith(".mp4"):
        try:
            imageio.mimsave(out_path, frames, fps=fps)
        except Exception as e:
            raise RuntimeError(
                "MP4 export failed (likely ffmpeg not installed). "
                "Use .gif instead, or install ffmpeg."
            ) from e
    else:
        raise ValueError("out_path must end with .gif or .mp4")

    print(f"Saved animation to: {out_path}")

# ============================================================
# Train + Val + Test (using best-val checkpoint)
# ============================================================

def train_and_test_ppo(
    data_dir,
    variant=2,
    feature_mode="full",
    train_episodes=6000,
    validation_episodes=100,
    test_episodes=100,
    gamma=0.99,
    lam=0.90,
    clip_ratio=0.08,
    pi_lr=3e-4,
    vf_lr=2e-4,
    train_epochs=10,
    minibatch_size=64,
    entropy_start=0.018,
    entropy_end=0.004,
    device="cpu",
    seed=19,
    print_interval=250,
    ckpt_dir="./checkpoints_v2_bestval",
    eval_every_updates=5,
    rollout_episodes_per_update=32,
    reward_norm_in_rollout=True,
    visualize_test=False,
    n_visualize_episodes=2,
    save_animation=False,
    eps_mix=0.0,
    init_weights_path=None,
    freeze_conv_updates=0,
):
    set_global_seed(seed)
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs("./code_data/plots", exist_ok=True)

    # Training env
    env = Environment(variant=variant, data_dir=data_dir)
    n_actions = 5

    raw0 = env.reset(mode="training")
    obs_vec0 = encode_obs(raw0, env, feature_mode=feature_mode)
    obs_dim = obs_vec0.shape[0]

    total_updates = max(1, int(np.ceil(train_episodes / float(rollout_episodes_per_update))))

    agent = PPOAgent(
        obs_dim=obs_dim,
        n_actions=n_actions,
        gamma=gamma,
        lam=lam,
        clip_ratio=clip_ratio,
        pi_lr=pi_lr,
        vf_lr=vf_lr,
        train_epochs=train_epochs,
        minibatch_size=minibatch_size,
        entropy_start=entropy_start,
        entropy_end=entropy_end,
        device=device,
        total_updates=total_updates,
        target_kl=0.015,
        max_grad_norm=0.5,
        vf_clip_ratio=0.2,
        eps_mix=eps_mix,
    )

    # Optional weight init (transfer learning)
    if init_weights_path is not None and os.path.exists(init_weights_path):
        ck = torch.load(init_weights_path, map_location=device)
        agent.policy.load_state_dict(ck["policy_state_dict"])
        agent.value_fn.load_state_dict(ck["value_state_dict"])
        print(f"[INIT] Loaded weights from: {init_weights_path}")
        if freeze_conv_updates and freeze_conv_updates > 0:
            agent.set_conv_trainable(False)
            print(f"[INIT] Froze conv layers for first {freeze_conv_updates} PPO updates")

    buffer = PPOBuffer(obs_dim=obs_dim, device=agent.device)

    train_returns = []
    best_val_mean = -np.inf
    best_ckpt_path = os.path.join(ckpt_dir, f"ppo_bestVAL_variant{variant}_seed{seed}.pt")

    updates_done = 0
    episodes_in_rollout = 0

    for ep in range(train_episodes):
        raw = env.reset(mode="training")
        obs_vec = encode_obs(raw, env, feature_mode=feature_mode)

        done = False
        ep_ret = 0.0

        while not done:
            # mask mismatch debug (throttled)
            if (ep % 20 == 0) and (env.step_count % 10 == 0):
                mask_env = valid_action_mask(env, 5)
                mask_obs = valid_action_mask_from_obs_batch(
                    torch.as_tensor(obs_vec, dtype=torch.float32).unsqueeze(0).to(agent.device),
                    5
                )[0].detach().cpu().numpy()

                if not np.array_equal(mask_env.astype(np.int32), mask_obs.astype(np.int32)):
                    print("MASK MISMATCH", "ep", ep, "t", env.step_count, "loc", env.agent_loc,
                          "env", mask_env.astype(int), "obs", mask_obs.astype(int))

            action, logp, value = agent.select_action(obs_vec, env=env, eval_mode=False)
            rew, raw_next, done_flag = env.step(action)

            obs_next_vec = encode_obs(raw_next, env, feature_mode=feature_mode)
            done_bool = bool(done_flag)

            with torch.no_grad():
                obs_next_t = torch.as_tensor(obs_next_vec, dtype=torch.float32, device=agent.device).unsqueeze(0)
                next_value = float(agent.value_fn(obs_next_t).item())

            buffer.store(obs_vec, action, rew, done_bool, logp, value, next_value)

            obs_vec = obs_next_vec
            ep_ret += rew
            done = done_bool

        train_returns.append(ep_ret)
        episodes_in_rollout += 1

        if (episodes_in_rollout >= rollout_episodes_per_update) or (ep == train_episodes - 1):
            buffer.compute_advantages(gamma=gamma, lam=lam, reward_norm=reward_norm_in_rollout)
            agent.update(buffer)
            buffer.reset()
            episodes_in_rollout = 0
            updates_done += 1

            # unfreeze convs after K updates
            if freeze_conv_updates and (updates_done == int(freeze_conv_updates)):
                agent.set_conv_trainable(True)
                print(f"[INIT] Unfroze conv layers after update {updates_done}")

            if (updates_done % eval_every_updates == 0) or (updates_done == total_updates):
                val_returns_tmp = run_eval(agent, data_dir, variant, feature_mode, mode="validation", episodes=validation_episodes)
                val_mean = float(np.mean(val_returns_tmp)) if len(val_returns_tmp) else -np.inf
                print(f"[VAL] update {updates_done}/{total_updates}  val_mean={val_mean:.3f}  best={best_val_mean:.3f}")

                if val_mean > best_val_mean:
                    best_val_mean = val_mean
                    torch.save(
                        {
                            "policy_state_dict": agent.policy.state_dict(),
                            "value_state_dict": agent.value_fn.state_dict(),
                            "best_val_mean": best_val_mean,
                            "update": updates_done,
                            "variant": variant,
                            "seed": seed,
                            "feature_mode": feature_mode,
                            "obs_dim": obs_dim,
                            "n_actions": n_actions,
                            "gamma": gamma,
                            "lam": lam,
                            "clip_ratio": clip_ratio,
                            "pi_lr": pi_lr,
                            "vf_lr": vf_lr,
                            "entropy_start": entropy_start,
                            "entropy_end": entropy_end,
                            "rollout_episodes_per_update": rollout_episodes_per_update,
                            "eps_mix": eps_mix,
                        },
                        best_ckpt_path,
                    )
                    print(f"[CKPT] saved new BEST-VAL -> {best_ckpt_path}")

        if (ep + 1) % print_interval == 0:
            avgN = float(np.mean(train_returns[-print_interval:]))
            print(f"[TRAIN] ep {ep+1}/{train_episodes}  last_ret={ep_ret:.1f}  avg(last {print_interval})={avgN:.1f}")

    if os.path.exists(best_ckpt_path):
        ckpt = torch.load(best_ckpt_path, map_location=device)
        agent.policy.load_state_dict(ckpt["policy_state_dict"])
        agent.value_fn.load_state_dict(ckpt["value_state_dict"])
        print(f"\nLoaded BEST-VAL checkpoint from update {ckpt['update']} with best_val_mean={ckpt['best_val_mean']:.2f}\n")
    else:
        print("\nNo BEST-VAL checkpoint found; using final weights.\n")

    # Save one episode as GIF
    if save_animation:
        viz_env = Environment(variant=variant, data_dir=data_dir)
        save_episode_animation(
            viz_env,
            agent,
            feature_mode=feature_mode,
            mode="testing",
            out_path="./code_data/plots/v2_test_episode.gif" if variant == 2 else f"./code_data/plots/v{variant}_test_episode.gif",
            fps=10,
        )

    # Final evaluation on fresh envs
    val_returns = run_eval(agent, data_dir, variant, feature_mode, mode="validation", episodes=validation_episodes)
    test_returns = run_eval(agent, data_dir, variant, feature_mode, mode="testing", episodes=test_episodes)

    summarize_returns("Training", train_returns)
    summarize_returns("Validation", val_returns)
    summarize_returns("Testing", test_returns)

    if visualize_test:
        viz_env = Environment(variant=variant, data_dir=data_dir)
        for k in range(n_visualize_episodes):
            visualize_episode(
                viz_env, agent,
                feature_mode=feature_mode,
                mode="testing",
                pause=0.06,
                title_prefix=f"[Episode {k+1}] "
            )

    return agent, train_returns, val_returns, test_returns, best_ckpt_path


# ============================================================
# Main (v8: epsilon-mix only)
# ============================================================

if __name__ == "__main__":
    DATA_DIR = "./data"
    device = "cuda" if torch.cuda.is_available() else "cpu"

    VARIANT = 2
    TRAIN_EPISODES = 6000
    VAL_EPISODES = 100
    TEST_EPISODES = 100
    SEED= 19

    EPS_MIX = 0.08  # try 0.05–0.15 (train only)

    print("\n" + "#" * 70)
    print(f"### Starting PPO v8 (epsilon-mix) seed={SEED} (VARIANT={VARIANT})")
    print("#" * 70 + "\n")

    t0 = time.time()

    agent, train_returns, val_returns, test_returns, best_ckpt_path = train_and_test_ppo(
        data_dir=DATA_DIR,
        variant=VARIANT,
        feature_mode="full",
        train_episodes=TRAIN_EPISODES,
        validation_episodes=VAL_EPISODES,
        test_episodes=TEST_EPISODES,
        device=device,
        seed=SEED,
        print_interval=250,
        ckpt_dir="./checkpoints_v2_bestval_v8",
        eval_every_updates=5,
        rollout_episodes_per_update=32,
        reward_norm_in_rollout=True,
        visualize_test=False,
        n_visualize_episodes=2,
        save_animation=False,
        eps_mix=EPS_MIX,
    )

    val_mean = float(np.mean(val_returns)) if len(val_returns) else float("nan")
    test_mean = float(np.mean(test_returns)) if len(test_returns) else float("nan")

    print(f"\nBEST checkpoint path: {best_ckpt_path}")
    print(f"[Seed {SEED}] Validation mean over {len(val_returns)} eps: {val_mean:.3f}")
    print(f"[Seed {SEED}] Testing   mean over {len(test_returns)} eps: {test_mean:.3f}")
    dt = time.time() - t0
    h = int(dt // 3600)
    m = int((dt % 3600) // 60)
    s = dt % 60
    print(f"[Seed {SEED}] Wall time: {h:02d}:{m:02d}:{s:06.3f}  (hh:mm:ss.sss)")

    benchmark = 250.685
    if test_mean > benchmark:
        print("YAY YOU BEAT THE BENCHMARK")
    else:
        print("Not yet above benchmark.")
