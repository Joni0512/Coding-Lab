import os
import random
import argparse
from copy import deepcopy
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Input

from environment import Environment

#  Utilities

def set_global_seed(seed: int):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    try:
        tf.config.experimental.enable_op_determinism(True)
    except Exception:
        pass
    tf.keras.backend.clear_session()


def get_initial_obs(env, mode: str):
    env.reset(mode)
    obs = env.get_obs()
    return np.asarray(obs, dtype=np.float32)

#  PPO Agent (vanilla PPO-Clip)

class PPOAgent:
    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        gamma: float = 0.99,
        lam: float = 0.95,
        clip_ratio: float = 0.2,
        actor_lr: float = 3e-4,
        critic_lr: float = 1e-3,
        entropy_coefficient: float = 0.01,
        train_policy_epochs: int = 10,
        train_value_epochs: int = 10,
        minibatch_size: int = 256,
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.lam = lam
        self.clip_ratio = clip_ratio
        self.entropy_coefficient = entropy_coefficient
        self.train_policy_epochs = train_policy_epochs
        self.train_value_epochs = train_value_epochs
        self.minibatch_size = minibatch_size

        # On-policy buffer
        self.buffer = {
            "states": [],
            "actions": [],
            "rewards": [],
            "dones": [],
            "logits": [],
            "values": [],
        }

        # Networks
        self.actor = self._build_mlp(self.state_dim, self.action_dim,
                                     hidden_sizes=[128, 128], logits=True)
        self.critic = self._build_mlp(self.state_dim, 1,
                                      hidden_sizes=[128, 128], logits=False)

        # Optimizers
        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)
        self.value_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)

    @staticmethod
    def _build_mlp(input_dim, output_dim, hidden_sizes, logits: bool) -> Model:
        x_in = Input(shape=(input_dim,), dtype=tf.float32)
        x = x_in
        for h in hidden_sizes:
            x = Dense(h, activation="relu")(x)
        if logits:
            out = Dense(output_dim, activation=None)(x)
        else:
            out = Dense(output_dim, activation="linear")(x)
        return Model(inputs=x_in, outputs=out)

    def _log_probs(self, logits, actions):
        log_probs_all = tf.nn.log_softmax(logits, axis=1)  # [B, A]
        actions_one_hot = tf.one_hot(actions, self.action_dim)  # [B, A]
        return tf.reduce_sum(actions_one_hot * log_probs_all, axis=1)  # [B]

    def select_action(self, state, deterministic: bool = False):
        s = np.asarray(state, dtype=np.float32).reshape(1, -1)
        logits = self.actor(s)
        probs = tf.nn.softmax(logits, axis=1)

        if deterministic:
            action = tf.argmax(probs, axis=1)
        else:
            action = tf.random.categorical(tf.math.log(probs), num_samples=1)

        return logits.numpy().squeeze(0), int(action.numpy().squeeze())

    def record(self, state, action, reward, done, logits, value):
        self.buffer["states"].append(np.asarray(state, dtype=np.float32))
        self.buffer["actions"].append(int(action))
        self.buffer["rewards"].append(float(reward))
        self.buffer["dones"].append(bool(done))
        self.buffer["logits"].append(np.asarray(logits, dtype=np.float32))
        self.buffer["values"].append(float(value))

    def compute_advantages(self, last_value=0.0, last_done=True):
        rewards = np.array(self.buffer["rewards"], dtype=np.float32)
        values = np.array(self.buffer["values"], dtype=np.float32)
        dones = np.array(self.buffer["dones"], dtype=np.float32)

        T = len(rewards)
        advantages = np.zeros_like(rewards, dtype=np.float32)
        gae = 0.0

        next_value = last_value
        next_not_done = 0.0 if last_done else 1.0

        for t in reversed(range(T)):
            if t == T - 1:
                v_tp1 = next_value
                not_done = next_not_done
            else:
                v_tp1 = values[t + 1]
                not_done = 1.0 - dones[t]

            delta = rewards[t] + self.gamma * v_tp1 * not_done - values[t]
            gae = delta + self.gamma * self.lam * not_done * gae
            advantages[t] = gae

        self.buffer["advantages"] = advantages
        self.buffer["returns"] = advantages + values

    def update_policy(self):
        buf = {
            k: np.array(v) for k, v in self.buffer.items()
            if k in ["states", "actions", "logits", "advantages", "returns"]
        }

        adv = buf["advantages"]
        buf["advantages"] = (adv - adv.mean()) / (adv.std() + 1e-8)

        N = buf["states"].shape[0]
        idxs = np.arange(N)

        for _ in range(self.train_policy_epochs):
            np.random.shuffle(idxs)
            for start in range(0, N, self.minibatch_size):
                mb = idxs[start:start + self.minibatch_size]

                with tf.GradientTape() as tape:
                    new_logits = self.actor(buf["states"][mb])
                    old_logp = self._log_probs(buf["logits"][mb], buf["actions"][mb])
                    new_logp = self._log_probs(new_logits, buf["actions"][mb])

                    ratio = tf.exp(new_logp - old_logp)  # π_new / π_old

                    unclipped = ratio * buf["advantages"][mb]
                    clipped = tf.clip_by_value(
                        ratio,
                        1.0 - self.clip_ratio,
                        1.0 + self.clip_ratio,
                    ) * buf["advantages"][mb]

                    probs = tf.nn.softmax(new_logits, axis=1)
                    log_probs = tf.nn.log_softmax(new_logits, axis=1)
                    entropy = -tf.reduce_mean(tf.reduce_sum(probs * log_probs, axis=1))

                    policy_loss = -tf.reduce_mean(tf.minimum(unclipped, clipped))
                    loss = policy_loss - self.entropy_coefficient * entropy

                grads = tape.gradient(loss, self.actor.trainable_variables)
                self.policy_optimizer.apply_gradients(
                    zip(grads, self.actor.trainable_variables)
                )

    def update_value_function(self):
        states = np.array(self.buffer["states"], dtype=np.float32)
        returns = np.array(self.buffer["returns"], dtype=np.float32)

        N = states.shape[0]
        idxs = np.arange(N)

        for _ in range(self.train_value_epochs):
            np.random.shuffle(idxs)
            for start in range(0, N, self.minibatch_size):
                mb = idxs[start:start + self.minibatch_size]

                with tf.GradientTape() as tape:
                    values = tf.squeeze(self.critic(states[mb]), axis=1)
                    loss = tf.reduce_mean((returns[mb] - values) ** 2)

                grads = tape.gradient(loss, self.critic.trainable_variables)
                self.value_optimizer.apply_gradients(
                    zip(grads, self.critic.trainable_variables)
                )

    def clear_buffer(self):
        self.buffer = {k: [] for k in self.buffer.keys()}


#  Training & Evaluation

def run_ppo_training(
    env: Environment,
    agent: PPOAgent,
    train_episodes: int = 500,
    max_steps_per_episode: int = 200,
    update_after_episodes: int = 1,
):
    episode_rewards = []

    for ep in range(train_episodes):
        env.reset("training")
        state = np.asarray(env.get_obs(), dtype=np.float32)

        ep_reward = 0.0
        done = False
        t = 0

        while not done and t < max_steps_per_episode:
            logits, action = agent.select_action(state, deterministic=False)
            value = float(agent.critic(state.reshape(1, -1)))

            reward, next_obs, done = env.step(action)
            next_state = np.asarray(next_obs, dtype=np.float32)

            agent.record(state, action, reward, done, logits, value)

            state = next_state
            ep_reward += reward
            t += 1

        if done:
            last_value = 0.0
            last_done = True
        else:
            last_value = float(agent.critic(state.reshape(1, -1)))
            last_done = False

        agent.compute_advantages(last_value=last_value, last_done=last_done)

        if (ep + 1) % update_after_episodes == 0:
            agent.update_policy()
            agent.update_value_function()
            agent.clear_buffer()

        episode_rewards.append(ep_reward)

        if (ep + 1) % 10 == 0:
            avg_10 = np.mean(episode_rewards[-10:])
            avg_100 = np.mean(episode_rewards[-100:])
            print(
                f"Episode {ep + 1}/{train_episodes} | "
                f"Return: {ep_reward:.1f} | Avg(10): {avg_10:.1f} | "
                f"Avg(100): {avg_100:.1f}"
            )

    return episode_rewards


def evaluate_policy(
    env: Environment,
    agent: PPOAgent,
    n_episodes: int = 100,
    mode: str = "validation",
    max_steps_per_episode: int = 200,
):

    rewards = []

    for _ in range(n_episodes):
        env.reset(mode)
        state = np.asarray(env.get_obs(), dtype=np.float32)

        done = False
        ep_reward = 0.0
        t = 0

        while not done and t < max_steps_per_episode:
            _, action = agent.select_action(state, deterministic=True)
            reward, next_obs, done = env.step(action)
            state = np.asarray(next_obs, dtype=np.float32)
            ep_reward += reward
            t += 1

        rewards.append(ep_reward)

    avg_reward = float(np.mean(rewards))
    print(f"[{mode}] Average reward over {n_episodes} episodes: {avg_reward:.3f}")
    return avg_reward, rewards


def summarize_training(train_rewards):
    rewards = np.asarray(train_rewards, dtype=np.float32)
    n = len(rewards)
    if n == 0:
        print("\n=== Training statistics ===")
        print("No episodes recorded.")
        print("================================\n")
        return

    best_return = float(rewards.max())
    best_ep = int(rewards.argmax()) + 1
    mean_all = float(rewards.mean())
    std_all = float(rewards.std())
    window = min(100, n)
    mean_last = float(rewards[-window:].mean())

    print("\n=== Training statistics ===")
    print(f"Episodes total:           {n}")
    print(f"Best episode:             {best_ep}  (return = {best_return:.2f})")
    print(f"Mean return (all):        {mean_all:.2f} ± {std_all:.2f}")
    print(f"Mean return (last {window}): {mean_last:.2f}")
    print("================================\n")


#  Main

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--data_dir",
        type=str,
        default="./data",
        help="Relative path to data directory (e.g. './data')",
    )
    parser.add_argument(
        "--variant",
        type=int,
        default=0,
        help="Problem variant: 0 (base), 1 (ext 1), 2 (ext 2)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=0,
        help="Random seed",
    )
    parser.add_argument(
        "--train_episodes",
        type=int,
        default=500,
        help="Number of training episodes",
    )
    parser.add_argument(
        "--val_episodes",
        type=int,
        default=100,
        help="Number of validation episodes for evaluation",
    )
    args = parser.parse_args()

    # Seed
    seed = args.seed
    set_global_seed(seed)

    # Environment
    data_dir = args.data_dir
    variant = args.variant
    env = Environment(variant, data_dir)

    # State/action dimensions
    init_obs = get_initial_obs(env, mode="training")
    state_dim = int(np.asarray(init_obs, dtype=np.float32).shape[0])
    action_dim = 5  # 0: idle, 1: up, 2: right, 3: down, 4: left

    print(f"Initialized environment with variant={variant}")
    print(f"Inferred state_dim={state_dim}, action_dim={action_dim}")

    # Agent
    agent = PPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        gamma=0.99,
        lam=0.95,
        clip_ratio=0.2,
        actor_lr=3e-4,
        critic_lr=1e-3,
        entropy_coefficient=0.01,
        train_policy_epochs=10,
        train_value_epochs=10,
        minibatch_size=256,
    )

    # Training
    max_steps_per_episode = 200
    train_rewards = run_ppo_training(
        env,
        agent,
        train_episodes=args.train_episodes,
        max_steps_per_episode=max_steps_per_episode,
        update_after_episodes=1,
    )

    summarize_training(train_rewards)

    # Plot training curve
    plt.figure(figsize=(8, 4))
    plt.plot(train_rewards, label="Episode return", alpha=0.5)

    if len(train_rewards) >= 10:
        window = max(10, len(train_rewards) // 20)
        kernel = np.ones(window, dtype=np.float32) / float(window)
        smoothed = np.convolve(train_rewards, kernel, mode="valid")
        x = np.arange(window - 1, window - 1 + len(smoothed))
        plt.plot(x, smoothed, label=f"Moving avg (window={window})", linewidth=2)

    plt.title(f"PPO training reward per episode (variant {variant})")
    plt.xlabel("Episode")
    plt.ylabel("Return")
    plt.grid(alpha=0.3)
    plt.legend()
    plt.tight_layout()

    os.makedirs("save_folder", exist_ok=True)
    fig_path = os.path.join(
        "save_folder",
        f"ppo_train_curve_variant{variant}_seed{seed}.png",
    )
    plt.savefig(fig_path, dpi=150)
    print(f"Saved training curve to: {fig_path}")

    plt.show()

    # Validation evaluation
    evaluate_policy(
        env,
        agent,
        n_episodes=args.val_episodes,
        mode="validation",
        max_steps_per_episode=max_steps_per_episode,
    )


if __name__ == "__main__":
    main()
