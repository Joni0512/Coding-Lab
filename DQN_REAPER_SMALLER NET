# ============================================================
# VARIANT 0 — Noisy Dueling Double-DQN + CNN + ReaPER (TRUE reliability-adjusted PER)
# + Φ potential feature channel (channel 9) -> 10 channels total
# + CHANGES YOU REQUESTED:
#   (6) TARGET UPDATE: HARD update every N updates (cfg["target_update_period"])
#   (7) SMALLER NET: Conv(16)->Conv(32)->Dense(64) (dueling heads also 64)
#   (ReaPER) priorities: Psi_t = (R_t^omega) * (|TD|+eps)^alpha
#          where R_t is computed within each episode (trajectory) from stored TDs:
#            finished episodes:   R_t = 1 - (sum_{i>t} td_i) / (sum_{i>=0} td_i)
#            ongoing episodes:    R_t = (sum_{i<=t} td_i) / F
#              with F = max episodic sum TD across buffer (conservative)
# ============================================================

# ---------------- Reproducibility / quiet TF logs ----------------
seed = 7
import os
os.environ["PYTHONHASHSEED"] = str(seed)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import random
random.seed(seed)

import numpy as np
np.random.seed(seed)

import tensorflow as tf
tf.random.set_seed(seed)

# ---- CPU stability/speed ----
try:
    tf.config.threading.set_intra_op_parallelism_threads(2)
    tf.config.threading.set_inter_op_parallelism_threads(2)
except Exception:
    pass

# Optional GPU memory growth
try:
    gpus = tf.config.list_physical_devices("GPU")
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
except Exception:
    pass

# ---------------- libs ----------------
from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import dijkstra

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

import matplotlib
matplotlib.rcParams["font.family"] = "DejaVu Sans"
matplotlib.rcParams["axes.unicode_minus"] = False
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

import imageio

from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Dense, Lambda, Reshape, Conv2D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber

# ---------------- Environment ----------------
from environment import Environment

# ============================================================
# Constants
# ============================================================
HEIGHT = 5
WIDTH = 5
CHANNELS = 10  # 0..8 + 9 Φ
DATA_DIR = "./data"

# ============================================================
# 1) Distance structures (Variant 0 full grid)
# ============================================================
def _build_distance_structs(env):
    if hasattr(env, "_dist_ready") and env._dist_ready:
        return

    H, W = 5, 5

    neighbor = np.zeros((25, 25), dtype=np.int32)
    for i in range(25):
        r, c = divmod(i, 5)
        for dr, dc in [(-1,0),(1,0),(0,-1),(0,1)]:
            nr, nc = r + dr, c + dc
            if 0 <= nr < 5 and 0 <= nc < 5:
                j = nr * 5 + nc
                neighbor[i, j] = 1

    graph = csr_matrix(neighbor)
    dist = dijkstra(graph, directed=False, unweighted=True)

    env._dist_matrix = dist
    env._dist_from_idx_grids = []

    for i in range(25):
        g = np.zeros((H, W), dtype=np.float32)
        for j in range(25):
            rr, cc = divmod(j, 5)
            g[rr, cc] = dist[i, j]
        env._dist_from_idx_grids.append(g)

    base = env.vertical_idx_target * 5 + env.horizontal_idx_target
    env._dist_base_grid = env._dist_from_idx_grids[base]

    finite = dist[dist < np.inf]
    env._max_dist = float(np.max(finite)) if finite.size > 0 else 1.0
    if env._max_dist <= 0:
        env._max_dist = 1.0

    env._dist_ready = True


def _coord_to_idx(coord):
    return coord[0] * 5 + coord[1]

# ============================================================
# 2) Observation encoding (+ Φ potential plane)
# ============================================================
def encode_obs(obs, env):
    """
    Returns flat vector length 5*5*10 = 250.
    Channel 9 is Φ potential plane:
      - choose item maximizing (-ALPHA*d_norm + BETA*ttl)
      - phi_plane = -ALPHA*(dist_agent/max_dist) + BETA*best_ttl
      - clip [-1, 1]
    """
    step, agent_loc, agent_load, item_locs, item_times = obs
    _build_distance_structs(env)

    H, W, C = HEIGHT, WIDTH, CHANNELS
    state = np.zeros((H, W, C), dtype=np.float32)

    # 6: eligible mask (variant 0: all cells eligible unless env says otherwise)
    eligible = getattr(env, "eligible_cells", [(r, c) for r in range(H) for c in range(W)])
    eligible_set = set(eligible)
    for r in range(H):
        for c in range(W):
            state[r, c, 6] = 1.0 if (r, c) in eligible_set else 0.0

    # 0: agent position
    ar, ac = agent_loc
    state[ar, ac, 0] = 1.0

    # 1,2,5: items
    for (loc, age) in zip(item_locs, item_times):
        r, c = loc
        state[r, c, 1] = 1.0
        state[r, c, 2] = age / float(env.max_response_time)
        state[r, c, 5] = (env.max_response_time - age) / float(env.max_response_time)

    # 3: distance agent->cell
    agent_idx = _coord_to_idx(agent_loc)
    dist_agent_grid = env._dist_from_idx_grids[agent_idx]
    max_dist = float(getattr(env, "_max_dist", 1.0))
    if max_dist <= 0:
        max_dist = 1.0
    state[:, :, 3] = dist_agent_grid / max_dist

    # 4: distance base->cell
    state[:, :, 4] = env._dist_base_grid / max_dist

    # 7: step norm
    state[:, :, 7] = step / float(env.episode_steps)

    # 8: load norm
    state[:, :, 8] = agent_load / float(env.agent_capacity)

    # 9: Φ potential feature
    ALPHA = 1.0
    BETA = 1.0
    phi_plane = np.zeros((H, W), dtype=np.float32)

    if len(item_locs) > 0:
        best_phi = -np.inf
        best_ttl = 0.0
        for (loc, age) in zip(item_locs, item_times):
            r, c = loc
            d_norm = dist_agent_grid[r, c] / max_dist
            ttl = (env.max_response_time - age) / float(env.max_response_time)
            phi = -ALPHA * d_norm + BETA * ttl
            if phi > best_phi:
                best_phi = phi
                best_ttl = ttl

        phi_plane = -ALPHA * (dist_agent_grid / max_dist) + BETA * best_ttl
        phi_plane = np.clip(phi_plane, -1.0, 1.0)

    state[:, :, 9] = phi_plane

    return state.reshape(-1)  # 250

# ============================================================
# 3) Action helpers
# ============================================================
def compute_valid_actions(env):
    valid = [0]  # idle
    r, c = env.agent_loc
    if r > 0: valid.append(1)   # up
    if c < 4: valid.append(2)   # right
    if r < 4: valid.append(3)   # down
    if c > 0: valid.append(4)   # left
    return np.array(valid, dtype=np.int32)


def valid_action_mask_from_states(states):
    """
    states: [B, 250]
    returns mask: [B, 5] bool
    Variant 0 only needs boundary checks.
    """
    B = states.shape[0]
    grid = states.reshape(B, HEIGHT, WIDTH, CHANNELS)
    agent_layer = grid[:, :, :, 0]

    flat = agent_layer.reshape(B, HEIGHT * WIDTH)
    idx = np.argmax(flat, axis=1)
    r = idx // WIDTH
    c = idx % WIDTH

    mask = np.zeros((B, 5), dtype=bool)
    mask[:, 0] = True
    mask[:, 1] = (r > 0)
    mask[:, 2] = (c < 4)
    mask[:, 3] = (r < 4)
    mask[:, 4] = (c > 0)
    return mask

# ============================================================
# 4) SumTree
# ============================================================
class SumTree:
    def __init__(self, capacity: int):
        self.capacity = 1
        while self.capacity < capacity:
            self.capacity <<= 1
        self.tree = np.zeros(2 * self.capacity, dtype=np.float32)

    def total(self) -> float:
        return float(self.tree[1])

    def add(self, p: float, idx: int):
        i = idx + self.capacity
        delta = p - self.tree[i]
        self.tree[i] = p
        i //= 2
        while i >= 1:
            self.tree[i] += delta
            i //= 2

    def find_prefixsum_idx(self, mass: float) -> int:
        i = 1
        while i < self.capacity:
            left = 2 * i
            if self.tree[left] >= mass:
                i = left
            else:
                mass -= self.tree[left]
                i = left + 1
        return i - self.capacity

# ============================================================
# 5) ReaPER Replay Buffer (TRUE episode reliability)
# ============================================================
class ReaPERReplayBuffer:
    """
    Stores transitions + (episode_id, t_in_episode).
    Priorities follow ReaPER:
        Psi = (R^omega) * (|TD| + eps)^alpha
    Reliability R computed within each episode group using the buffer's current abs TD estimates.
    """

    def __init__(
        self,
        state_dim,
        capacity=30_000,
        alpha=0.4,          # exponent on |TD|
        omega=0.2,          # exponent on reliability R
        beta0=0.2,
        beta_final=1.0,
        beta_anneal_steps=200_000,
        eps=1e-6,
        priority_clip=None,
        td_for_priority_clip=None,
    ):
        self.capacity = int(capacity)
        self.alpha = float(alpha)
        self.omega = float(omega)
        self.eps = float(eps)

        self.beta0 = float(beta0)
        self.beta_final = float(beta_final)
        self.beta_anneal_steps = max(1, int(beta_anneal_steps))
        self._beta_updates = 0

        self.priority_clip = priority_clip
        self.td_for_priority_clip = td_for_priority_clip

        self.ptr = 0
        self.n = 0

        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity,), dtype=np.int32)
        self.rewards = np.zeros((capacity,), dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros((capacity,), dtype=np.float32)

        # episode bookkeeping
        self.ep_id = np.zeros((capacity,), dtype=np.int64)
        self.t_in_ep = np.zeros((capacity,), dtype=np.int32)

        # TD tracking for reliability
        self.abs_td = np.zeros((capacity,), dtype=np.float32)  # current |TD| estimate per transition

        # episode -> list of buffer indices (in order of time)
        self._ep_to_indices = {}      # ep_id -> [idx0, idx1, ...] (time order)
        self._ep_done = {}            # ep_id -> bool

        self.sumtree = SumTree(capacity)
        self.max_priority = 1.0

        # cache of max episodic sum TD in buffer (for ongoing episode reliability)
        self._F = 1.0

    @property
    def beta(self):
        t = min(1.0, self._beta_updates / float(self.beta_anneal_steps))
        return (1.0 - t) * self.beta0 + t * self.beta_final

    def __len__(self):
        return self.n

    def _remove_index_from_episode(self, idx):
        """When ring buffer overwrites an old slot, remove it from its episode list."""
        old_ep = int(self.ep_id[idx])
        if old_ep == 0 and old_ep not in self._ep_to_indices:
            return
        if old_ep in self._ep_to_indices:
            lst = self._ep_to_indices[old_ep]
            # remove idx if present
            # (O(n) but episodes are small; okay)
            try:
                lst.remove(idx)
            except ValueError:
                pass
            if not lst:
                # cleanup episode bookkeeping
                self._ep_to_indices.pop(old_ep, None)
                self._ep_done.pop(old_ep, None)

    def add(self, s, a, r, ns, d, episode_id, t_in_episode):
        i = self.ptr

        # if overwriting an existing transition, clean it out of its episode group
        if self.n == self.capacity:
            self._remove_index_from_episode(i)

        self.states[i] = s
        self.actions[i] = int(a)
        self.rewards[i] = float(r)
        self.next_states[i] = ns
        self.dones[i] = float(d)

        self.ep_id[i] = int(episode_id)
        self.t_in_ep[i] = int(t_in_episode)

        # initialize TD high to ensure it is sampled
        self.abs_td[i] = float(self.max_priority)

        # register in episode map
        if episode_id not in self._ep_to_indices:
            self._ep_to_indices[episode_id] = []
            self._ep_done[episode_id] = False
        self._ep_to_indices[episode_id].append(i)

        # if this transition ends the episode, mark done (important for true reliability)
        if float(d) >= 1.0:
            self._ep_done[episode_id] = True

        # initial priority = max
        p = (self.max_priority + self.eps) ** self.alpha
        # reliability starts ~1 (we set R=1 for unseen, effectively)
        # so Psi ~ |TD|^alpha
        if self.priority_clip is not None:
            p = min(p, float(self.priority_clip))
        self.sumtree.add(p, i)

        self.ptr = (self.ptr + 1) % self.capacity
        self.n = min(self.n + 1, self.capacity)

    def sample(self, batch_size):
        batch_size = int(batch_size)
        total = self.sumtree.total()
        seg = total / max(1, batch_size)

        idxs = []
        for i in range(batch_size):
            mass = np.random.uniform(i * seg, (i + 1) * seg)
            idx = self.sumtree.find_prefixsum_idx(mass)
            if idx >= self.n:
                idx = np.random.randint(self.n)
            idxs.append(idx)

        idxs = np.array(idxs, dtype=np.int32)

        leaf_idx = idxs + self.sumtree.capacity
        p = self.sumtree.tree[leaf_idx]
        p = np.clip(p, 1e-12, None)
        P = p / max(1e-12, total)

        self._beta_updates += 1
        w = (self.n * P) ** (-self.beta)
        w = w / (w.max() + 1e-12)

        return (
            self.states[idxs],
            self.actions[idxs],
            self.rewards[idxs],
            self.next_states[idxs],
            self.dones[idxs],
            idxs,
            w.astype(np.float32),
        )

    def _recompute_F(self):
        """F = max episodic sum abs_td across episodes currently present."""
        F = 1e-6
        for ep, idxs in self._ep_to_indices.items():
            if not idxs:
                continue
            s = float(np.sum(self.abs_td[np.array(idxs, dtype=np.int32)]))
            if s > F:
                F = s
        self._F = max(F, 1e-6)

    def _recompute_episode_priorities(self, episode_id):
        """
        Recompute reliability R_t and Psi_t for all transitions in this episode,
        then update SumTree priorities for those indices.
        """
        idxs = self._ep_to_indices.get(episode_id, None)
        if not idxs:
            return

        # sort by time-in-episode (robust even if indices are not appended strictly in order)
        idxs_sorted = sorted(idxs, key=lambda j: int(self.t_in_ep[j]))
        idxs_arr = np.array(idxs_sorted, dtype=np.int32)

        td = self.abs_td[idxs_arr].astype(np.float32)
        td = np.clip(td, 0.0, None)

        # finished episode -> true reliability
        if self._ep_done.get(episode_id, False):
            total = float(np.sum(td))
            if total <= 1e-12:
                R = np.ones_like(td, dtype=np.float32)
            else:
                # suffix sums (future TD)
                suffix = np.cumsum(td[::-1])[::-1]  # sum from t..end
                # future = sum_{i>t} = suffix[t] - td[t]
                future = suffix - td
                R = 1.0 - (future / total)
                R = np.clip(R, 0.0, 1.0).astype(np.float32)
        else:
            # ongoing episode -> conservative reliability using F
            self._recompute_F()
            F = float(self._F)
            prefix = np.cumsum(td)  # sum_{i<=t}
            R = prefix / max(F, 1e-6)
            R = np.clip(R, 0.0, 1.0).astype(np.float32)

        # ReaPER sampling criterion
        Psi = (np.power(R + 1e-12, self.omega) *
               np.power(td + self.eps, self.alpha))

        if self.priority_clip is not None:
            Psi = np.minimum(Psi, float(self.priority_clip))

        # update max_priority proxy (in TD-space) for new adds
        # (keep it in TD units; we store abs_td separately already)
        self.max_priority = max(self.max_priority, float(np.max(td)) if td.size else self.max_priority)

        # push to sumtree
        for j, p in zip(idxs_arr, Psi):
            self.sumtree.add(float(p), int(j))

    def update_priorities(self, idxs, td_errors):
        """
        Update abs TD for sampled transitions, then recompute full priorities
        for the *episodes* those transitions belong to (true ReaPER coupling).
        """
        td = np.abs(td_errors).astype(np.float32)
        if self.td_for_priority_clip is not None:
            td = np.minimum(td, float(self.td_for_priority_clip))

        idxs = np.asarray(idxs, dtype=np.int32)
        # write abs_td
        self.abs_td[idxs] = td

        # recompute per affected episode (usually small number)
        affected_eps = set(int(self.ep_id[i]) for i in idxs)
        for ep in affected_eps:
            self._recompute_episode_priorities(ep)

# ============================================================
# 6) NoisyDense (no noise at inference)
# ============================================================
class NoisyDense(tf.keras.layers.Layer):
    def __init__(self, units, activation=None, sigma0=0.125, **kwargs):
        super().__init__(**kwargs)
        self.units = int(units)
        self.activation = tf.keras.activations.get(activation)
        self.sigma0 = float(sigma0)

    def build(self, input_shape):
        in_dim = int(input_shape[-1])
        mu_range = 1.0 / np.sqrt(in_dim)

        self.mu_w = self.add_weight(
            name="mu_w",
            shape=(in_dim, self.units),
            initializer=tf.keras.initializers.RandomUniform(-mu_range, mu_range),
            trainable=True,
        )
        self.sigma_w = self.add_weight(
            name="sigma_w",
            shape=(in_dim, self.units),
            initializer=tf.keras.initializers.Constant(self.sigma0 / np.sqrt(in_dim)),
            trainable=True,
        )
        self.mu_b = self.add_weight(
            name="mu_b",
            shape=(self.units,),
            initializer=tf.keras.initializers.RandomUniform(-mu_range, mu_range),
            trainable=True,
        )
        self.sigma_b = self.add_weight(
            name="sigma_b",
            shape=(self.units,),
            initializer=tf.keras.initializers.Constant(self.sigma0 / np.sqrt(in_dim)),
            trainable=True,
        )
        super().build(input_shape)

    def call(self, inputs, training=None):
        if training is False:
            out = tf.matmul(inputs, self.mu_w) + self.mu_b
            return self.activation(out) if self.activation is not None else out

        def f(x):
            return tf.sign(x) * tf.sqrt(tf.abs(x))

        in_dim = self.mu_w.shape[0]
        out_dim = self.units

        eps_in = f(tf.random.normal([in_dim]))
        eps_out = f(tf.random.normal([out_dim]))

        eps_w = tf.einsum("i,j->ij", eps_in, eps_out)
        w = self.mu_w + self.sigma_w * eps_w
        b = self.mu_b + self.sigma_b * eps_out

        out = tf.matmul(inputs, w) + b
        return self.activation(out) if self.activation is not None else out

# ============================================================
# 7) DQN Agent (Dueling Double-DQN + Noisy + ReaPER + HARD target updates)
# ============================================================
class DQN_Agent:
    def __init__(self, state_size, action_size, cfg):
        self.state_size = int(state_size)
        self.action_size = int(action_size)

        self.gamma = float(cfg.get("gamma", 0.99))
        self.epsilon = float(cfg.get("epsilon", 1.0))
        self.epsilon_min = float(cfg.get("epsilon_min", 0.05))
        self.epsilon_decay = float(cfg.get("epsilon_decay", 0.995))
        self.batch_size = int(cfg.get("batch_size", 32))

        self.use_noisy = bool(cfg.get("use_noisy", True))
        self.noisy_sigma0 = float(cfg.get("noisy_sigma0", 0.125))

        # HARD target update period (requested change)
        self.target_update_period = int(cfg.get("target_update_period", 1000))

        # ReaPER params
        self.use_reaper = bool(cfg.get("use_reaper", True))
        if not self.use_reaper:
            raise ValueError("This script expects ReaPER (use_reaper=True).")

        reaper_alpha = float(cfg.get("reaper_alpha", 0.4))     # exponent on |TD|
        reaper_omega = float(cfg.get("reaper_omega", 0.2))     # exponent on reliability
        reaper_beta0 = float(cfg.get("reaper_beta0", 0.2))
        reaper_beta1 = float(cfg.get("reaper_beta1", 1.0))
        reaper_eps = float(cfg.get("reaper_eps", 1e-6))
        reaper_anneal = int(cfg.get("reaper_beta_anneal_steps", 200_000))

        priority_clip = cfg.get("priority_clip", None)
        td_clip = cfg.get("td_for_priority_clip", None)

        # simplified net (requested change): fixed sizes
        self.hidden_units = int(cfg.get("units", 64))  # keep cfg, but default 64

        self.model = self._build_model()
        self.target_model = self._build_model()
        self.target_model.set_weights(self.model.get_weights())

        self.memory = ReaPERReplayBuffer(
            state_dim=self.state_size,
            capacity=int(cfg.get("replay_capacity", 30_000)),
            alpha=reaper_alpha,
            omega=reaper_omega,
            beta0=reaper_beta0,
            beta_final=reaper_beta1,
            beta_anneal_steps=reaper_anneal,
            eps=reaper_eps,
            priority_clip=priority_clip,
            td_for_priority_clip=td_clip,
        )

        self.learn_start = int(cfg.get("learn_start", max(5000, 5 * self.batch_size)))
        self.total_updates = 0
        self._huber_none = tf.keras.losses.Huber(delta=1.0, reduction=tf.keras.losses.Reduction.NONE)

    def _dense_layer(self, units, activation=None):
        if self.use_noisy:
            return NoisyDense(units, activation=activation, sigma0=self.noisy_sigma0)
        return Dense(units, activation=activation)

    def _build_model(self):
        DenseLayer = self._dense_layer

        inp = Input(shape=(self.state_size,))
        x = Reshape((HEIGHT, WIDTH, CHANNELS))(inp)

        # (7) smaller net
        x = Conv2D(16, kernel_size=3, padding="same", activation="relu")(x)
        x = Conv2D(32, kernel_size=3, padding="same", activation="relu")(x)
        x = Flatten()(x)
        x = DenseLayer(64, activation="relu")(x)

        # dueling heads (also 64)
        adv = DenseLayer(64, activation="relu")(x)
        adv = DenseLayer(self.action_size)(adv)

        val = DenseLayer(64, activation="relu")(x)
        val = DenseLayer(1)(val)

        def combine_streams(tensors):
            v, a = tensors
            a_mean = tf.reduce_mean(a, axis=1, keepdims=True)
            return v + (a - a_mean)

        q = Lambda(combine_streams)([val, adv])

        model = Model(inputs=inp, outputs=q)
        model.compile(
            optimizer=Adam(learning_rate=3e-4, clipnorm=10.0),
            loss=Huber(delta=1.0),
            run_eagerly=False,
        )
        return model

    def select_action(self, state_vec, valid_actions, training=True):
        if training and np.random.rand() < self.epsilon:
            return int(np.random.choice(valid_actions))

        q = self.model(state_vec, training=False).numpy()[0]
        masked_q = np.full_like(q, -1e9, dtype=np.float32)
        masked_q[valid_actions] = q[valid_actions]
        return int(np.argmax(masked_q))

    def record(self, s, a, r, ns, d, episode_id, t_in_episode):
        self.memory.add(
            s.reshape(-1).astype(np.float32),
            int(a),
            float(r),
            ns.reshape(-1).astype(np.float32),
            float(d),
            int(episode_id),
            int(t_in_episode),
        )

    def update_epsilon(self):
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def _hard_update_target(self):
        self.target_model.set_weights(self.model.get_weights())

    @tf.function(reduce_retracing=True)
    def _train_step_weighted(self, S, A, Tgt, is_w):
        with tf.GradientTape() as tape:
            q_all = self.model(S, training=True)  # noisy during training
            idx = tf.stack([tf.range(tf.shape(A)[0]), A], axis=1)
            q_sa = tf.gather_nd(q_all, idx)
            per_sample = self._huber_none(Tgt, q_sa)
            loss = tf.reduce_mean(is_w * per_sample)

        grads = tape.gradient(loss, self.model.trainable_variables)
        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))
        td = Tgt - q_sa
        return loss, td

    def update_weights(self):
        if len(self.memory) < self.learn_start:
            return

        S, A, R, NS, D, idxs, is_w = self.memory.sample(self.batch_size)

        valid_next_mask = valid_action_mask_from_states(NS)

        # Double-DQN: online selects
        q_next_online = self.model(NS, training=False).numpy()
        q_next_online[~valid_next_mask] = -1e9
        best_next = np.argmax(q_next_online, axis=1)

        # target evaluates
        q_next_target = self.target_model(NS, training=False).numpy()
        q_next_target[~valid_next_mask] = -1e9
        best_next_q = q_next_target[np.arange(self.batch_size), best_next]

        y = R + self.gamma * best_next_q * (1.0 - D)

        S_tf = tf.convert_to_tensor(S, dtype=tf.float32)
        A_tf = tf.convert_to_tensor(A, dtype=tf.int32)
        y_tf = tf.convert_to_tensor(y, dtype=tf.float32)
        w_tf = tf.convert_to_tensor(is_w, dtype=tf.float32)

        _, td_tf = self._train_step_weighted(S_tf, A_tf, y_tf, w_tf)
        td = td_tf.numpy()

        # ReaPER: update TDs, recompute episode reliabilities + priorities
        self.memory.update_priorities(idxs, td)

        # (6) HARD target updates
        self.total_updates += 1
        if self.total_updates % self.target_update_period == 0:
            self._hard_update_target()

# ============================================================
# 8) Evaluation
# ============================================================
def evaluate_average_reward(agent, variant, data_dir, episodes=100, mode="testing", max_steps=200):
    env = Environment(variant, data_dir)
    max_steps = min(getattr(env, "episode_steps", 200), max_steps)

    if mode == "testing":
        max_eps = min(int(episodes), len(env.test_episodes))
    elif mode == "validation":
        max_eps = min(int(episodes), len(env.validation_episodes))
    else:
        max_eps = int(episodes)

    total = 0.0
    for _ in range(max_eps):
        obs = env.reset(mode)
        s = encode_obs(obs, env).reshape(1, -1)

        done = 0
        ep_ret = 0.0
        t = 0
        while not done and t < max_steps:
            valid = compute_valid_actions(env)
            a = agent.select_action(s, valid_actions=valid, training=False)
            r, nxt, done = env.step(a)
            s = encode_obs(nxt, env).reshape(1, -1)
            ep_ret += float(r)
            t += 1

        total += ep_ret

    return total / float(max_eps)

# ============================================================
# 9) Training with VAL100 selection + checkpointing every 25
# ============================================================
def train_variant0_with_val100_selection(
    cfg,
    train_episodes=800,
    variant=0,
    data_dir="./data",
    save_folder="save_folder_variant0_val100",
    save_every=25
):
    os.makedirs(save_folder, exist_ok=True)

    env = Environment(variant=variant, data_dir=data_dir)
    obs0 = env.reset("training")
    state_size = encode_obs(obs0, env).shape[0]
    action_size = 5

    agent = DQN_Agent(state_size, action_size, cfg)

    max_steps = min(getattr(env, "episode_steps", 200), 200)
    rewards = np.zeros(train_episodes, dtype=np.float32)

    best_val100 = -np.inf
    best_path = os.path.join(save_folder, f"best_by_val100_variant{variant}.weights.h5")
    ckpt_log = []

    # episode id counter for ReaPER
    episode_counter = 1

    for ep in range(train_episodes):
        obs = env.reset("training")
        s = encode_obs(obs, env).reshape(1, -1)

        done = False
        ep_ret = 0.0
        t = 0

        # new episode id
        ep_id = episode_counter
        episode_counter += 1

        if ep < 400:
            update_every = 8
            per_update_steps = 1
        else:
            update_every = 4
            per_update_steps = 1

        step_since_update = 0

        while not done and t < max_steps:
            valid = compute_valid_actions(env)
            a = agent.select_action(s, valid_actions=valid, training=True)

            r, nxt, done = env.step(a)
            ns = encode_obs(nxt, env).reshape(1, -1)

            truncated = (t + 1 >= max_steps)
            terminal_flag = float(done or truncated)

            # ReaPER needs (episode_id, t_in_episode)
            agent.record(s, a, r, ns, terminal_flag, episode_id=ep_id, t_in_episode=t)

            step_since_update += 1
            if step_since_update % update_every == 0:
                for _ in range(per_update_steps):
                    agent.update_weights()

            s = ns
            ep_ret += float(r)
            t += 1

            if truncated:
                break

        rewards[ep] = ep_ret
        agent.update_epsilon()

        if (ep + 1) % 25 == 0:
            avg25 = float(np.mean(rewards[max(0, ep + 1 - 25):ep + 1]))
            print(f"[TRAIN] ep {ep+1:4d}/{train_episodes}  ret={ep_ret:7.1f}  avg25={avg25:7.1f}  eps={agent.epsilon:.3f}")

        if (ep + 1) % save_every == 0:
            ckpt_path = os.path.join(save_folder, f"checkpoint_ep{ep+1:04d}.weights.h5")
            agent.model.save_weights(ckpt_path, overwrite=True)

            val100_mean = evaluate_average_reward(
                agent, variant, data_dir, episodes=100, mode="validation", max_steps=max_steps
            )
            ckpt_log.append((ep + 1, float(val100_mean), ckpt_path))

            print(f"[CKPT] saved: {ckpt_path}")
            print(f"[VAL100] ep {ep+1}: mean over 100 val eps = {val100_mean:.3f}")

            if val100_mean > best_val100:
                best_val100 = float(val100_mean)
                agent.model.save_weights(best_path, overwrite=True)
                print(f"  -> NEW BEST by VAL100 saved: {best_path} (best_val100={best_val100:.3f})")

    if os.path.exists(best_path):
        agent.model.load_weights(best_path)
        agent.target_model.set_weights(agent.model.get_weights())
        print(f"\nLoaded BEST-by-VAL100 weights: {best_path} (best_val100={best_val100:.3f})\n")
    else:
        print("\nWARNING: best_by_val100 weights not found; using last weights.\n")

    plt.figure(figsize=(10, 4))
    plt.plot(rewards, alpha=0.8)
    plt.title("Training rewards (variant 0)")
    plt.xlabel("episode")
    plt.ylabel("reward")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    rpath = os.path.join(save_folder, "train_rewards.png")
    plt.savefig(rpath, dpi=160)
    plt.close()
    print("[PLOT] saved:", rpath)

    log_path = os.path.join(save_folder, "val100_checkpoints.csv")
    with open(log_path, "w", encoding="utf-8") as f:
        f.write("train_episode,val100_mean,checkpoint_path\n")
        for e, m, p in ckpt_log:
            f.write(f"{e},{m:.6f},{p}\n")
    print("[LOG] saved:", log_path)

    return agent, rewards, ckpt_log

# ============================================================
# 10) GIF renderer + heatmaps (unchanged from your last version)
# ============================================================
class PrettyRendererGIF:
    def __init__(self, env, show_ttl=False):
        self.env = env
        self.show_ttl = bool(show_ttl)
        self.fig = None
        self.ax = None
        self.agent_sc = None
        self.items_sc = None
        self.info = None
        self.ttl_texts = []

    def setup(self):
        if self.fig is not None:
            return
        plt.ioff()
        self.fig, self.ax = plt.subplots(figsize=(6.2, 6.2))
        ax = self.ax
        ax.set_aspect("equal")

        H, W = self.env.vertical_cell_count, self.env.horizontal_cell_count
        ax.set_xlim(-0.5, W - 0.5)
        ax.set_ylim(H - 0.5, -0.5)

        ax.set_xticks(np.arange(-.5, W, 1), minor=True)
        ax.set_yticks(np.arange(-.5, H, 1), minor=True)
        ax.grid(which="minor", linestyle="-", linewidth=0.6, alpha=0.28)
        ax.set_xticks([])
        ax.set_yticks([])

        eligible_set = set(getattr(self.env, "eligible_cells", [(r, c) for r in range(H) for c in range(W)]))

        for r in range(H):
            for c in range(W):
                blocked = (r, c) not in eligible_set
                col = (0.965, 0.965, 1.0) if not blocked else (0.85, 0.85, 0.88)
                ax.add_patch(Rectangle((c - 0.5, r - 0.5), 1, 1,
                                       facecolor=col, edgecolor="none", zorder=0))

        tr, tc = self.env.target_loc
        ax.scatter([tc], [tr], marker="s", s=560, edgecolors="black", linewidths=1.5, zorder=3)
        ax.text(tc, tr, "T", ha="center", va="center", fontsize=16, fontweight="bold", zorder=4)

        self.agent_sc = ax.scatter([], [], marker="o", s=560,
                                   edgecolors="black", linewidths=1.5, zorder=5)
        self.items_sc = ax.scatter([], [], marker="*", s=380,
                                   edgecolors="black", linewidths=1.0, zorder=4)

        self.info = ax.text(0.02, 1.02, "", transform=ax.transAxes,
                            ha="left", va="bottom", fontsize=10)

    def _clear_ttl(self):
        for t in self.ttl_texts:
            t.remove()
        self.ttl_texts.clear()

    def update(self, step, ep_return, action=None):
        self.setup()

        ar, ac = self.env.agent_loc
        self.agent_sc.set_offsets(np.array([[ac, ar]], dtype=np.float32))

        item_locs = getattr(self.env, "item_locs", [])
        if len(item_locs) > 0:
            xs = [c for (r, c) in item_locs]
            ys = [r for (r, c) in item_locs]
            self.items_sc.set_offsets(np.array(list(zip(xs, ys)), dtype=np.float32))
        else:
            self.items_sc.set_offsets(np.zeros((0, 2), dtype=np.float32))

        self._clear_ttl()
        if self.show_ttl and hasattr(self.env, "item_times") and len(item_locs) > 0:
            for (loc, age) in zip(item_locs, self.env.item_times):
                r, c = loc
                ttl = int(getattr(self.env, "max_response_time", 0) - age)
                self.ttl_texts.append(
                    self.ax.text(c, r - 0.22, f"{ttl}", ha="center", va="center", fontsize=9, zorder=6)
                )

        names = {0: "idle", 1: "up", 2: "right", 3: "down", 4: "left"}
        self.info.set_text(
            f"step={step:03d}  return={ep_return:7.2f}  action={names.get(action, '-')}\n"
            f"items={len(item_locs)}"
        )

    def grab_frame(self):
        self.fig.canvas.draw()
        buf = np.asarray(self.fig.canvas.buffer_rgba(), dtype=np.uint8)  # (H, W, 4)
        return buf[:, :, :3].copy()

    def close(self):
        if self.fig is not None:
            plt.close(self.fig)
            self.fig = None
            self.ax = None


def save_episode_animation(agent, variant=0, data_dir="./data", mode="testing",
                           max_steps=200, fps=10, out_path="episode.gif",
                           show_ttl=False):
    env = Environment(variant=variant, data_dir=data_dir)
    obs = env.reset(mode)
    s = encode_obs(obs, env).reshape(1, -1)

    renderer = PrettyRendererGIF(env, show_ttl=show_ttl)
    frames = []

    done = False
    ep_ret = 0.0
    t = 0

    renderer.update(t, ep_ret, action=None)
    frames.append(renderer.grab_frame())

    while not done and t < max_steps:
        valid = compute_valid_actions(env)
        a = agent.select_action(s, valid_actions=valid, training=False)

        r, nxt, done = env.step(a)
        s = encode_obs(nxt, env).reshape(1, -1)

        ep_ret += float(r)
        t += 1

        renderer.update(t, ep_ret, action=a)
        frames.append(renderer.grab_frame())

    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
    if not out_path.lower().endswith(".gif"):
        out_path = out_path + ".gif"
    imageio.mimsave(out_path, frames, fps=fps)

    renderer.close()
    print(f"[VIS] saved: {out_path} | return={ep_ret:.2f} | steps={t}")
    return out_path


def save_item_position_heatmap(variant=0, data_dir="./data", mode="testing",
                               episodes=300, max_steps=200,
                               out_path="item_position_heatmap.png"):
    env = Environment(variant=variant, data_dir=data_dir)
    H, W = env.vertical_cell_count, env.horizontal_cell_count
    heat = np.zeros((H, W), dtype=np.float32)

    for _ in range(int(episodes)):
        _ = env.reset(mode)
        done = False
        t = 0
        while not done and t < max_steps:
            for (r, c) in getattr(env, "item_locs", []):
                heat[r, c] += 1.0
            a = int(np.random.choice(compute_valid_actions(env)))
            _, _, done = env.step(a)
            t += 1

    eligible_set = set(getattr(env, "eligible_cells", [(r, c) for r in range(H) for c in range(W)]))
    masked = heat.copy()
    for r in range(H):
        for c in range(W):
            if (r, c) not in eligible_set:
                masked[r, c] = np.nan

    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
    plt.figure(figsize=(6.6, 6.0))
    plt.title(f"Item POSITION heatmap (summed over steps)\nvariant={variant} | mode={mode} | episodes={episodes}")
    plt.imshow(masked, origin="upper")
    plt.colorbar(label="count (summed over steps)")
    tr, tc = env.target_loc
    plt.scatter([tc], [tr], marker="s", s=220, edgecolors="black", linewidths=1.5)
    plt.text(tc, tr, "T", ha="center", va="center", fontweight="bold")
    plt.xticks(range(W)); plt.yticks(range(H))
    plt.grid(alpha=0.25)
    plt.tight_layout()
    plt.savefig(out_path, dpi=200)
    plt.close()
    print(f"[HEATMAP] saved: {out_path}")
    return out_path


def save_item_spawn_event_heatmap(variant=0, data_dir="./data", mode="testing",
                                  episodes=300, max_steps=200,
                                  out_path="item_spawn_event_heatmap_PERCENT.png"):
    """
    Heatmap shows % of ALL spawn events that occurred at each cell.
    """
    env = Environment(variant=variant, data_dir=data_dir)
    H, W = env.vertical_cell_count, env.horizontal_cell_count
    heat = np.zeros((H, W), dtype=np.float32)

    total_spawns = 0.0

    for _ in range(int(episodes)):
        _ = env.reset(mode)
        done = False
        t = 0
        prev = set(getattr(env, "item_locs", []))
        while not done and t < max_steps:
            cur = set(getattr(env, "item_locs", []))
            new_items = cur - prev
            for (r, c) in new_items:
                heat[r, c] += 1.0
                total_spawns += 1.0

            a = int(np.random.choice(compute_valid_actions(env)))
            _, _, done = env.step(a)
            prev = cur
            t += 1

    if total_spawns > 0:
        heat = 100.0 * heat / total_spawns

    eligible_set = set(getattr(env, "eligible_cells", [(r, c) for r in range(H) for c in range(W)]))
    masked = heat.copy()
    for r in range(H):
        for c in range(W):
            if (r, c) not in eligible_set:
                masked[r, c] = np.nan

    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
    plt.figure(figsize=(6.6, 6.0))
    plt.title(f"Item SPAWN-EVENT heatmap (% of all spawns)\nvariant={variant} | mode={mode} | episodes={episodes}")
    plt.imshow(masked, origin="upper")
    plt.colorbar(label="spawn events (%)")
    tr, tc = env.target_loc
    plt.scatter([tc], [tr], marker="s", s=220, edgecolors="black", linewidths=1.5)
    plt.text(tc, tr, "T", ha="center", va="center", fontweight="bold")
    plt.xticks(range(W)); plt.yticks(range(H))
    plt.grid(alpha=0.25)
    plt.tight_layout()
    plt.savefig(out_path, dpi=200)
    plt.close()
    print(f"[SPAWN %] saved: {out_path} | total_spawns={int(total_spawns)}")
    return out_path

# ============================================================
# 11) MAIN
# ============================================================
if __name__ == "__main__":
    VARIANT = 0

    TRAIN_EPISODES = 800
    VAL_EPISODES_FINAL = 100
    TEST_EPISODES_FINAL = 100
    MAX_STEPS = 200

    save_folder = os.path.join(os.getcwd(), "save_folder_variant0_reaper_val100_hardtarget_smallnet")
    os.makedirs(save_folder, exist_ok=True)

    cfg = dict(
        gamma=0.9389065584049,
        epsilon=1.0,
        epsilon_min=0.079088807071618797,
        epsilon_decay=0.99,
        batch_size=32,

        # (7) small net
        units=64,

        use_noisy=True,
        noisy_sigma0=0.12546649741233012,

        # ReaPER
        use_reaper=True,
        reaper_alpha=0.4,          # like PER alpha
        reaper_omega=0.2,          # reliability exponent (paper suggests extra regularization)
        reaper_beta0=0.2,
        reaper_beta1=1.0,
        reaper_eps=1e-6,
        reaper_beta_anneal_steps=200_000,

        replay_capacity=30_000,
        learn_start=5000,

        priority_clip=None,
        td_for_priority_clip=None,

        # (6) hard target update period
        target_update_period=1000,  # try 500 or 1000
    )

    agent, rewards, ckpt_log = train_variant0_with_val100_selection(
        cfg,
        train_episodes=TRAIN_EPISODES,
        variant=VARIANT,
        data_dir=DATA_DIR,
        save_folder=save_folder,
        save_every=25,
    )

    avg_val = evaluate_average_reward(agent, VARIANT, DATA_DIR, episodes=VAL_EPISODES_FINAL, mode="validation", max_steps=MAX_STEPS)
    avg_test = evaluate_average_reward(agent, VARIANT, DATA_DIR, episodes=TEST_EPISODES_FINAL, mode="testing", max_steps=MAX_STEPS)

    print(f"[Variant {VARIANT}] Validation avg over {VAL_EPISODES_FINAL} episodes: {avg_val:.3f}")
    print(f"[Variant {VARIANT}] Test       avg over {TEST_EPISODES_FINAL} episodes: {avg_test:.3f}")

    plt.figure(figsize=(14, 6))
    window = 25
    kernel = np.ones(window, dtype=np.float32) / float(window)

    r = np.array(rewards, dtype=np.float32)
    x = np.arange(1, len(r) + 1)

    raw_line, = plt.plot(x, r, alpha=0.20, label="Train (raw)")
    if len(r) >= window:
        mov = np.convolve(r, kernel, mode="valid")
        xm = np.arange(window, len(r) + 1)
        plt.plot(xm, mov, label="Train (25-ep avg)", color=raw_line.get_color())

    plt.axhline(y=avg_val, linestyle=":", alpha=0.9, label=f"Val avg (100 eps) = {avg_val:.1f}")
    plt.axhline(y=avg_test, linestyle="--", alpha=0.9, label=f"Test avg (100 eps) = {avg_test:.1f}")

    plt.title("Variant 0 – Noisy Dueling Double-DQN + TRUE ReaPER + Φ\n"
              "HARD target update; small net; checkpoint every 25 eps; select BEST by VAL100")
    plt.xlabel("Training episode")
    plt.ylabel("Reward")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()

    out_plot = os.path.join(save_folder, "variant0_train_curve.png")
    plt.savefig(out_plot, dpi=150)
    plt.close()
    print(f"[PLOT] saved: {out_plot}")

    gif_path = os.path.join(save_folder, f"variant{VARIANT}_late_episode_testing.gif")
    save_episode_animation(
        agent,
        variant=VARIANT,
        data_dir=DATA_DIR,
        mode="testing",
        max_steps=MAX_STEPS,
        fps=10,
        out_path=gif_path,
        show_ttl=False
    )

    pos_heat = os.path.join(save_folder, f"variant{VARIANT}_item_position_heatmap_testing.png")
    spawn_heat = os.path.join(save_folder, f"variant{VARIANT}_item_spawn_event_heatmap_testing_PERCENT.png")

    save_item_position_heatmap(
        variant=VARIANT,
        data_dir=DATA_DIR,
        mode="testing",
        episodes=300,
        max_steps=MAX_STEPS,
        out_path=pos_heat
    )

    save_item_spawn_event_heatmap(
        variant=VARIANT,
        data_dir=DATA_DIR,
        mode="testing",
        episodes=300,
        max_steps=MAX_STEPS,
        out_path=spawn_heat
    )

    print("\nDONE. Outputs in:", save_folder)
