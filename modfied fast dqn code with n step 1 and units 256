# ============================================================
# FAST VARIANT-0 DQN (Noisy + PER + Double + Dueling Q-map)
# + 10 channels incl. Φ (deliverability score)
# + FAST v0 action masking via lookup tables
# + Replay buffer capacity = 30k
# + Validation and Testing: guaranteed 100 unique episodes each (no repeats)
# + Extra verification: check val/test overlap + check exact CSV set usage
#
# CHANGES REQUESTED:
# - "Late turbo" update schedule (after ep 450 -> update_every=4, per_update_steps=3)
# - units=256 (used in a small FC trunk after convs; keeps Q-map head)
# - plt import fixed
# ============================================================

# ------------- Reproducibility / Quiet logs -----------------
seed = 2
import os
os.environ["PYTHONHASHSEED"] = str(seed)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import random
random.seed(seed)

import numpy as np
np.random.seed(seed)

import tensorflow as tf
tf.random.set_seed(seed)

# --- Optional: GPU memory growth ----
try:
    gpus = tf.config.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
except Exception:
    pass

from collections import deque, Counter

# --- Plotting (fix for NameError: plt not defined) ---
import matplotlib.pyplot as plt

# --- SciPy for shortest paths --------
from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import dijkstra

# ============================================================
# Environment (PATCHED: validation no-replacement + tracking IDs)
# ============================================================

import pandas as pd
from copy import deepcopy
from itertools import compress


class Environment(object):
    def __init__(self, variant, data_dir):
        self.variant = variant
        self.vertical_cell_count = 5
        self.horizontal_cell_count = 5
        self.vertical_idx_target = 2
        self.horizontal_idx_target = 0
        self.target_loc = (self.vertical_idx_target, self.horizontal_idx_target)
        self.episode_steps = 200
        self.max_response_time = 15 if self.variant == 2 else 10
        self.reward = 25 if self.variant == 2 else 15

        # Make data_dir absolute relative to this file if needed
        if not os.path.isabs(data_dir):
            data_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), data_dir))
        self.data_dir = data_dir

        # -------- variant-aware base path --------
        self._base = os.path.join(self.data_dir, f'variant_{self.variant}')

        # Episode id lists
        self.training_episodes = pd.read_csv(os.path.join(self._base, 'training_episodes.csv')).training_episodes.tolist()
        self.validation_episodes = pd.read_csv(os.path.join(self._base, 'validation_episodes.csv')).validation_episodes.tolist()
        self.test_episodes = pd.read_csv(os.path.join(self._base, 'test_episodes.csv')).test_episodes.tolist()

        self.remaining_training_episodes = deepcopy(self.training_episodes)

        # ---- validation without replacement ----
        self.remaining_validation_episodes = deepcopy(self.validation_episodes)

        # Capacity
        self.agent_capacity = 1 if self.variant in (0, 2) else 3

        # Eligible cells (holes in variant 2)
        if self.variant in (0, 1):
            self.eligible_cells = [(r, c) for r in range(5) for c in range(5)]
        else:
            self.eligible_cells = [
                (0,0),        (0,2), (0,3), (0,4),
                (1,0),        (1,2),        (1,4),
                (2,0),        (2,2),        (2,4),
                (3,0), (3,1), (3,2),        (3,4),
                (4,0), (4,1), (4,2),        (4,4)
            ]

        # -------- discover available episode files & offset --------
        ep_dir = os.path.join(self._base, 'episode_data')
        files = [f for f in os.listdir(ep_dir) if f.startswith('episode_') and f.endswith('.csv')]
        if not files:
            raise FileNotFoundError(f"No episode files found in {ep_dir}")

        def _parse_num(fname: str) -> int:
            core = fname[len('episode_'):-4]
            return int(core)

        self._available_ids = sorted(_parse_num(f) for f in files)
        self._available_set = set(self._available_ids)
        self._ep_min = min(self._available_ids)

        # tracking
        self.last_episode_list_id = None
        self.last_episode_file_id = None

    def reset(self, mode):
        modes = ['training', 'validation', 'testing']
        if mode not in modes:
            raise ValueError('Invalid mode. Expected one of: %s' % modes)

        self.step_count = 0
        self.agent_loc = (self.vertical_idx_target, self.horizontal_idx_target)
        self.agent_load = 0
        self.item_locs = []
        self.item_times = []

        # choose episode id from lists
        if mode == "testing":
            episode = self.test_episodes[0]
            self.test_episodes.remove(episode)

        elif mode == "validation":
            if not self.remaining_validation_episodes:
                self.remaining_validation_episodes = deepcopy(self.validation_episodes)
            episode = self.remaining_validation_episodes.pop(0)

        else:  # training
            if not self.remaining_training_episodes:
                self.remaining_training_episodes = deepcopy(self.training_episodes)
            episode = random.choice(self.remaining_training_episodes)
            self.remaining_training_episodes.remove(episode)

        file_id = episode if episode in self._available_set else episode + self._ep_min

        # tracking IDs
        self.last_episode_list_id = episode
        self.last_episode_file_id = file_id

        ep_dir = os.path.join(self._base, 'episode_data')
        cand1 = os.path.join(ep_dir, f'episode_{file_id}.csv')
        cand2 = os.path.join(ep_dir, f'episode_{file_id:03d}.csv')
        if os.path.exists(cand1):
            ep_path = cand1
        elif os.path.exists(cand2):
            ep_path = cand2
        else:
            raise FileNotFoundError(
                f"Episode file not found for id {file_id}. Tried:\n  {cand1}\n  {cand2}\n"
                f"Available ids (min..max): {self._available_ids[0]}..{self._available_ids[-1]}"
            )

        self.data = pd.read_csv(ep_path, index_col=0)
        return self.get_obs()

    def step(self, act):
        self.step_count += 1
        rew = 0
        done = 1 if self.step_count == self.episode_steps else 0

        if act != 0:
            if act == 1:
                new_loc = (self.agent_loc[0] - 1, self.agent_loc[1])
            elif act == 2:
                new_loc = (self.agent_loc[0], self.agent_loc[1] + 1)
            elif act == 3:
                new_loc = (self.agent_loc[0] + 1, self.agent_loc[1])
            elif act == 4:
                new_loc = (self.agent_loc[0], self.agent_loc[1] - 1)
            else:
                new_loc = self.agent_loc

            if new_loc in self.eligible_cells:
                self.agent_loc = new_loc
                rew += -1

        # pickup
        if (self.agent_load < self.agent_capacity) and (self.agent_loc in self.item_locs):
            self.agent_load += 1
            idx = self.item_locs.index(self.agent_loc)
            self.item_locs.pop(idx)
            self.item_times.pop(idx)
            rew += self.reward / 2

        # dropoff
        if self.agent_loc == self.target_loc and self.agent_load > 0:
            rew += self.agent_load * self.reward / 2
            self.agent_load = 0

        # age items
        self.item_times = [t + 1 for t in self.item_times]
        mask = [t < self.max_response_time for t in self.item_times]
        self.item_locs = list(compress(self.item_locs, mask))
        self.item_times = list(compress(self.item_times, mask))

        # add items for this step
        new_items_df = self.data[self.data.step == self.step_count]
        new_items = list(zip(new_items_df.vertical_idx, new_items_df.horizontal_idx))
        new_items = [loc for loc in new_items if loc not in self.item_locs]
        self.item_locs += new_items
        self.item_times += [0] * len(new_items)

        return rew, self.get_obs(), done

    def get_obs(self):
        return (self.step_count, self.agent_loc, self.agent_load, tuple(self.item_locs), tuple(self.item_times))


# ============================================================
# Constants / Precomputes (v0)
# ============================================================

HEIGHT = 5
WIDTH = 5
CHANNELS = 10
ACTIONS = 5

# fast valid actions by cell (v0 full grid)
VALID_ACTS_BY_CELL = []
VALID_MASK_BY_CELL = np.zeros((25, ACTIONS), dtype=bool)
for r in range(5):
    for c in range(5):
        v = [0]
        if r > 0: v.append(1)
        if c < 4: v.append(2)
        if r < 4: v.append(3)
        if c > 0: v.append(4)
        v = np.array(v, dtype=np.int32)
        VALID_ACTS_BY_CELL.append(v)
        VALID_MASK_BY_CELL[r * 5 + c, v] = True

# In v0 with base (2,0): max dist_agent+dist_base over grid = 14
MAX_POSSIBLE_SUM_DIST_V0 = 14.0


# ============================================================
# Distance structures
# ============================================================

def _build_distance_structs(env):
    if hasattr(env, "_dist_ready") and env._dist_ready:
        return

    if env.variant not in (0, 1):
        raise RuntimeError("This script is intended for variant 0 (full grid).")

    neighbor_matrix = np.zeros((25, 25), dtype=int)
    for i in range(25):
        for j in range(i + 1, 25):
            i_r, i_c = i // 5, i % 5
            j_r, j_c = j // 5, j % 5
            if (j_r - i_r == 0 and j_c - i_c == 1) or (j_r - i_r == 1 and j_c - i_c == 0):
                neighbor_matrix[i, j] = 1
                neighbor_matrix[j, i] = 1

    graph = csr_matrix(neighbor_matrix)
    dist_matrix, _ = dijkstra(csgraph=graph, directed=False, return_predecessors=True, unweighted=True)

    env._dist_matrix = dist_matrix

    dist_grids = []
    for idx in range(25):
        grid = np.zeros((5, 5), dtype=np.float32)
        for j in range(25):
            r = j // 5
            c = j % 5
            grid[r, c] = dist_matrix[idx, j]
        dist_grids.append(grid)
    env._dist_from_idx_grids = dist_grids

    base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
    idx_base = base_coord[0] * 5 + base_coord[1]
    env._dist_base_grid = dist_grids[idx_base]

    finite = dist_matrix[dist_matrix < np.inf]
    env._max_dist = float(np.max(finite)) if finite.size > 0 else 1.0
    if env._max_dist <= 0:
        env._max_dist = 1.0

    env._dist_ready = True


def _coord_to_idx(env, coord):
    return coord[0] * 5 + coord[1]


# ============================================================
# Encoding (Φ deliverability score)
# ============================================================

def encode_obs(obs, env):
    step, agent_loc, agent_load, item_locs, item_times = obs
    _build_distance_structs(env)

    state = np.zeros((5, 5, 10), dtype=np.float32)
    state[:, :, 6] = 1.0  # eligible all ones in v0

    ar, ac = agent_loc
    state[ar, ac, 0] = 1.0

    agent_idx = _coord_to_idx(env, agent_loc)
    dist_agent = env._dist_from_idx_grids[agent_idx]
    dist_base = env._dist_base_grid
    max_dist = float(getattr(env, "_max_dist", 1.0) or 1.0)

    state[:, :, 3] = dist_agent / max_dist
    state[:, :, 4] = dist_base / max_dist

    state[:, :, 7] = step / float(env.episode_steps)
    state[:, :, 8] = agent_load / float(env.agent_capacity)

    if len(item_locs) > 0:
        locs = np.array(item_locs, dtype=np.int32)
        ages = np.array(item_times, dtype=np.float32)
        rr = locs[:, 0]
        cc = locs[:, 1]

        ttl_norm = (float(env.max_response_time) - ages) / float(env.max_response_time)
        age_norm = ages / float(env.max_response_time)

        state[rr, cc, 1] = 1.0
        state[rr, cc, 2] = age_norm
        state[rr, cc, 5] = ttl_norm

        # Φ: ttl - (dist_agent + dist_base)/max_possible_sum
        dsum_items = (dist_agent[rr, cc] + dist_base[rr, cc]).astype(np.float32)
        score_items = ttl_norm - (dsum_items / MAX_POSSIBLE_SUM_DIST_V0)
        best_ttl = float(ttl_norm[np.argmax(score_items)])

        dsum_plane = (dist_agent + dist_base).astype(np.float32)
        phi_plane = best_ttl - (dsum_plane / MAX_POSSIBLE_SUM_DIST_V0)
        state[:, :, 9] = np.clip(phi_plane, -1.0, 1.0)

    return state.reshape(-1)


# ============================================================
# Replay buffers (PER) - capacity 30k
# ============================================================

class SumTree:
    def __init__(self, capacity: int):
        self.capacity = 1
        while self.capacity < capacity:
            self.capacity <<= 1
        self.tree = np.zeros(2 * self.capacity, dtype=np.float32)

    def total(self):
        return self.tree[1]

    def add(self, p: float, idx: int):
        i = idx + self.capacity
        delta = p - self.tree[i]
        self.tree[i] = p
        i //= 2
        while i >= 1:
            self.tree[i] += delta
            i //= 2

    def find_prefixsum_idx(self, mass: float) -> int:
        i = 1
        while i < self.capacity:
            left = 2 * i
            if self.tree[left] >= mass:
                i = left
            else:
                mass -= self.tree[left]
                i = left + 1
        return i - self.capacity


class PrioritizedReplayBuffer:
    def __init__(self, state_dim, capacity=30_000, alpha=0.6, beta0=0.4,
                 beta_final=1.0, beta_anneal_steps=120_000, eps=1e-6):
        self.capacity = capacity
        self.alpha = alpha
        self.eps = eps
        self.beta0 = beta0
        self.beta_final = beta_final
        self.beta_anneal_steps = max(1, beta_anneal_steps)
        self.ptr = 0
        self.n = 0

        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity,), dtype=np.int32)
        self.rewards = np.zeros((capacity,), dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros((capacity,), dtype=np.float32)

        self.sumtree = SumTree(capacity)
        self.max_priority = 1.0
        self._beta_updates = 0

    @property
    def beta(self):
        t = min(1.0, self._beta_updates / float(self.beta_anneal_steps))
        return (1.0 - t) * self.beta0 + t * self.beta_final

    def __len__(self):
        return self.n

    def add(self, s, a, r, ns, d):
        i = self.ptr
        self.states[i] = s
        self.actions[i] = a
        self.rewards[i] = r
        self.next_states[i] = ns
        self.dones[i] = d

        p = (self.max_priority + self.eps) ** self.alpha
        self.sumtree.add(p, i)

        self.ptr = (self.ptr + 1) % self.capacity
        self.n = min(self.n + 1, self.capacity)

    def sample(self, batch_size):
        total = self.sumtree.total()
        seg = total / max(1, batch_size)

        idxs = []
        for i in range(batch_size):
            mass = np.random.uniform(i * seg, (i + 1) * seg)
            idx = self.sumtree.find_prefixsum_idx(mass)
            if idx >= self.n:
                idx = np.random.randint(self.n)
            idxs.append(idx)

        idxs = np.array(idxs, dtype=np.int32)

        leaf_idx = idxs + self.sumtree.capacity
        p = self.sumtree.tree[leaf_idx]
        p = np.clip(p, 1e-12, None)
        P = p / max(1e-12, total)

        self._beta_updates += 1
        w = (self.n * P) ** (-self.beta)
        w = w / (w.max() + 1e-12)

        return (
            self.states[idxs],
            self.actions[idxs],
            self.rewards[idxs],
            self.next_states[idxs],
            self.dones[idxs],
            idxs,
            w.astype(np.float32)
        )

    def update_priorities(self, idxs, td_errors):
        prios = (np.abs(td_errors) + self.eps) ** self.alpha
        self.max_priority = max(self.max_priority, float(np.max(prios)))
        for i, p in zip(idxs, prios):
            self.sumtree.add(float(p), int(i))


# ============================================================
# Noisy Dense (kept available; not strictly required here)
# ============================================================

class NoisyDense(tf.keras.layers.Layer):
    def __init__(self, units, activation=None, sigma0=0.5, **kwargs):
        super(NoisyDense, self).__init__(**kwargs)
        self.units = units
        self.activation = tf.keras.activations.get(activation)
        self.sigma0 = float(sigma0)

    def build(self, input_shape):
        in_dim = int(input_shape[-1])
        mu_range = 1.0 / np.sqrt(in_dim)

        self.mu_w = self.add_weight(
            name="mu_w",
            shape=(in_dim, self.units),
            initializer=tf.keras.initializers.RandomUniform(-mu_range, mu_range),
            trainable=True,
        )
        self.sigma_w = self.add_weight(
            name="sigma_w",
            shape=(in_dim, self.units),
            initializer=tf.keras.initializers.Constant(self.sigma0 / np.sqrt(in_dim)),
            trainable=True,
        )
        self.mu_b = self.add_weight(
            name="mu_b",
            shape=(self.units,),
            initializer=tf.keras.initializers.RandomUniform(-mu_range, mu_range),
            trainable=True,
        )
        self.sigma_b = self.add_weight(
            name="sigma_b",
            shape=(self.units,),
            initializer=tf.keras.initializers.Constant(self.sigma0 / np.sqrt(in_dim)),
            trainable=True,
        )
        super(NoisyDense, self).build(input_shape)

    def call(self, inputs):
        def f(x):
            return tf.sign(x) * tf.sqrt(tf.abs(x))

        in_dim = tf.shape(self.mu_w)[0]
        eps_in = f(tf.random.normal((in_dim,)))
        eps_out = f(tf.random.normal((self.units,)))

        eps_w = tf.tensordot(eps_in, eps_out, axes=0)
        eps_b = eps_out

        w = self.mu_w + self.sigma_w * eps_w
        b = self.mu_b + self.sigma_b * eps_b

        out = tf.matmul(inputs, w) + b
        if self.activation is not None:
            out = self.activation(out)
        return out


# ============================================================
# DQN Agent (CNN + small FC trunk (units=256) + DUELING Q-MAP)
# ============================================================

from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Dense, Lambda, Reshape, Conv2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber


class DQN_Agent:
    def __init__(self, state_size, no_of_actions, agent_hyperparameters=None, old_model_path=''):
        if agent_hyperparameters is None:
            agent_hyperparameters = {}
        self.state_size = int(state_size)
        self.action_size = int(no_of_actions)

        self.gamma = float(agent_hyperparameters.get('gamma', 0.97))
        self.epsilon = float(agent_hyperparameters.get('epsilon', 1.0))
        self.batch_size = int(agent_hyperparameters.get('batch_size', 64))
        self.epsilon_min = float(agent_hyperparameters.get('epsilon_min', 0.02))
        self.epsilon_decay = float(agent_hyperparameters.get('epsilon_decay', 0.995))
        self.units = int(agent_hyperparameters.get('units', 256))

        self.use_per = bool(agent_hyperparameters.get('use_per', True))
        per_alpha = float(agent_hyperparameters.get('per_alpha', 0.6))
        per_beta0 = float(agent_hyperparameters.get('per_beta0', 0.4))
        per_beta1 = float(agent_hyperparameters.get('per_beta1', 1.0))
        per_eps = float(agent_hyperparameters.get('per_eps', 1e-6))
        per_anneal = int(agent_hyperparameters.get('per_beta_anneal_steps', 120_000))

        self.n_step = int(agent_hyperparameters.get('n_step', 1))
        self.gamma_n = self.gamma ** self.n_step

        self.use_noisy = bool(agent_hyperparameters.get('use_noisy', True))
        self.noisy_sigma0 = float(agent_hyperparameters.get('noisy_sigma0', 0.10))

        self.tau = float(agent_hyperparameters.get('tau', 0.005))

        self.model = self._build_model(self.state_size, self.action_size, self.units, old_model_path)
        self.target_model = self._build_model(self.state_size, self.action_size, self.units, '')
        self.target_model.set_weights(self.model.get_weights())

        self.memory = PrioritizedReplayBuffer(
            state_dim=self.state_size,
            capacity=30_000,
            alpha=per_alpha,
            beta0=per_beta0,
            beta_final=per_beta1,
            beta_anneal_steps=per_anneal,
            eps=per_eps
        ) if self.use_per else None

        self.learn_start = max(5000, 5 * self.batch_size)
        self.total_updates = 0
        self._n_step_buffer = deque(maxlen=self.n_step)

        self._huber_none = tf.keras.losses.Huber(delta=1.0, reduction=tf.keras.losses.Reduction.NONE)

    def _build_model(self, state_size, action_size, units, old_model_path=''):
        inp = Input(shape=(state_size,))
        grid = Reshape((HEIGHT, WIDTH, CHANNELS))(inp)
        agent_layer = Lambda(lambda x: x[:, :, :, 0:1], name="agent_layer")(grid)

        x = Conv2D(32, 3, padding='same', activation='relu')(grid)
        x = Conv2D(64, 3, padding='same', activation='relu')(x)

        # ---- units=256 trunk (per-cell, preserves Q-map idea) ----
        x = Conv2D(units, 1, padding='same', activation='relu', name="trunk_1x1")(x)

        # Dueling Q-map head
        v_map = Conv2D(1, 1, padding="same", name="v_map")(x)
        a_map = Conv2D(action_size, 1, padding="same", name="a_map")(x)
        a_mean = Lambda(lambda t: tf.reduce_mean(t, axis=-1, keepdims=True), name="a_mean")(a_map)
        q_map = Lambda(lambda va: va[0] + (va[1] - va[2]), name="q_map")([v_map, a_map, a_mean])

        def gather_agent_q(args):
            qmap, agent = args
            b = tf.shape(qmap)[0]
            Ht = tf.shape(qmap)[1]
            Wt = tf.shape(qmap)[2]
            flat = tf.reshape(agent, [b, Ht * Wt])
            idx_flat = tf.argmax(flat, axis=1, output_type=tf.int32)
            r = idx_flat // Wt
            c = idx_flat % Wt
            batch_idx = tf.range(b, dtype=tf.int32)
            gather_idx = tf.stack([batch_idx, r, c], axis=1)
            return tf.gather_nd(qmap, gather_idx)

        q = Lambda(gather_agent_q, name="q_agent")([q_map, agent_layer])

        model = Model(inputs=inp, outputs=q)
        model.compile(
            optimizer=Adam(learning_rate=3e-4, clipnorm=10.0),
            loss=Huber(delta=1.0),
            run_eagerly=False
        )
        if old_model_path:
            model.load_weights(old_model_path)
        return model

    def select_action(self, state, valid_actions, training=True):
        if training and np.random.rand() < self.epsilon:
            return int(np.random.choice(valid_actions))

        q = self.model(state, training=False).numpy()[0]
        masked_q = np.full_like(q, -1e9, dtype=np.float32)
        masked_q[valid_actions] = q[valid_actions]
        return int(np.argmax(masked_q))

    def _add_single_transition(self, state, action, reward, next_state, done):
        self.memory.add(state.reshape(-1), int(action), float(reward),
                        next_state.reshape(-1), float(done))

    def record(self, state, action, reward, next_state, done):
        if self.n_step <= 1:
            self._add_single_transition(state, action, reward, next_state, done)
            return

        self._n_step_buffer.append((state, action, reward, next_state, done))

        if len(self._n_step_buffer) == self.n_step and not done:
            self._flush_n_step()
            self._n_step_buffer.popleft()

        if done:
            while len(self._n_step_buffer) > 0:
                self._flush_n_step()
                self._n_step_buffer.popleft()

    def _flush_n_step(self):
        R = 0.0
        gamma = 1.0
        for (_, _, r, _, d) in self._n_step_buffer:
            R += gamma * float(r)
            gamma *= self.gamma
            if d:
                break

        state_0, action_0, _, _, _ = self._n_step_buffer[0]
        _, _, _, next_state_last, done_last = self._n_step_buffer[-1]
        self._add_single_transition(state_0, action_0, R, next_state_last, done_last)

    def update_epsilon(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def _soft_update_target(self):
        tau = float(self.tau)
        one_minus = 1.0 - tau
        for src, tgt in zip(self.model.weights, self.target_model.weights):
            tgt.assign(tau * src + one_minus * tgt)

    @tf.function
    def _train_step_weighted_q(self, S, A, Tgt, is_w):
        with tf.GradientTape() as tape:
            q_all = self.model(S, training=True)
            idx = tf.stack([tf.range(tf.shape(A)[0]), A], axis=1)
            q_sa = tf.gather_nd(q_all, idx)
            per_sample = self._huber_none(Tgt, q_sa)
            loss = tf.reduce_mean(is_w * per_sample)

        grads = tape.gradient(loss, self.model.trainable_variables)
        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))
        td = Tgt - q_sa
        return loss, td

    def update_weights(self):
        if len(self.memory) < self.learn_start:
            return

        S, A, R, NS, D, idxs, is_w = self.memory.sample(self.batch_size)

        grid = NS.reshape(self.batch_size, 5, 5, 10)
        agent_flat = grid[:, :, :, 0].reshape(self.batch_size, 25)
        idx_flat = agent_flat.argmax(axis=1)
        valid_next_mask = VALID_MASK_BY_CELL[idx_flat]

        NS_tf = tf.convert_to_tensor(NS, dtype=tf.float32)

        q_next_online = self.model(NS_tf, training=False).numpy()
        q_next_online[~valid_next_mask] = -1e9
        best_next_actions = np.argmax(q_next_online, axis=1)

        q_next_target = self.target_model(NS_tf, training=False).numpy()
        q_next_target[~valid_next_mask] = -1e9
        best_next_q = q_next_target[np.arange(self.batch_size), best_next_actions]

        Tgt_np = R + self.gamma_n * best_next_q * (1.0 - D)

        S_tf = tf.convert_to_tensor(S, dtype=tf.float32)
        A_tf = tf.convert_to_tensor(A, dtype=tf.int32)
        Tgt_tf = tf.convert_to_tensor(Tgt_np, dtype=tf.float32)
        w_tf = tf.convert_to_tensor(is_w, dtype=tf.float32)

        _, td_tf = self._train_step_weighted_q(S_tf, A_tf, Tgt_tf, w_tf)
        td_errors = td_tf.numpy()

        if self.use_per:
            self.memory.update_priorities(idxs, np.abs(td_errors))

        self._soft_update_target()
        self.total_updates += 1


# ============================================================
# Gym wrapper (Late turbo schedule)
# ============================================================

class GymEnvironment:
    def __init__(self, save_path, variant=0, data_dir="./data"):
        self.env = Environment(variant=variant, data_dir=data_dir)
        self.variant = variant
        self.max_timesteps = getattr(self.env, "episode_steps", 200)
        self.save_path = save_path

    def trainDQN(self, agent, no_episodes):
        rew = self.runDQN(agent, no_episodes, training=True)
        agent.model.save_weights(
            os.path.join(self.save_path, f"dueling_double_dqn_variant{self.variant}.weights.h5"),
            overwrite=True
        )
        return rew

    def runDQN(self, agent, no_episodes, training=False):
        rew = np.zeros(no_episodes, dtype=np.float32)

        for episode in range(no_episodes):

            # ---- LATE TURBO SCHEDULE (requested) ----
            if training:
                if episode < 200:
                    update_every = 12
                    per_update_steps = 1
                elif episode < 450:
                    update_every = 8
                    per_update_steps = 2
                else:
                    update_every = 4
                    per_update_steps = 3
            else:
                update_every = 4
                per_update_steps = 1

            mode = "training" if training else "validation"
            obs = self.env.reset(mode=mode)
            state_vec = encode_obs(obs, self.env).reshape(1, -1)

            done = 0
            rwd = 0.0
            t = 0
            step_since_update = 0

            while not done and t < self.max_timesteps:
                ar, ac = self.env.agent_loc
                valid_actions = VALID_ACTS_BY_CELL[ar * 5 + ac]

                action = agent.select_action(state_vec, valid_actions=valid_actions, training=training)

                reward, next_obs, done = self.env.step(action)
                next_state_vec = encode_obs(next_obs, self.env).reshape(1, -1)
                rwd += reward

                terminal_flag = float(done)

                if training:
                    agent.record(state_vec, action, reward, next_state_vec, terminal_flag)
                    step_since_update += 1
                    if step_since_update % update_every == 0:
                        for _ in range(per_update_steps):
                            agent.update_weights()

                state_vec = next_state_vec
                t += 1

            rew[episode] = rwd

            if training and (episode + 1) % 25 == 0:
                start = max(0, episode + 1 - 25)
                avg25 = np.mean(rew[start:episode + 1])
                print(f"[TRAIN] Ep {episode + 1}/{no_episodes} Return: {rwd:.1f} | Avg25: {avg25:.1f}")

            if training:
                agent.update_epsilon()

        return rew


# ============================================================
# Checks: val/test overlap + uniqueness + exact CSV set usage
# ============================================================

def check_val_test_overlap(variant, data_dir):
    env = Environment(variant, data_dir)

    val_list = list(env.validation_episodes)
    test_list = list(env.test_episodes)

    val_set = set(val_list)
    test_set = set(test_list)

    print("VAL size:", len(val_set))
    print("TEST size:", len(test_set))
    print("overlap(list ids):", len(val_set & test_set))
    print("same set(list ids):", val_set == test_set)
    print("examples overlap(list ids):", list(val_set & test_set)[:10])

    # --- IMPORTANT: also compare the *actual* file ids after offset mapping ---
    def to_file_id(ep):
        return ep if ep in env._available_set else ep + env._ep_min

    val_files = set(map(to_file_id, val_list))
    test_files = set(map(to_file_id, test_list))

    print("overlap(file ids):", len(val_files & test_files))
    print("same set(file ids):", val_files == test_files)
    print("examples overlap(file ids):", list(val_files & test_files)[:10])


def verify_eval_unique(variant, data_dir, mode, expected_count=100):
    env = Environment(variant, data_dir)
    seen_list = []
    seen_file = []

    for _ in range(expected_count):
        env.reset(mode)
        seen_list.append(env.last_episode_list_id)
        seen_file.append(env.last_episode_file_id)

    u_list = len(set(seen_list))
    u_file = len(set(seen_file))
    print(f"[VERIFY {mode.upper()}] requested={expected_count} | unique_list_ids={u_list} | unique_file_ids={u_file}")

    if u_list != expected_count:
        dup = [k for k, v in Counter(seen_list).items() if v > 1]
        print(f"[VERIFY {mode.upper()}] DUPLICATES in list ids:", dup[:10])

    if u_file != expected_count:
        dup = [k for k, v in Counter(seen_file).items() if v > 1]
        print(f"[VERIFY {mode.upper()}] DUPLICATES in file ids:", dup[:10])

    env2 = Environment(variant, data_dir)
    target = set(env2.validation_episodes) if mode == "validation" else set(env2.test_episodes)
    got = set(seen_list)
    print(f"[VERIFY {mode.upper()}] matches csv set: {got == target}")
    if got != target:
        print("missing:", list(target - got)[:10])
        print("extra:", list(got - target)[:10])


# ============================================================
# Evaluation (avg reward)
# ============================================================

def evaluate_average_reward(agent, variant, data_dir, episodes=100, mode="testing"):
    env = Environment(variant, data_dir)
    max_steps = min(getattr(env, "episode_steps", 200), 200)

    if mode == "testing":
        max_eps = min(episodes, len(env.test_episodes))
    elif mode == "validation":
        max_eps = min(episodes, len(env.validation_episodes))
    else:
        max_eps = episodes

    total = 0.0
    seen = []

    for _ in range(max_eps):
        obs = env.reset(mode)
        seen.append(env.last_episode_file_id)

        s_vec = encode_obs(obs, env).reshape(1, -1)
        done = 0
        ep_ret = 0.0
        steps = 0

        while not done and steps < max_steps:
            ar, ac = env.agent_loc
            valid_actions = VALID_ACTS_BY_CELL[ar * 5 + ac]
            a = agent.select_action(s_vec, valid_actions=valid_actions, training=False)

            r, nxt, done = env.step(a)
            s_vec = encode_obs(nxt, env).reshape(1, -1)
            ep_ret += r
            steps += 1

        total += ep_ret

    print(f"[{mode.upper()}] ran={max_eps} unique_file_ids={len(set(seen))}")
    return total / float(max_eps)


# ============================================================
# Main
# ============================================================

if __name__ == "__main__":
    VARIANT = 0
    TRAIN_EPISODES_FULL = 800
    VAL_EPISODES_FINAL = 100
    TEST_EPISODES_FINAL = 100
    NO_OF_ACTIONS = 5
    DATA_DIR = "./data"

    BASE_CONFIG = dict(
        gamma=0.97,
        epsilon=1.0,
        batch_size=64,      # keep fast; change to 128 if GPU and you want
        epsilon_min=0.02,
        epsilon_decay=0.995,
        units=256,          # requested
        tau=0.005,
        use_per=True,
        per_alpha=0.6,
        per_beta0=0.4,
        per_beta1=1.0,
        per_eps=1e-6,
        per_beta_anneal_steps=120_000,
        n_step=1,
        use_noisy=True,
        noisy_sigma0=0.10,
        variant_name="variant0_fast_late_turbo_units256",
    )

    wd = os.getcwd()
    save_folder = os.path.join(wd, 'save_folder_variant0_fast_late_turbo_units256')
    os.makedirs(save_folder, exist_ok=True)

    # ---- checks (fast, no training) ----
    check_val_test_overlap(VARIANT, DATA_DIR)
    verify_eval_unique(VARIANT, DATA_DIR, mode="validation", expected_count=100)
    verify_eval_unique(VARIANT, DATA_DIR, mode="testing", expected_count=100)

    # infer state size
    env_tmp = Environment(VARIANT, DATA_DIR)
    obs0 = env_tmp.reset("training")
    state_sample = encode_obs(obs0, env_tmp)
    state_size = state_sample.shape[0]
    print(f"Variant {VARIANT}: inferred state_size={state_size}, action_size={NO_OF_ACTIONS}")

    agent = DQN_Agent(state_size, NO_OF_ACTIONS, BASE_CONFIG)

    env_train = GymEnvironment(
        save_path=save_folder,
        variant=VARIANT,
        data_dir=DATA_DIR
    )

    rew_train = env_train.trainDQN(agent, TRAIN_EPISODES_FULL)

    avg_val = evaluate_average_reward(agent, VARIANT, DATA_DIR, episodes=VAL_EPISODES_FINAL, mode="validation")
    avg_test = evaluate_average_reward(agent, VARIANT, DATA_DIR, episodes=TEST_EPISODES_FINAL, mode="testing")

    print(f"[Variant {VARIANT}] Validation avg over {VAL_EPISODES_FINAL} episodes: {avg_val:.3f}")
    print(f"[Variant {VARIANT}] Test       avg over {TEST_EPISODES_FINAL} episodes: {avg_test:.3f}")

    # plot
    plt.figure(figsize=(14, 6))
    window = 25
    kernel = np.ones(window, dtype=np.float32) / float(window)
    r = np.array(rew_train, dtype=np.float32)
    x = np.arange(1, len(r) + 1)

    raw_line, = plt.plot(x, r, alpha=0.20, label="Train (raw)")
    if len(r) >= window:
        mov = np.convolve(r, kernel, mode="valid")
        xm = np.arange(window, len(r) + 1)
        plt.plot(xm, mov, label="Train (25-ep avg)", color=raw_line.get_color())

    plt.axhline(y=avg_val, linestyle=":", alpha=0.9, label=f"Val avg ({avg_val:.1f})")
    plt.axhline(y=avg_test, linestyle="--", alpha=0.9, label=f"Test avg ({avg_test:.1f})")

    plt.title("Variant 0 – FAST Double DQN + PER(30k) + DUELING Q-MAP + Φ deliverability\n"
          "Late-turbo updates + units=256 + n-step=1")

    plt.xlabel("Training episode")
    plt.ylabel("Reward")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()

    out_plot = os.path.join(save_folder, "variant0_fast_late_turbo_units256.png")
    plt.savefig(out_plot, dpi=150)
    print(f"\nSaved plot to: {out_plot}")
    plt.show()
