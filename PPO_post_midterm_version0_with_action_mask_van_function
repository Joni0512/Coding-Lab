
import random
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.distributions import Categorical

from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import dijkstra

from env_midterm import Environment

# ============================================================
# Utilities
# ============================================================

def set_global_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def valid_action_mask(env, n_actions=5):
    mask = np.zeros(n_actions, dtype=np.bool_)
    mask[0] = True  # idle always allowed

    r, c = env.agent_loc
    H, W = env.vertical_cell_count, env.horizontal_cell_count

    moves = [
        (1, -1, 0),  # up
        (2, 0, 1),   # right
        (3, 1, 0),   # down
        (4, 0, -1),  # left
    ]
    for act, dr, dc in moves:
        nr, nc = r + dr, c + dc
        if 0 <= nr < H and 0 <= nc < W and (nr, nc) in env.eligible_cells:
            mask[act] = True

    return mask

def valid_action_mask_from_obs_batch(obs_batch, n_actions=5):
    """
    obs_batch: torch tensor [B, 225] with channels-first encoding (C,H,W) flattened.
    Uses:
      channel 0: agent position one-hot
      channel 6: eligible mask
    Returns: torch.bool tensor [B, n_actions]
    """
    device = obs_batch.device
    B = obs_batch.shape[0]

    grid = obs_batch.view(B, CHANNELS, HEIGHT, WIDTH)
    agent = grid[:, 0]                 # [B,H,W]
    eligible = grid[:, 6] > 0.5        # [B,H,W] bool

    mask = torch.zeros((B, n_actions), dtype=torch.bool, device=device)
    mask[:, 0] = True  # idle always valid

    # find agent position (row, col) per batch element
    pos = agent.view(B, -1).argmax(dim=1)   # 0..H*W-1
    ar = (pos // WIDTH).long()
    ac = (pos % WIDTH).long()

    b_idx = torch.arange(B, device=device)

    # ---- UP (action 1): ar-1 ----
    ok = (ar > 0)
    if ok.any():
        b = b_idx[ok]
        r = ar[ok] - 1
        c = ac[ok]
        ok2 = eligible[b, r, c]
        mask[b[ok2], 1] = True

    # ---- RIGHT (action 2): ac+1 ----
    ok = (ac < WIDTH - 1)
    if ok.any():
        b = b_idx[ok]
        r = ar[ok]
        c = ac[ok] + 1
        ok2 = eligible[b, r, c]
        mask[b[ok2], 2] = True

    # ---- DOWN (action 3): ar+1 ----
    ok = (ar < HEIGHT - 1)
    if ok.any():
        b = b_idx[ok]
        r = ar[ok] + 1
        c = ac[ok]
        ok2 = eligible[b, r, c]
        mask[b[ok2], 3] = True

    # ---- LEFT (action 4): ac-1 ----
    ok = (ac > 0)
    if ok.any():
        b = b_idx[ok]
        r = ar[ok]
        c = ac[ok] - 1
        ok2 = eligible[b, r, c]
        mask[b[ok2], 4] = True

    return mask

# ============================================================
# CNN SHAPE
# ============================================================

CHANNELS = 10
HEIGHT = 5
WIDTH = 5


# ============================================================
# Distance structures
# ============================================================

def _build_distance_structs(env):
    if hasattr(env, "_dist_ready") and env._dist_ready:
        return

    variant = env.variant
    H, W = 5, 5

    if variant in (0, 1):
        neighbor_matrix = np.zeros((25, 25), dtype=int)
        for i in range(25):
            for j in range(i + 1, 25):
                i_r, i_c = i // 5, i % 5
                j_r, j_c = j // 5, j % 5
                if (j_r - i_r == 0 and j_c - i_c == 1) or (j_r - i_r == 1 and j_c - i_c == 0):
                    neighbor_matrix[i, j] = 1
                    neighbor_matrix[j, i] = 1

        graph = csr_matrix(neighbor_matrix)
        dist_matrix, _ = dijkstra(csgraph=graph, directed=False, return_predecessors=True, unweighted=True)

        env._mapping = None
        env._coord_to_idx = None
        env._dist_matrix = dist_matrix

        dist_grids = []
        for idx in range(25):
            grid = np.zeros((H, W), dtype=np.float32)
            for j in range(25):
                r = j // 5
                c = j % 5
                grid[r, c] = dist_matrix[idx, j]
            dist_grids.append(grid)
        env._dist_from_idx_grids = dist_grids

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        idx_base = base_coord[0] * 5 + base_coord[1]
        env._dist_base_grid = dist_grids[idx_base]

        finite = dist_matrix[dist_matrix < np.inf]
        env._max_dist = float(np.max(finite)) if finite.size > 0 else 0.0

    else:
        mapping = [
            (0, 0), (1, 0), (2, 0), (3, 0), (4, 0),
            (3, 1), (4, 1),
            (0, 2), (1, 2), (2, 2), (3, 2), (4, 2),
            (0, 3),
            (0, 4), (1, 4), (2, 4), (3, 4), (4, 4)
        ]
        env._mapping = mapping
        env._coord_to_idx = {coord: i for i, coord in enumerate(mapping)}

        neighbor_matrix = np.zeros((18, 18), dtype=int)

        def link(i, j):
            neighbor_matrix[i, j] = 1
            neighbor_matrix[j, i] = 1

        link(0, 1)
        link(1, 2)
        link(2, 3)
        link(3, 4)
        link(3, 5)
        link(4, 6)
        link(5, 6)
        link(5, 10)
        link(6, 11)
        link(7, 8)
        link(7, 12)
        link(8, 9)
        link(9, 10)
        link(10, 11)
        link(12, 13)
        link(13, 14)
        link(14, 15)
        link(15, 16)
        link(16, 17)

        graph = csr_matrix(neighbor_matrix)
        dist_matrix, _ = dijkstra(csgraph=graph, directed=False, return_predecessors=True, unweighted=True)
        env._dist_matrix = dist_matrix

        dist_grids = []
        for idx in range(len(mapping)):
            grid = np.zeros((H, W), dtype=np.float32)
            for j, coord in enumerate(mapping):
                r, c = coord
                grid[r, c] = dist_matrix[idx, j]
            dist_grids.append(grid)
        env._dist_from_idx_grids = dist_grids

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        idx_base = env._coord_to_idx[base_coord]
        env._dist_base_grid = dist_grids[idx_base]

        finite = dist_matrix[dist_matrix < np.inf]
        env._max_dist = float(np.max(finite)) if finite.size > 0 else 0.0

    env._dist_ready = True

def _coord_to_idx(env, coord):
    if env.variant in (0, 1):
        return coord[0] * 5 + coord[1]
    return env._coord_to_idx[coord]

# ============================================================
# PPO observation encoding (EXTERNAL now, CHW flatten!)
# ============================================================

def encode_obs(obs, env, feature_mode="full"):
    """
    Returns flat vector length 10*5*5 = 250.

    IMPORTANT:
    PPO net does view(-1, C, H, W), so we MUST flatten in (C,H,W) order.
    """
    step, agent_loc, agent_load, item_locs, item_times = obs

    _build_distance_structs(env)

    H, W, C = HEIGHT, WIDTH, CHANNELS
    state = np.zeros((C, H, W), dtype=np.float32)
    ALPHA = 1.0
    BETA = 1.0

    # 6: eligible mask
    if env.variant in (0, 1):
        state[6, :, :] = 1.0
    else:
        eligible_set = set(env.eligible_cells)
        for r in range(H):
            for c in range(W):
                state[6, r, c] = 1.0 if (r, c) in eligible_set else 0.0

    # 0: agent position
    ar, ac = agent_loc
    state[0, ar, ac] = 1.0

    # 1,2,5: items (same semantics as your old PPO env)
    for (loc, age) in zip(item_locs, item_times):
        r, c = loc
        state[1, r, c] = 1.0
        state[2, r, c] = age / env.max_response_time
        state[5, r, c] = (env.max_response_time - age) / env.max_response_time

    # 3: distance agent->cell
    agent_idx = _coord_to_idx(env, agent_loc)
    dist_agent_grid = env._dist_from_idx_grids[agent_idx]

    max_dist = getattr(env, "_max_dist", 0.0)
    if max_dist and max_dist > 0:
        state[3, :, :] = dist_agent_grid / max_dist
        state[4, :, :] = env._dist_base_grid / max_dist
    else:
        state[3, :, :] = 0.0
        state[4, :, :] = 0.0

    # 7: step norm
    state[7, :, :] = step / float(env.episode_steps)

    # 8: load norm
    state[8, :, :] = agent_load / float(env.agent_capacity)

    # 9: Φ potential feature
    phi_plane = np.zeros((H, W), dtype=np.float32)

    if len(item_locs) > 0:
        agent_idx = _coord_to_idx(env, agent_loc)
        dist_agent_grid = env._dist_from_idx_grids[agent_idx]
        max_dist = env._max_dist if env._max_dist > 0 else 1.0

        best_phi = -np.inf
        best_ttl = 0.0

        for (loc, age) in zip(item_locs, item_times):
            r, c = loc
            d = dist_agent_grid[r, c] / max_dist
            ttl = (env.max_response_time - age) / env.max_response_time
            phi = -ALPHA * d + BETA * ttl
            if phi > best_phi:
                best_phi = phi
                best_ttl = ttl

        # spatialized Φ
        phi_plane = -ALPHA * (dist_agent_grid / max_dist) + BETA * best_ttl
        phi_plane = np.clip(phi_plane, -1.0, 1.0)

    state[9, :, :] = phi_plane

    # -------- feature_mode toggles (same behavior as your old PPO env) --------
    fm = feature_mode
    if fm == "full":
        pass
    elif fm == "reduced_full":
        state[2, :, :] = 0.0
        state[7, :, :] = 0.0
    elif fm == "no_distance":
        state[3, :, :] = 0.0
        state[4, :, :] = 0.0
    elif fm == "expiry_focused":
        state[2, :, :] = 0.0
        threshold = 3
        for (loc, age) in zip(item_locs, item_times):
            r, c = loc
            if env.max_response_time - age <= threshold:
                state[2, r, c] = 1.0
    elif fm == "minimal":
        keep = {0, 1, 5, 8}
        for ch in range(C):
            if ch not in keep:
                state[ch, :, :] = 0.0
    else:
        raise ValueError(f"Invalid feature_mode: {fm}")

    return state.reshape(-1)  # CHW flatten


# ============================================================
# Noisy Linear Layer (PyTorch NoisyNets)
# ============================================================

class NoisyLinear(nn.Module):
    def __init__(self, in_features, out_features, sigma_init=0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features

        bound = 1.0 / np.sqrt(in_features)
        self.mu_weight = nn.Parameter(torch.empty(in_features, out_features).uniform_(-bound, bound))
        self.mu_bias = nn.Parameter(torch.empty(out_features).uniform_(-bound, bound))

        self.sigma_weight = nn.Parameter(torch.full((in_features, out_features), sigma_init / np.sqrt(in_features)))
        self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init / np.sqrt(in_features)))

    @staticmethod
    def f(x):
        return torch.sign(x) * torch.sqrt(torch.abs(x))

    def forward(self, x):
        if not self.training:
            # deterministic: use only mu
            return x @ self.mu_weight + self.mu_bias

        eps_in = torch.randn(self.in_features, device=x.device)
        eps_out = torch.randn(self.out_features, device=x.device)
        eps_in = self.f(eps_in)
        eps_out = self.f(eps_out)
        eps_w = torch.ger(eps_in, eps_out)
        eps_b = eps_out
        weight = self.mu_weight + self.sigma_weight * eps_w
        bias = self.mu_bias + self.sigma_bias * eps_b
        return x @ weight + bias


# ============================================================
# CNN Policy & Value Nets
# ============================================================

class PolicyNet(nn.Module):
    def __init__(self, obs_dim, n_actions, hidden_dim=128):
        super().__init__()
        self.channels = CHANNELS
        self.height = HEIGHT
        self.width = WIDTH
        assert obs_dim == self.channels * self.height * self.width

        self.conv1 = nn.Conv2d(self.channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        conv_out_dim = 64 * self.height * self.width

        self.fc1 = NoisyLinear(conv_out_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.logits = nn.Linear(hidden_dim, n_actions)

    def forward(self, x):
        x = x.view(-1, self.channels, self.height, self.width)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.ln1(x)
        x = F.relu(x)
        return self.logits(x)


class ValueNet(nn.Module):
    def __init__(self, obs_dim, hidden_dim=128):
        super().__init__()
        self.channels = CHANNELS
        self.height = HEIGHT
        self.width = WIDTH
        assert obs_dim == self.channels * self.height * self.width

        self.conv1 = nn.Conv2d(self.channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        conv_out_dim = 64 * self.height * self.width

        self.fc1 = nn.Linear(conv_out_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.v = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = x.view(-1, self.channels, self.height, self.width)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.ln1(x)
        x = F.relu(x)
        return self.v(x).squeeze(-1)


# ============================================================
# PPO Buffer
# ============================================================

class PPOBuffer:
    def __init__(self, obs_dim, device):
        self.obs_dim = obs_dim
        self.device = device
        self.reset()

    def reset(self):
        self.obs = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.log_probs = []
        self.values = []

    def store(self, obs, action, reward, done, log_prob, value):
        self.obs.append(np.array(obs, dtype=np.float32))
        self.actions.append(int(action))
        self.rewards.append(float(reward))
        self.dones.append(float(done))
        self.log_probs.append(float(log_prob))
        self.values.append(float(value))

    def compute_advantages(self, gamma, lam):
        rewards = torch.tensor(self.rewards, dtype=torch.float32, device=self.device)
        values = torch.tensor(self.values, dtype=torch.float32, device=self.device)
        dones = torch.tensor(self.dones, dtype=torch.float32, device=self.device)

        T = len(rewards)
        advantages = torch.zeros(T, dtype=torch.float32, device=self.device)
        gae = 0.0
        next_value = 0.0

        for t in reversed(range(T)):
            mask = 1.0 - dones[t]
            delta = rewards[t] + gamma * next_value * mask - values[t]
            gae = delta + gamma * lam * mask * gae
            advantages[t] = gae
            next_value = values[t]

        self.advantages = advantages
        self.returns = advantages + values

    def get(self):
        obs_np = np.asarray(self.obs, dtype=np.float32)
        actions_np = np.asarray(self.actions, dtype=np.int64)
        logp_np = np.asarray(self.log_probs, dtype=np.float32)

        obs = torch.as_tensor(obs_np, dtype=torch.float32, device=self.device)
        actions = torch.as_tensor(actions_np, dtype=torch.long, device=self.device)
        log_probs = torch.as_tensor(logp_np, dtype=torch.float32, device=self.device)

        return obs, actions, log_probs, self.advantages, self.returns


# ============================================================
# PPO Agent
# ============================================================

class PPOAgent:
    def __init__(
        self,
        obs_dim,
        n_actions,
        gamma=0.9842694774795389,
        lam=0.8547793488849357,
        clip_ratio=0.07098308442073593,
        pi_lr=0.0004018661633092908,
        vf_lr=0.00022102510315199418,
        train_epochs=15,
        minibatch_size=32,
        entropy_coef=0.008662172739153698,
        device="cpu",
    ):
        self.obs_dim = obs_dim
        self.n_actions = n_actions
        self.gamma = gamma
        self.lam = lam
        self.clip_ratio = clip_ratio
        self.train_epochs = train_epochs
        self.minibatch_size = minibatch_size
        self.entropy_coef = entropy_coef
        self.device = torch.device(device)

        self.policy = PolicyNet(obs_dim, n_actions).to(self.device)
        self.value_fn = ValueNet(obs_dim).to(self.device)

        self.pi_optimizer = Adam(self.policy.parameters(), lr=pi_lr)
        self.vf_optimizer = Adam(self.value_fn.parameters(), lr=vf_lr)

    @torch.no_grad()
    @torch.no_grad()
    def select_action(self, obs_vec, env=None, eval_mode=False):

        # ---- TOGGLE NOISYNET BEHAVIOR ----
        if eval_mode:
            self.policy.eval()
            self.value_fn.eval()
        else:
            self.policy.train()
            self.value_fn.train()

        obs_t = torch.as_tensor(obs_vec, dtype=torch.float32, device=self.device).unsqueeze(0)

        logits = self.policy(obs_t)  # shape [1, n_actions]

        # ---- ACTION MASKING ----
        if env is not None:
            mask_np = valid_action_mask(env, self.n_actions)  # (A,)
            mask_t = torch.as_tensor(mask_np, device=self.device).unsqueeze(0)  # (1, A)
            logits = logits.masked_fill(~mask_t, -1e9)  # forbid invalid actions

        dist = Categorical(logits=logits)

        if eval_mode:
            action = torch.argmax(dist.probs, dim=-1)
        else:
            action = dist.sample()

        log_prob = dist.log_prob(action)
        value = self.value_fn(obs_t)

        return int(action.item()), float(log_prob.item()), float(value.item())

    def update(self, buffer: PPOBuffer):
        self.policy.train()
        self.value_fn.train()
        obs, actions, old_log_probs, advantages, returns = buffer.get()
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        N = obs.size(0)
        idxs = np.arange(N)

        for _ in range(self.train_epochs):
            np.random.shuffle(idxs)
            for start in range(0, N, self.minibatch_size):
                mb_idx = idxs[start:start + self.minibatch_size]
                mb_idx_t = torch.as_tensor(mb_idx, dtype=torch.long, device=self.device)

                mb_obs = obs[mb_idx_t]
                mb_actions = actions[mb_idx_t]
                mb_old_logp = old_log_probs[mb_idx_t]
                mb_adv = advantages[mb_idx_t]
                mb_returns = returns[mb_idx_t]
                logits = self.policy(mb_obs)

                mask = valid_action_mask_from_obs_batch(mb_obs, self.n_actions)  # [B,A]
                logits = logits.masked_fill(~mask, -1e9)

                dist = Categorical(logits=logits)
                dist = Categorical(logits=logits)
                new_logp = dist.log_prob(mb_actions)
                entropy = dist.entropy().mean()

                ratio = torch.exp(new_logp - mb_old_logp)
                unclipped = ratio * mb_adv
                clipped = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * mb_adv
                policy_loss = -torch.min(unclipped, clipped).mean()

                values = self.value_fn(mb_obs)
                value_loss = F.mse_loss(values, mb_returns)

                self.pi_optimizer.zero_grad()
                (policy_loss - self.entropy_coef * entropy).backward()
                self.pi_optimizer.step()

                self.vf_optimizer.zero_grad()
                value_loss.backward()
                self.vf_optimizer.step()


# ============================================================
# Train + Val + Test
# ============================================================

def summarize_returns(name, returns):
    arr = np.asarray(returns, dtype=np.float32)
    n = len(arr)
    print(f"\n=== {name} statistics ===")
    if n == 0:
        print("No episodes recorded.")
        print("================================\n")
        return
    best_ret = float(arr.max())
    best_ep = int(arr.argmax()) + 1
    mean_all = float(arr.mean())
    std_all = float(arr.std())
    window = min(100, n)
    mean_last = float(arr[-window:].mean())
    print(f"Episodes total:              {n}")
    print(f"Best episode:                {best_ep}  (return = {best_ret:.2f})")
    print(f"Mean return (all):           {mean_all:.2f} ± {std_all:.2f}")
    print(f"Mean return (last {window}): {mean_last:.2f}")
    print("================================\n")


def train_and_test_ppo(
    data_dir,
    variant=1,
    feature_mode="full",
    train_episodes=800,
    validation_episodes=100,
    test_episodes=100,
    gamma=0.9842694774795389,
    lam=0.8547793488849357,
    clip_ratio=0.07098308442073593,
    pi_lr=0.0004018661633092908,
    vf_lr=0.00022102510315199418,
    train_epochs=15,
    minibatch_size=32,
    entropy_coef=0.008662172739153698,
    device="cpu",
    print_interval=25,
    seed=7,
    show_plot=True,
):
    set_global_seed(seed)

    env = Environment(variant=variant, data_dir=data_dir)
    n_actions = 5

    raw0 = env.reset(mode="training")
    obs_vec0 = encode_obs(raw0, env, feature_mode=feature_mode)
    obs_dim = obs_vec0.shape[0]
    print(f"Obs_dim={obs_dim}, n_actions={n_actions}, feature_mode='{feature_mode}'")

    agent = PPOAgent(
        obs_dim=obs_dim,
        n_actions=n_actions,
        gamma=gamma,
        lam=lam,
        clip_ratio=clip_ratio,
        pi_lr=pi_lr,
        vf_lr=vf_lr,
        train_epochs=train_epochs,
        minibatch_size=minibatch_size,
        entropy_coef=entropy_coef,
        device=device,
    )

    buffer = PPOBuffer(obs_dim=obs_dim, device=agent.device)
    train_returns = []

    # -------- TRAIN --------
    for ep in range(train_episodes):
        raw = env.reset(mode="training")
        obs_vec = encode_obs(raw, env, feature_mode=feature_mode)

        done = False
        ep_ret = 0.0
        buffer.reset()

        while not done:
            action, logp, value = agent.select_action(obs_vec, env=env, eval_mode=False)
            rew, raw_next, done_flag = env.step(action)
            obs_next_vec = encode_obs(raw_next, env, feature_mode=feature_mode)

            buffer.store(obs_vec, action, rew, done_flag, logp, value)

            obs_vec = obs_next_vec
            ep_ret += rew
            done = bool(done_flag)

        buffer.compute_advantages(gamma=gamma, lam=lam)
        agent.update(buffer)
        train_returns.append(ep_ret)

        if (ep + 1) % print_interval == 0:
            avgN = float(np.mean(train_returns[-print_interval:]))
            print(f"[TRAIN] Episode {ep+1}/{train_episodes}  Return: {ep_ret:.1f}  Avg(last {print_interval}): {avgN:.1f}")

    # -------- VAL (eval only) --------
    val_returns = []
    max_val_eps = min(validation_episodes, len(env.validation_episodes))
    for ep in range(max_val_eps):
        raw = env.reset(mode="validation")
        obs_vec = encode_obs(raw, env, feature_mode=feature_mode)

        done = False
        ep_ret = 0.0
        while not done:
            action, _, _ = agent.select_action(obs_vec, env=env, eval_mode=True)
            rew, raw_next, done_flag = env.step(action)
            obs_vec = encode_obs(raw_next, env, feature_mode=feature_mode)
            ep_ret += rew
            done = bool(done_flag)

        val_returns.append(ep_ret)
        if (ep + 1) % print_interval == 0:
            avgN = float(np.mean(val_returns[-min(print_interval, ep+1):]))
            print(f"[VAL] Episode {ep+1}/{max_val_eps}  Return: {ep_ret:.1f}  Avg(last {min(print_interval, ep+1)}): {avgN:.1f}")

    # -------- TEST --------
    test_returns = []
    max_test_eps = min(test_episodes, len(env.test_episodes))
    for ep in range(max_test_eps):
        raw = env.reset(mode="testing")
        obs_vec = encode_obs(raw, env, feature_mode=feature_mode)

        done = False
        ep_ret = 0.0
        while not done:
            action, _, _ = agent.select_action(obs_vec, env=env, eval_mode=True)
            rew, raw_next, done_flag = env.step(action)
            obs_vec = encode_obs(raw_next, env, feature_mode=feature_mode)
            ep_ret += rew
            done = bool(done_flag)

        test_returns.append(ep_ret)
        if (ep + 1) % print_interval == 0:
            avgN = float(np.mean(test_returns[-min(print_interval, ep+1):]))
            print(f"[TEST] Episode {ep+1}/{max_test_eps}  Return: {ep_ret:.1f}  Avg(last {min(print_interval, ep+1)}): {avgN:.1f}")

    if show_plot:
        # Training curve
        episodes_train = np.arange(1, train_episodes + 1)
        plt.figure()
        plt.plot(episodes_train, train_returns, label="Train return")
        if train_episodes >= print_interval:
            kernel = np.ones(print_interval) / print_interval
            mov_avg = np.convolve(train_returns, kernel, mode="valid")
            plt.plot(np.arange(print_interval, train_episodes + 1), mov_avg, label=f"{print_interval}-ep avg")
        plt.xlabel("Episode")
        plt.ylabel("Return")
        plt.title("PPO – Training Returns (external encoder, DQN-style env)")
        plt.legend()
        plt.tight_layout()
        plt.show()

        # Full 1000 vs benchmark
        all_returns = train_returns + val_returns + test_returns
        ep_idx = np.arange(1, len(all_returns) + 1)
        plt.figure()
        plt.plot(ep_idx, all_returns, label="Episode return")
        benchmark = 385.425
        plt.axhline(y=benchmark, linestyle="--", label=f"Benchmark {benchmark:.3f}")
        plt.axvline(x=len(train_returns), linestyle=":", label="Train/Val boundary")
        plt.axvline(x=len(train_returns) + len(val_returns), linestyle=":", label="Val/Test boundary")
        plt.xlabel("Episode (Train + Val + Test)")
        plt.ylabel("Return")
        plt.title("PPO – Full 1000 episodes vs benchmark")
        plt.legend()
        plt.tight_layout()
        plt.show()

    summarize_returns("Training", train_returns)
    summarize_returns("Validation", val_returns)
    summarize_returns("Testing", test_returns)

    return agent, train_returns, val_returns, test_returns


if __name__ == "__main__":
    DATA_DIR = "./data"
    device = "cuda" if torch.cuda.is_available() else "cpu"

    VARIANT = 1
    TRAIN_EPISODES = 800
    VAL_EPISODES = 100
    TEST_EPISODES = 100
    SEED= 7

    print("\n" + "#" * 70)
    print(f"### Starting PPO run (external encoder) for seed = {SEED}")
    print("#" * 70 + "\n")

    agent, train_returns, val_returns, test_returns = train_and_test_ppo(
        data_dir=DATA_DIR,
        variant=VARIANT,
        feature_mode="full",          # <- same as your old PPO env
        train_episodes=TRAIN_EPISODES,
        validation_episodes=VAL_EPISODES,
        test_episodes=TEST_EPISODES,
        gamma=0.9842694774795389,
        lam=0.8547793488849357,
        clip_ratio=0.07098308442073593,
        pi_lr=0.0004018661633092908,
        vf_lr=0.00022102510315199418,
        train_epochs=15,
        minibatch_size=32,
        entropy_coef=0.008662172739153698,
        device=device,
        seed=SEED,
        show_plot=True,
        print_interval=25,
    )

    val_mean = float(np.mean(val_returns)) if len(val_returns) > 0 else float("nan")
    test_mean = float(np.mean(test_returns)) if len(test_returns) > 0 else float("nan")
    print(f"[Seed {SEED}] Validation mean over {len(val_returns)} eps: {val_mean:.3f}")
    print(f"[Seed {SEED}] Testing   mean over {len(test_returns)} eps: {test_mean:.3f}")
