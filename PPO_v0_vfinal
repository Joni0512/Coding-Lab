'''
RESULTS
Seed 10: 218.720 (runtime = 00:49:12.702)
Seed 11: 216.675 (runtime = 00:50:51)
Seed 12: 218.900 (runtime = 00:50:42)
Seed 13: 218.650 (runtime = 00:47:24)
Seed 14: 216.625 (runtime = 00:52:19)
Seed 15: 216.740 (runtime = 00:50:40)
Seed 16: 217.055 (runtime = 00:53:57)
Seed 17: 216.335 (runtime = 01:11:51)
Seed 18: 217.820 (runtime = 00:47:52)
Seed 19: 213.870 (runtime = 00:55:11)

Mean across 10 seeds = 217.14 (greedy 216.920)
'''


# ============================================================
# PPO (Standalone, BO-extracted) — SAME algorithm/loop as BO run_one_seed()
# Style matches your “speed + clean logging” script.
#
# No Optuna / no BO call
# Seed hardcoded in __main__
# SAME rollout/update/checkpoint-by-validation logic as BO
# SAME networks, losses, action masking, GAE, annealing scheme as BO code
# Includes your requested best HPs (below)
#
# NOTE: To reproduce BO per-seed numbers, keep:
# - same env_midterm.py + same ./data
# - same device type (cuda vs cpu)
# - same torch version + deterministic settings
# ============================================================

import os
import random
import numpy as np
import time
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.distributions import Categorical

from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import dijkstra

from env_midterm import Environment

# ============================================================
# STRICT reproducibility controls (same spirit as BO script)
# ============================================================

STRICT_DETERMINISM = True

def _configure_torch_determinism():
    if not torch.cuda.is_available():
        return
    torch.backends.cuda.matmul.allow_tf32 = False
    torch.backends.cudnn.allow_tf32 = False

def _configure_cpu_threads():
    try:
        torch.set_num_threads(1)
        torch.set_num_interop_threads(1)
    except Exception:
        pass

_configure_torch_determinism()
_configure_cpu_threads()

def set_global_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    if STRICT_DETERMINISM:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        try:
            torch.use_deterministic_algorithms(True)
        except Exception:
            pass
    else:
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# ============================================================
# Episode CSV caching (transparent, non-algorithmic)
# ============================================================

_EPISODE_DF_CACHE = {}

try:
    import pandas as pd
    _pd_read_csv_orig = pd.read_csv

    def _pd_read_csv_cached(path, *args, **kwargs):
        try:
            if (
                isinstance(path, str)
                and ("episode_data" in path)
                and (os.path.basename(path).startswith("episode_"))
                and path.endswith(".csv")
            ):
                base = os.path.dirname(os.path.dirname(path))  # .../variant_x
                fname = os.path.basename(path)
                core = fname[len("episode_"):-4]
                file_id = int(core)
                key = (base, file_id)
                if key in _EPISODE_DF_CACHE:
                    return _EPISODE_DF_CACHE[key].copy(deep=False)
                df = _pd_read_csv_orig(path, *args, **kwargs)
                _EPISODE_DF_CACHE[key] = df
                return df.copy(deep=False)
        except Exception:
            pass
        return _pd_read_csv_orig(path, *args, **kwargs)

    pd.read_csv = _pd_read_csv_cached
except Exception:
    pd = None

# ============================================================
# CNN SHAPE (matches BO)
# ============================================================

CHANNELS = 10
HEIGHT = 5
WIDTH = 5
N_ACTIONS = 5

# ============================================================
# Distance structures (matches BO)
# ============================================================

def _build_distance_structs(env):
    if hasattr(env, "_dist_ready") and env._dist_ready:
        return

    variant = env.variant
    H, W = 5, 5

    if variant in (0, 1):
        neighbor_matrix = np.zeros((25, 25), dtype=int)
        for i in range(25):
            for j in range(i + 1, 25):
                i_r, i_c = i // 5, i % 5
                j_r, j_c = j // 5, j % 5
                if (j_r - i_r == 0 and j_c - i_c == 1) or (j_r - i_r == 1 and j_c - i_c == 0):
                    neighbor_matrix[i, j] = 1
                    neighbor_matrix[j, i] = 1

        graph = csr_matrix(neighbor_matrix)
        dist_matrix, _ = dijkstra(csgraph=graph, directed=False, return_predecessors=True, unweighted=True)

        env._mapping = None
        env._coord_to_idx = None
        env._dist_matrix = dist_matrix

        dist_grids = []
        for idx in range(25):
            grid = np.zeros((H, W), dtype=np.float32)
            for j in range(25):
                r = j // 5
                c = j % 5
                grid[r, c] = dist_matrix[idx, j]
            dist_grids.append(grid)
        env._dist_from_idx_grids = dist_grids

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        idx_base = base_coord[0] * 5 + base_coord[1]
        env._dist_base_grid = dist_grids[idx_base]

        finite = dist_matrix[dist_matrix < np.inf]
        env._max_dist = float(np.max(finite)) if finite.size > 0 else 0.0

    else:
        mapping = [
            (0, 0), (1, 0), (2, 0), (3, 0), (4, 0),
            (3, 1), (4, 1),
            (0, 2), (1, 2), (2, 2), (3, 2), (4, 2),
            (0, 3),
            (0, 4), (1, 4), (2, 4), (3, 4), (4, 4)
        ]
        env._mapping = mapping
        env._coord_to_idx = {coord: i for i, coord in enumerate(mapping)}

        neighbor_matrix = np.zeros((18, 18), dtype=int)

        def link(i, j):
            neighbor_matrix[i, j] = 1
            neighbor_matrix[j, i] = 1

        link(0, 1); link(1, 2); link(2, 3); link(3, 4)
        link(3, 5); link(4, 6); link(5, 6)
        link(5, 10); link(6, 11)
        link(7, 8); link(7, 12); link(8, 9); link(9, 10); link(10, 11)
        link(12, 13); link(13, 14); link(14, 15); link(15, 16); link(16, 17)

        graph = csr_matrix(neighbor_matrix)
        dist_matrix, _ = dijkstra(csgraph=graph, directed=False, return_predecessors=True, unweighted=True)
        env._dist_matrix = dist_matrix

        dist_grids = []
        for idx in range(len(mapping)):
            grid = np.zeros((H, W), dtype=np.float32)
            for j, coord in enumerate(mapping):
                r, c = coord
                grid[r, c] = dist_matrix[idx, j]
            dist_grids.append(grid)
        env._dist_from_idx_grids = dist_grids

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        idx_base = env._coord_to_idx[base_coord]
        env._dist_base_grid = dist_grids[idx_base]

        finite = dist_matrix[dist_matrix < np.inf]
        env._max_dist = float(np.max(finite)) if finite.size > 0 else 0.0

    if not hasattr(env, "_eligible_set"):
        env._eligible_set = set(env.eligible_cells)

    env._dist_ready = True

def _coord_to_idx(env, coord):
    if env.variant in (0, 1):
        return coord[0] * 5 + coord[1]
    return env._coord_to_idx[coord]

# ============================================================
# Action masks (matches BO)
# ============================================================

def valid_action_mask(env, n_actions=5):
    mask = np.zeros(n_actions, dtype=np.bool_)
    mask[0] = True

    r, c = env.agent_loc
    H, W = env.vertical_cell_count, env.horizontal_cell_count

    moves = [
        (1, -1, 0),  # up
        (2, 0, 1),   # right
        (3, 1, 0),   # down
        (4, 0, -1),  # left
    ]
    elig = getattr(env, "_eligible_set", None)
    if elig is None:
        elig = set(env.eligible_cells)
        env._eligible_set = elig

    for act, dr, dc in moves:
        nr, nc = r + dr, c + dc
        if 0 <= nr < H and 0 <= nc < W and (nr, nc) in elig:
            mask[act] = True

    return mask

def valid_action_mask_from_obs_batch(obs_batch, n_actions=5):
    device = obs_batch.device
    B = obs_batch.shape[0]

    grid = obs_batch.view(B, CHANNELS, HEIGHT, WIDTH)
    agent = grid[:, 0]
    eligible = grid[:, 6] > 0.5

    mask = torch.zeros((B, n_actions), dtype=torch.bool, device=device)
    mask[:, 0] = True

    pos = agent.view(B, -1).argmax(dim=1)
    ar = (pos // WIDTH).long()
    ac = (pos % WIDTH).long()

    b_idx = torch.arange(B, device=device)

    # UP (1)
    ok = (ar > 0)
    if ok.any():
        b = b_idx[ok]; r = ar[ok] - 1; c = ac[ok]
        ok2 = eligible[b, r, c]
        mask[b[ok2], 1] = True

    # RIGHT (2)
    ok = (ac < WIDTH - 1)
    if ok.any():
        b = b_idx[ok]; r = ar[ok]; c = ac[ok] + 1
        ok2 = eligible[b, r, c]
        mask[b[ok2], 2] = True

    # DOWN (3)
    ok = (ar < HEIGHT - 1)
    if ok.any():
        b = b_idx[ok]; r = ar[ok] + 1; c = ac[ok]
        ok2 = eligible[b, r, c]
        mask[b[ok2], 3] = True

    # LEFT (4)
    ok = (ac > 0)
    if ok.any():
        b = b_idx[ok]; r = ar[ok]; c = ac[ok] - 1
        ok2 = eligible[b, r, c]
        mask[b[ok2], 4] = True

    return mask

# ============================================================
# Observation encoding (matches BO)
# ============================================================

def encode_obs(obs, env, feature_mode="full"):
    step, agent_loc, agent_load, item_locs, item_times = obs
    _build_distance_structs(env)

    H, W, C = HEIGHT, WIDTH, CHANNELS
    state = np.zeros((C, H, W), dtype=np.float32)

    ALPHA = 1.0
    BETA = 1.0
    DELTA = 0.6

    # 6: eligible mask
    if env.variant in (0, 1):
        state[6, :, :] = 1.0
    else:
        elig = getattr(env, "_eligible_set", None)
        if elig is None:
            elig = set(env.eligible_cells)
            env._eligible_set = elig
        for r in range(H):
            for c in range(W):
                state[6, r, c] = 1.0 if (r, c) in elig else 0.0

    # 0: agent position
    ar, ac = agent_loc
    state[0, ar, ac] = 1.0

    # 1,2,5: items
    mrt = float(env.max_response_time)
    for (loc, age) in zip(item_locs, item_times):
        r, c = loc
        state[1, r, c] = 1.0
        state[2, r, c] = float(age) / mrt
        state[5, r, c] = (mrt - float(age)) / mrt

    # 3/4: distances normalized
    agent_idx = _coord_to_idx(env, agent_loc)
    dist_agent_grid = env._dist_from_idx_grids[agent_idx]

    max_dist = float(getattr(env, "_max_dist", 0.0))
    if max_dist > 0:
        state[3, :, :] = dist_agent_grid / max_dist
        state[4, :, :] = env._dist_base_grid / max_dist
    else:
        state[3, :, :] = 0.0
        state[4, :, :] = 0.0

    # 7: step norm
    state[7, :, :] = float(step) / float(env.episode_steps)

    # 8: load norm
    state[8, :, :] = float(agent_load) / float(env.agent_capacity)

    # 9: Phi plane
    phi_plane = np.zeros((H, W), dtype=np.float32)
    if len(item_locs) > 0 and max_dist and max_dist > 0:
        best = np.full((H, W), -np.inf, dtype=np.float32)

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        base_idx = _coord_to_idx(env, base_coord)

        for (loc, age) in zip(item_locs, item_times):
            ttl = (mrt - float(age)) / mrt
            item_idx = _coord_to_idx(env, loc)

            dist_to_item_grid = env._dist_from_idx_grids[item_idx]
            d_item_base = float(env._dist_matrix[item_idx, base_idx])

            cand = (
                -ALPHA * (dist_to_item_grid / max_dist)
                + BETA * ttl
                - DELTA * (d_item_base / max_dist)
            )
            best = np.maximum(best, cand.astype(np.float32))

        phi_plane = np.clip(best, -1.0, 1.0)
    else:
        phi_plane[:, :] = 0.0

    state[9, :, :] = phi_plane

    fm = feature_mode
    if fm == "full":
        pass
    elif fm == "reduced_full":
        state[2, :, :] = 0.0
        state[7, :, :] = 0.0
    elif fm == "no_distance":
        state[3, :, :] = 0.0
        state[4, :, :] = 0.0
    elif fm == "expiry_focused":
        state[2, :, :] = 0.0
        threshold = 3
        for (loc, age) in zip(item_locs, item_times):
            r, c = loc
            if env.max_response_time - age <= threshold:
                state[2, r, c] = 1.0
    elif fm == "minimal":
        keep = {0, 1, 5, 8}
        for ch in range(C):
            if ch not in keep:
                state[ch, :, :] = 0.0
    else:
        raise ValueError(f"Invalid feature_mode: {fm}")

    return state.reshape(-1)

# ============================================================
# Noisy Linear (matches BO)
# ============================================================

class NoisyLinear(nn.Module):
    def __init__(self, in_features, out_features, sigma_init=0.35):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features

        bound = 1.0 / np.sqrt(in_features)
        self.mu_weight = nn.Parameter(torch.empty(in_features, out_features).uniform_(-bound, bound))
        self.mu_bias = nn.Parameter(torch.empty(out_features).uniform_(-bound, bound))

        self.sigma_weight = nn.Parameter(torch.full((in_features, out_features), sigma_init / np.sqrt(in_features)))
        self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init / np.sqrt(in_features)))

    @staticmethod
    def f(x):
        return torch.sign(x) * torch.sqrt(torch.abs(x) + 1e-8)

    def forward(self, x):
        if not self.training:
            return x @ self.mu_weight + self.mu_bias

        eps_in = torch.randn(self.in_features, device=x.device)
        eps_out = torch.randn(self.out_features, device=x.device)
        eps_in = self.f(eps_in)
        eps_out = self.f(eps_out)
        eps_w = torch.ger(eps_in, eps_out)
        eps_b = eps_out

        weight = self.mu_weight + self.sigma_weight * eps_w
        bias = self.mu_bias + self.sigma_bias * eps_b
        return x @ weight + bias

# ============================================================
# CNN Policy & Value (matches BO)
# ============================================================

class PolicyNet(nn.Module):
    def __init__(self, obs_dim, n_actions, hidden_dim=256):
        super().__init__()
        self.channels = CHANNELS
        self.height = HEIGHT
        self.width = WIDTH
        assert obs_dim == self.channels * self.height * self.width

        self.conv1 = nn.Conv2d(self.channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        conv_out_dim = 64 * self.height * self.width

        self.fc1 = NoisyLinear(conv_out_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.logits = nn.Linear(hidden_dim, n_actions)

    def forward(self, x):
        x = x.view(-1, self.channels, self.height, self.width)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.ln1(x)
        x = F.relu(x)
        return self.logits(x)

class ValueNet(nn.Module):
    def __init__(self, obs_dim, hidden_dim=256):
        super().__init__()
        self.channels = CHANNELS
        self.height = HEIGHT
        self.width = WIDTH
        assert obs_dim == self.channels * self.height * self.width

        self.conv1 = nn.Conv2d(self.channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        conv_out_dim = 64 * self.height * self.width

        self.fc1 = nn.Linear(conv_out_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.v = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = x.view(-1, self.channels, self.height, self.width)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.ln1(x)
        x = F.relu(x)
        return self.v(x).squeeze(-1)

# ============================================================
# PPO Rollout Buffer (matches BO)
# ============================================================

class RolloutBuffer:
    def __init__(self, obs_dim, device):
        self.obs_dim = obs_dim
        self.device = device
        self.reset()

    def reset(self):
        self.obs = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.logps = []
        self.values = []
        self.bootstraps = []

    def add(self, obs, action, reward, done, logp, value, bootstrap_value):
        self.obs.append(np.asarray(obs, dtype=np.float32))
        self.actions.append(int(action))
        self.rewards.append(float(reward))
        self.dones.append(float(done))
        self.logps.append(float(logp))
        self.values.append(float(value))
        self.bootstraps.append(float(bootstrap_value))

    def compute_gae(self, gamma, lam):
        rewards = torch.tensor(self.rewards, dtype=torch.float32, device=self.device)
        values  = torch.tensor(self.values,  dtype=torch.float32, device=self.device)
        dones   = torch.tensor(self.dones,   dtype=torch.float32, device=self.device)
        boots   = torch.tensor(self.bootstraps, dtype=torch.float32, device=self.device)

        T = rewards.shape[0]
        adv = torch.zeros(T, dtype=torch.float32, device=self.device)

        gae = 0.0
        next_value = 0.0

        for t in reversed(range(T)):
            if dones[t] > 0.5:
                next_value = boots[t]  # time-limit bootstrap (as BO)
                # do not reset gae

            delta = rewards[t] + gamma * next_value - values[t]
            gae = delta + gamma * lam * gae
            adv[t] = gae
            next_value = values[t]

        ret = adv + values
        adv = (adv - adv.mean()) / (adv.std() + 1e-8)
        return adv, ret

    def as_tensors(self):
        obs = torch.as_tensor(np.asarray(self.obs, dtype=np.float32), device=self.device)
        actions = torch.as_tensor(np.asarray(self.actions, dtype=np.int64), device=self.device)
        old_logp = torch.as_tensor(np.asarray(self.logps, dtype=np.float32), device=self.device)
        old_v = torch.as_tensor(np.asarray(self.values, dtype=np.float32), device=self.device)
        return obs, actions, old_logp, old_v

# ============================================================
# PPO Agent (matches BO — including annealing rule)
# ============================================================

class PPOAgent:
    def __init__(
        self,
        obs_dim,
        n_actions,
        gamma,
        lam,
        clip_ratio,
        pi_lr,
        vf_lr,
        train_epochs,
        minibatch_size,
        entropy_coef,
        device="cpu",
        target_kl=0.02,
        max_grad_norm=0.5,
        vf_clip_ratio=0.2,
    ):
        self.obs_dim = obs_dim
        self.n_actions = n_actions
        self.gamma = float(gamma)
        self.lam = float(lam)
        self.clip_ratio = float(clip_ratio)
        self.train_epochs = int(train_epochs)
        self.minibatch_size = int(minibatch_size)
        self.entropy_coef = float(entropy_coef)
        self.device = torch.device(device)

        self.target_kl = float(target_kl)
        self.max_grad_norm = float(max_grad_norm)
        self.vf_clip_ratio = float(vf_clip_ratio)

        self.policy = PolicyNet(obs_dim, n_actions).to(self.device)
        self.value_fn = ValueNet(obs_dim).to(self.device)

        self.pi_optimizer = Adam(self.policy.parameters(), lr=float(pi_lr), eps=1e-5)
        self.vf_optimizer = Adam(self.value_fn.parameters(), lr=float(vf_lr), eps=1e-5)

        self._pi_lr0 = float(pi_lr)
        self._vf_lr0 = float(vf_lr)
        self._ent0 = float(entropy_coef)

    @torch.no_grad()
    def select_action(self, obs_vec, env=None, eval_mode=False):
        if eval_mode:
            self.policy.eval()
            self.value_fn.eval()
        else:
            self.policy.train()
            self.value_fn.train()

        obs_t = torch.as_tensor(obs_vec, dtype=torch.float32, device=self.device).unsqueeze(0)
        logits = self.policy(obs_t)

        if env is not None:
            mask_np = valid_action_mask(env, self.n_actions)
            mask_t = torch.as_tensor(mask_np, device=self.device).unsqueeze(0)
            logits = logits.masked_fill(~mask_t, -1e9)

        dist = Categorical(logits=logits)
        if eval_mode:
            action = torch.argmax(dist.probs, dim=-1)
        else:
            action = dist.sample()

        log_prob = dist.log_prob(action)
        value = self.value_fn(obs_t)
        return int(action.item()), float(log_prob.item()), float(value.item())

    def set_annealed_hparams(self, progress: float):
        # EXACTLY like BO script
        pi_lr = self._pi_lr0 * (1.0 - progress)
        vf_lr = self._vf_lr0 * (1.0 - progress)
        for pg in self.pi_optimizer.param_groups:
            pg["lr"] = pi_lr
        for pg in self.vf_optimizer.param_groups:
            pg["lr"] = vf_lr

        self.entropy_coef = max(0.002, self._ent0 * (1.0 - progress))
        # BO script uses fixed base constant here (NOT "self.clip_ratio")
        self.clip_ratio = max(0.04, 0.07098308442073593 * (1.0 - 0.5 * progress))

    def update(self, obs, actions, old_logp, old_values, advantages, returns):
        self.policy.train()
        self.value_fn.train()

        N = obs.size(0)
        idxs = np.arange(N)

        for _ in range(self.train_epochs):
            np.random.shuffle(idxs)
            approx_kls = []

            for start in range(0, N, self.minibatch_size):
                mb_idx = idxs[start:start + self.minibatch_size]
                mb_idx_t = torch.as_tensor(mb_idx, dtype=torch.long, device=self.device)

                mb_obs = obs[mb_idx_t]
                mb_actions = actions[mb_idx_t]
                mb_old_logp = old_logp[mb_idx_t]
                mb_old_v = old_values[mb_idx_t]
                mb_adv = advantages[mb_idx_t]
                mb_ret = returns[mb_idx_t]

                logits = self.policy(mb_obs)

                # IMPORTANT: action masking inside update (as BO)
                mask = valid_action_mask_from_obs_batch(mb_obs, self.n_actions)
                logits = logits.masked_fill(~mask, -1e9)

                dist = Categorical(logits=logits)

                new_logp = dist.log_prob(mb_actions)
                entropy = dist.entropy().mean()

                ratio = torch.exp(new_logp - mb_old_logp)
                unclipped = ratio * mb_adv
                clipped = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * mb_adv
                policy_loss = -torch.min(unclipped, clipped).mean()

                v_pred = self.value_fn(mb_obs)
                v_pred_clipped = mb_old_v + torch.clamp(v_pred - mb_old_v, -self.vf_clip_ratio, self.vf_clip_ratio)
                v_loss_unclipped = F.smooth_l1_loss(v_pred, mb_ret)
                v_loss_clipped = F.smooth_l1_loss(v_pred_clipped, mb_ret)
                value_loss = torch.max(v_loss_unclipped, v_loss_clipped)

                self.pi_optimizer.zero_grad(set_to_none=True)
                (policy_loss - self.entropy_coef * entropy).backward()
                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
                self.pi_optimizer.step()

                self.vf_optimizer.zero_grad(set_to_none=True)
                value_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.value_fn.parameters(), self.max_grad_norm)
                self.vf_optimizer.step()

                approx_kl = (mb_old_logp - new_logp).mean().item()
                approx_kls.append(approx_kl)

            if len(approx_kls) > 0 and float(np.mean(approx_kls)) > 1.5 * self.target_kl:
                break

# ============================================================
# Eval helpers (silent)
# ============================================================

def run_eval_silent(env, agent, feature_mode, mode, episodes):
    returns = []
    max_eps = min(episodes, len(env.validation_episodes) if mode == "validation" else len(env.test_episodes))

    with torch.inference_mode():
        for _ in range(max_eps):
            raw = env.reset(mode=mode)
            obs_vec = encode_obs(raw, env, feature_mode=feature_mode)

            done = False
            ep_ret = 0.0
            while not done:
                action, _, _ = agent.select_action(obs_vec, env=env, eval_mode=True)
                rew, raw_next, done_flag = env.step(action)
                obs_vec = encode_obs(raw_next, env, feature_mode=feature_mode)
                ep_ret += rew
                done = bool(done_flag)

            returns.append(ep_ret)

    return returns

def summarize_returns(name, returns):
    arr = np.asarray(returns, dtype=np.float32)
    n = len(arr)
    print(f"\n=== {name} statistics ===")
    if n == 0:
        print("No episodes recorded.")
        print("================================\n")
        return
    best_ret = float(arr.max())
    best_ep = int(arr.argmax()) + 1
    mean_all = float(arr.mean())
    std_all = float(arr.std())
    window = min(100, n)
    mean_last = float(arr[-window:].mean())
    print(f"Episodes total:              {n}")
    print(f"Best episode:                {best_ep}  (return = {best_ret:.2f})")
    print(f"Mean return (all):           {mean_all:.2f} ± {std_all:.2f}")
    print(f"Mean return (last {window}): {mean_last:.2f}")
    print("================================\n")

# ============================================================
# Train + Val + Test (EXACT BO loop structure)
# ============================================================

def train_and_test_ppo(
    data_dir,
    variant=0,
    feature_mode="full",
    train_episodes=6000,
    validation_episodes=100,
    test_episodes=100,

    # === YOUR REQUESTED BEST HPs ===
    gamma=0.9903908067862474,
    lam=0.8445345102837427,
    clip_ratio=0.06462447063962125,
    pi_lr=0.0004426371952957778,
    vf_lr=0.000355444282633541,
    train_epochs=12,
    minibatch_size=512,
    entropy_coef=0.012732440647433297,
    rollout_steps=1536,
    save_every_updates=10,

    device="cpu",
    seed=19,

    show_plot=False,
    ckpt_dir="./checkpoints",
    train_print_every_episodes=250,  # style logging (safe)
):
    set_global_seed(seed)
    os.makedirs(ckpt_dir, exist_ok=True)

    env = Environment(variant=variant, data_dir=data_dir)
    n_actions = N_ACTIONS

    raw0 = env.reset(mode="training")
    obs_vec0 = encode_obs(raw0, env, feature_mode=feature_mode)
    obs_dim = obs_vec0.shape[0]
    print(f"Obs_dim={obs_dim}, n_actions={n_actions}, feature_mode='{feature_mode}', variant={variant}, seed={seed}")

    agent = PPOAgent(
        obs_dim=obs_dim,
        n_actions=n_actions,
        gamma=gamma,
        lam=lam,
        clip_ratio=clip_ratio,
        pi_lr=pi_lr,
        vf_lr=vf_lr,
        train_epochs=train_epochs,
        minibatch_size=minibatch_size,
        entropy_coef=entropy_coef,
        device=device,
        target_kl=0.02,
        max_grad_norm=0.5,
        vf_clip_ratio=0.2,
    )

    # BO script:
    total_steps_budget = int(train_episodes) * 200
    num_updates = max(1, total_steps_budget // int(rollout_steps))

    buffer = RolloutBuffer(obs_dim=obs_dim, device=agent.device)
    train_returns = []

    best_val_mean = -np.inf
    best_ckpt_path = os.path.join(ckpt_dir, f"ppo_bestVAL_variant{variant}_seed{seed}.pt")

    raw = env.reset(mode="training")
    obs_vec = encode_obs(raw, env, feature_mode=feature_mode)
    ep_ret = 0.0
    completed_eps = 0

    for update_idx in range(num_updates):
        progress = update_idx / max(1, num_updates - 1)
        agent.set_annealed_hparams(progress)

        buffer.reset()
        steps_collected = 0

        while steps_collected < rollout_steps:
            action, logp, value = agent.select_action(obs_vec, env=env, eval_mode=False)
            rew, raw_next, done_flag = env.step(action)
            next_obs_vec = encode_obs(raw_next, env, feature_mode=feature_mode)

            done_bool = bool(done_flag)
            ep_ret += rew

            # BO script: bootstrap only when done
            if done_bool:
                with torch.no_grad():
                    nxt = torch.as_tensor(next_obs_vec, dtype=torch.float32, device=agent.device).unsqueeze(0)
                    bootstrap_value = float(agent.value_fn(nxt).item())
            else:
                bootstrap_value = 0.0

            buffer.add(obs_vec, action, rew, done_bool, logp, value, bootstrap_value)
            steps_collected += 1
            obs_vec = next_obs_vec

            if done_bool:
                train_returns.append(ep_ret)
                completed_eps += 1
                if train_print_every_episodes and (completed_eps % int(train_print_every_episodes) == 0):
                    avgN = float(np.mean(train_returns[-int(train_print_every_episodes):]))
                    print(f"[TRAIN] Episodes {completed_eps}  (update {update_idx+1}/{num_updates})  Avg(last {int(train_print_every_episodes)}): {avgN:.3f}")
                ep_ret = 0.0
                raw = env.reset(mode="training")
                obs_vec = encode_obs(raw, env, feature_mode=feature_mode)

        obs_t, actions_t, old_logp_t, old_v_t = buffer.as_tensors()
        adv_t, ret_t = buffer.compute_gae(gamma=agent.gamma, lam=agent.lam)
        agent.update(obs_t, actions_t, old_logp_t, old_v_t, adv_t, ret_t)

        # BO script: validation checkpoint every save_every_updates
        if (update_idx + 1) % int(save_every_updates) == 0 or (update_idx + 1) == num_updates:
            val_env = Environment(variant=variant, data_dir=data_dir)
            val_returns = run_eval_silent(val_env, agent, feature_mode, mode="validation", episodes=validation_episodes)
            val_mean = float(np.mean(val_returns)) if len(val_returns) else -np.inf
            print(f"[CKPT] update {update_idx+1}/{num_updates}  val_mean={val_mean:.3f}  best_val={best_val_mean:.3f}")

            if val_mean > best_val_mean:
                best_val_mean = val_mean
                torch.save(
                    {
                        "policy_state_dict": agent.policy.state_dict(),
                        "value_state_dict": agent.value_fn.state_dict(),
                        "best_val_mean": best_val_mean,
                        "update": update_idx + 1,
                        "variant": variant,
                        "seed": seed,
                        "feature_mode": feature_mode,
                        "obs_dim": obs_dim,
                        "n_actions": n_actions,
                        "gamma": agent.gamma,
                        "lam": agent.lam,
                        "clip_ratio": agent.clip_ratio,
                        "entropy_coef": agent.entropy_coef,
                        "rollout_steps": int(rollout_steps),
                    },
                    best_ckpt_path,
                )
                print(f"[CKPT] saved new best-val checkpoint -> {best_ckpt_path}")

    # Load best checkpoint (by validation)
    if os.path.exists(best_ckpt_path):
        ckpt = torch.load(best_ckpt_path, map_location=device)
        agent.policy.load_state_dict(ckpt["policy_state_dict"])
        agent.value_fn.load_state_dict(ckpt["value_state_dict"])
        print(f"\nLoaded BEST-VAL checkpoint from update {ckpt['update']} with best_val_mean={ckpt['best_val_mean']:.2f}\n")
    else:
        print("\nNo BEST checkpoint found; using final weights.\n")

    # Final evaluation (same as BO)
    val_env = Environment(variant=variant, data_dir=data_dir)
    test_env = Environment(variant=variant, data_dir=data_dir)

    val_returns = run_eval_silent(val_env, agent, feature_mode, mode="validation", episodes=validation_episodes)
    test_returns = run_eval_silent(test_env, agent, feature_mode, mode="testing", episodes=test_episodes)

    if show_plot:
        episodes_train = np.arange(1, len(train_returns) + 1)
        plt.figure()
        plt.plot(episodes_train, train_returns, label="Train return")
        plt.xlabel("Episode")
        plt.ylabel("Return")
        plt.title("PPO – Training Returns")
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(ckpt_dir, f"train_curve_variant{variant}_seed{seed}.png"), dpi=160)
        plt.close()

    summarize_returns("Training (completed episodes)", train_returns)
    summarize_returns("Validation", val_returns)
    summarize_returns("Testing", test_returns)

    return agent, train_returns, val_returns, test_returns, best_ckpt_path

# ============================================================
# Main (hardcoded seed, style like your file)
# ============================================================

if __name__ == "__main__":
    DATA_DIR = "./data"
    device = "cuda" if torch.cuda.is_available() else "cpu"

    VARIANT = 0
    TRAIN_EPISODES = 6000
    VAL_EPISODES = 100
    TEST_EPISODES = 100
    SEED= 19

    # IMPORTANT: same checkpoint dir structure per seed (prevents collisions)
    CKPT_DIR = os.path.join("./checkpoints_bo_extracted", f"seed_{SEED}")

    print("\n" + "#" * 70)
    print(f"### Starting PPO BO-extracted run for seed = {SEED}")
    print("#" * 70 + "\n")

    t0 = time.time()
    agent, train_returns, val_returns, test_returns, best_ckpt_path = train_and_test_ppo(
        data_dir=DATA_DIR,
        variant=VARIANT,
        feature_mode="full",
        train_episodes=TRAIN_EPISODES,
        validation_episodes=VAL_EPISODES,
        test_episodes=TEST_EPISODES,

        # === REQUIRED HPs (as requested) ===
        gamma=0.9903908067862474,
        lam=0.8445345102837427,
        clip_ratio=0.06462447063962125,
        pi_lr=0.0004426371952957778,
        vf_lr=0.000355444282633541,
        train_epochs=12,
        minibatch_size=512,
        entropy_coef=0.012732440647433297,
        rollout_steps=1536,
        save_every_updates=10,

        device=device,
        seed=SEED,
        show_plot=False,
        ckpt_dir=CKPT_DIR,
        train_print_every_episodes=250,
    )

    val_mean = float(np.mean(val_returns)) if len(val_returns) > 0 else float("nan")
    test_mean = float(np.mean(test_returns)) if len(test_returns) > 0 else float("nan")
    print(f"\nBEST checkpoint path: {best_ckpt_path}")
    print(f"[Seed {SEED}] Validation mean over {len(val_returns)} eps: {val_mean:.3f}")
    print(f"[Seed {SEED}] Testing   mean over {len(test_returns)} eps: {test_mean:.3f}")

    dt = time.time() - t0
    h = int(dt // 3600)
    m = int((dt % 3600) // 60)
    s = dt % 60
    print(f"[Seed {SEED}] Wall time: {h:02d}:{m:02d}:{s:06.3f}  (hh:mm:ss.sss)")
    benchmark = 216.920
    if test_mean > benchmark:
        print("YAY YOU BEAT THE BENCHMARK")
    else:
        print("KEEP TRYING - SO CLOSE")
