import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
import matplotlib.pyplot as plt


# 0.  IMPORT YOUR ENV

from environment import Environment   # example



# 1.  STATE ENCODING


def encode_obs(obs, env):
    """
    obs = (step_count, agent_loc, agent_load, item_locs, item_times)

    Encode as:
      - agent one-hot over 25 cells
      - item presence one-hot over 25 cells
      - normalized step
      - normalized load
    """
    step, agent_loc, agent_load, item_locs, item_times = obs

    H, W = env.vertical_cell_count, env.horizontal_cell_count
    grid_size = H * W

    agent_feat = np.zeros(grid_size, dtype=np.float32)
    idx_agent = agent_loc[0] * W + agent_loc[1]
    agent_feat[idx_agent] = 1.0

    item_feat = np.zeros(grid_size, dtype=np.float32)
    for (r, c) in item_locs:
        idx = r * W + c
        item_feat[idx] = 1.0

    step_norm = np.array([step / env.episode_steps], dtype=np.float32)
    load_norm = np.array([agent_load / env.agent_capacity], dtype=np.float32)

    return np.concatenate([agent_feat, item_feat, step_norm, load_norm], axis=0)
    # dimension = 25 + 25 + 1 + 1 = 52



# 2.  NETWORKS FOR DISCRETE SAC


class QNetwork(nn.Module):
    def __init__(self, obs_dim, n_actions, hidden_dim=128):
        super().__init__()
        self.fc1 = nn.Linear(obs_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.out = nn.Linear(hidden_dim, n_actions)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.out(x)  # [B, n_actions]


class CategoricalPolicy(nn.Module):
    def __init__(self, obs_dim, n_actions, hidden_dim=128):
        super().__init__()
        self.fc1 = nn.Linear(obs_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.logits = nn.Linear(hidden_dim, n_actions)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        logits = self.logits(x)
        log_probs = F.log_softmax(logits, dim=-1)
        probs = log_probs.exp()
        return probs, log_probs, logits



# 3.  REPLAY BUFFER


class ReplayBuffer:
    def __init__(self, capacity, obs_dim):
        self.capacity = capacity
        self.obs_buf = np.zeros((capacity, obs_dim), dtype=np.float32)
        self.obs_next_buf = np.zeros((capacity, obs_dim), dtype=np.float32)
        self.act_buf = np.zeros((capacity,), dtype=np.int64)
        self.rew_buf = np.zeros((capacity,), dtype=np.float32)
        self.done_buf = np.zeros((capacity,), dtype=np.float32)
        self.ptr = 0
        self.size = 0

    def store(self, obs, act, rew, next_obs, done):
        self.obs_buf[self.ptr] = obs
        self.obs_next_buf[self.ptr] = next_obs
        self.act_buf[self.ptr] = act
        self.rew_buf[self.ptr] = rew
        self.done_buf[self.ptr] = done

        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)

    def sample_batch(self, batch_size):
        idxs = np.random.randint(0, self.size, size=batch_size)
        return dict(
            obs=self.obs_buf[idxs],
            obs_next=self.obs_next_buf[idxs],
            act=self.act_buf[idxs],
            rew=self.rew_buf[idxs],
            done=self.done_buf[idxs],
        )



# 4.  DISCRETE SAC AGENT


class DiscreteSACAgent:
    def __init__(
        self,
        obs_dim,
        n_actions,
        gamma=0.8,
        tau=0.005,
        alpha=0.2,
        lr=3e-4,
        hidden_dim=128,
        device="cpu",
    ):
        self.obs_dim = obs_dim
        self.n_actions = n_actions
        self.gamma = gamma
        self.tau = tau
        self.alpha = alpha
        self.device = torch.device(device)

        self.q1 = QNetwork(obs_dim, n_actions, hidden_dim).to(self.device)
        self.q2 = QNetwork(obs_dim, n_actions, hidden_dim).to(self.device)
        self.q1_target = QNetwork(obs_dim, n_actions, hidden_dim).to(self.device)
        self.q2_target = QNetwork(obs_dim, n_actions, hidden_dim).to(self.device)

        self.policy = CategoricalPolicy(obs_dim, n_actions, hidden_dim).to(self.device)

        self.q1_target.load_state_dict(self.q1.state_dict())
        self.q2_target.load_state_dict(self.q2.state_dict())

        self.q1_opt = Adam(self.q1.parameters(), lr=lr)
        self.q2_opt = Adam(self.q2.parameters(), lr=lr)
        self.policy_opt = Adam(self.policy.parameters(), lr=lr)

    @torch.no_grad()
    def act(self, obs_vec, eval_mode=False):
        obs_t = torch.as_tensor(obs_vec, dtype=torch.float32, device=self.device).unsqueeze(0)
        probs, _, _ = self.policy(obs_t)
        probs = probs[0]
        if eval_mode:
            a = probs.argmax().item()
        else:
            dist = torch.distributions.Categorical(probs=probs)
            a = dist.sample().item()
        return a

    def update(self, replay_buffer, batch_size):
        if replay_buffer.size < batch_size:
            return

        batch = replay_buffer.sample_batch(batch_size)

        obs = torch.as_tensor(batch["obs"], dtype=torch.float32, device=self.device)
        obs_next = torch.as_tensor(batch["obs_next"], dtype=torch.float32, device=self.device)
        act = torch.as_tensor(batch["act"], dtype=torch.long, device=self.device)
        rew = torch.as_tensor(batch["rew"], dtype=torch.float32, device=self.device)
        done = torch.as_tensor(batch["done"], dtype=torch.float32, device=self.device)

        # Critic 
        q1_values = self.q1(obs)
        q2_values = self.q2(obs)
        q1_s_a = q1_values.gather(1, act.unsqueeze(1)).squeeze(1)
        q2_s_a = q2_values.gather(1, act.unsqueeze(1)).squeeze(1)

        with torch.no_grad():
            probs_next, logp_next, _ = self.policy(obs_next)
            q1_next = self.q1_target(obs_next)
            q2_next = self.q2_target(obs_next)
            q_min_next = torch.min(q1_next, q2_next)

            v_next = (probs_next * (q_min_next - self.alpha * logp_next)).sum(dim=1)
            y = rew + self.gamma * (1 - done) * v_next

        critic_loss1 = F.mse_loss(q1_s_a, y)
        critic_loss2 = F.mse_loss(q2_s_a, y)
        critic_loss = critic_loss1 + critic_loss2

        self.q1_opt.zero_grad()
        self.q2_opt.zero_grad()
        critic_loss.backward()
        self.q1_opt.step()
        self.q2_opt.step()

        #  Actor 
        probs, logp, _ = self.policy(obs)
        q1_pi = self.q1(obs)
        q2_pi = self.q2(obs)
        q_min = torch.min(q1_pi, q2_pi).detach()

        actor_loss = (probs * (self.alpha * logp - q_min)).sum(dim=1).mean()

        self.policy_opt.zero_grad()
        actor_loss.backward()
        self.policy_opt.step()

        # ----- Target soft update -----
        with torch.no_grad():
            for p, p_targ in zip(self.q1.parameters(), self.q1_target.parameters()):
                p_targ.data.mul_(1.0 - self.tau)
                p_targ.data.add_(self.tau * p.data)
            for p, p_targ in zip(self.q2.parameters(), self.q2_target.parameters()):
                p_targ.data.mul_(1.0 - self.tau)
                p_targ.data.add_(self.tau * p.data)



# 5.  TRAIN + TEST + PLOT


def train_and_test_sac(
    data_dir,
    variant=0,
    train_episodes=1000,
    test_episodes=100,
    replay_size=100000,
    batch_size=64,
    start_steps=1000,
    update_after=1000,
    update_every=1,
    gamma=0.99,
    tau=0.005,
    alpha=0.2,
    lr=3e-4,
    device="cpu",
    print_interval=25,
):
    env = Environment(variant=variant, data_dir=data_dir)
    n_actions = 5

    obs0 = env.reset(mode="training")
    obs_vec0 = encode_obs(obs0, env)
    obs_dim = obs_vec0.shape[0]

    buffer = ReplayBuffer(replay_size, obs_dim)
    agent = DiscreteSACAgent(
        obs_dim=obs_dim,
        n_actions=n_actions,
        gamma=gamma,
        tau=tau,
        alpha=alpha,
        lr=lr,
        device=device,
    )

    total_steps = 0
    train_returns = []

    #  TRAINING 
    for ep in range(train_episodes):
        obs = env.reset(mode="training")
        obs_vec = encode_obs(obs, env)
        done = False
        ep_ret = 0.0

        while not done:
            if total_steps < start_steps:
                act = np.random.randint(0, n_actions)
            else:
                act = agent.act(obs_vec, eval_mode=False)

            rew, obs_next, done_flag = env.step(act)
            obs_next_vec = encode_obs(obs_next, env)

            buffer.store(obs_vec, act, rew, obs_next_vec, float(done_flag))

            obs_vec = obs_next_vec
            ep_ret += rew
            done = bool(done_flag)
            total_steps += 1

            if total_steps >= update_after and total_steps % update_every == 0:
                agent.update(buffer, batch_size)

        train_returns.append(ep_ret)

        if (ep + 1) % print_interval == 0:
            avg25 = np.mean(train_returns[-print_interval:])
            print(f"[TRAIN] Episode {ep+1}/{train_episodes}  "
                  f"Return: {ep_ret:.1f}  Avg(last {print_interval}): {avg25:.1f}")

    # -  TESTING 
    test_returns = []
    for ep in range(test_episodes):
        obs = env.reset(mode="testing")
        obs_vec = encode_obs(obs, env)
        done = False
        ep_ret = 0.0

        while not done:
            act = agent.act(obs_vec, eval_mode=True)
            rew, obs_next, done_flag = env.step(act)
            obs_vec = encode_obs(obs_next, env)
            ep_ret += rew
            done = bool(done_flag)

        test_returns.append(ep_ret)

        if (ep + 1) % print_interval == 0:
            avg25 = np.mean(test_returns[-print_interval:])
            print(f"[TEST] Episode {ep+1}/{test_episodes}  "
                  f"Return: {ep_ret:.1f}  Avg(last {print_interval}): {avg25:.1f}")

    #  PLOT 
    episodes = np.arange(1, train_episodes + 1)
    plt.figure()
    plt.plot(episodes, train_returns, label="Train return")

    if train_episodes >= 25:
        kernel = np.ones(print_interval) / print_interval
        mov_avg = np.convolve(train_returns, kernel, mode="valid")
        plt.plot(np.arange(print_interval, train_episodes + 1),
                 mov_avg, label=f"{print_interval}-episode avg")

    plt.xlabel("Episode")
    plt.ylabel("Return")
    plt.title("Discrete SAC â€“ Training Returns")
    plt.legend()
    plt.tight_layout()
    plt.show()

    return agent, train_returns, test_returns


# 6.  MAIN ENTRY POINT


if __name__ == "__main__":
    DATA_DIR = "./data"
    device = "cuda" if torch.cuda.is_available() else "cpu"

    agent, train_returns, test_returns = train_and_test_sac(
        data_dir=DATA_DIR,
        variant=0,
        train_episodes=1000,
        test_episodes=100,
        device=device,
    )
