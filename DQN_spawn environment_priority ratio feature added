# ============================================================
# COMPLETE DQN SCRIPT (NO BO) — Variant focus + Priority Ratio feature
# Compatible with your NEW environment.py (eligible_mask_grid, spawn_count_grid, spawn_prob_grid)
#
# - Noisy Dueling Double DQN + CNN
# - PER (Prioritized Replay) with fixed SumTree indexing
# - Action masking
# - Train 800 / Val 100 / Test 100
# - Plot training curve + Val/Test averages + (optional) greedy benchmark line
#
# NEW FEATURE (added on top of your 11 channels):
#   Channel 11: priority_ratio_norm (on item cells)
#     pr = ( 1/(1+d_agent) + 1/(1+d_target) ) * (1/t_i)
#     t_i = remaining lifetime = max_response_time - age, clipped >= 1
#     pr_norm = clip(pr/2, 0..1)
#
# Uses your old BO-best "Combo 2" parameters (hardcoded below).
# ============================================================

# ------------- Reproducibility / Quiet logs -----------------
seed = 2
import os
os.environ["PYTHONHASHSEED"] = str(seed)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"   # silence TF INFO/WARN

import random
random.seed(seed)

import numpy as np
np.random.seed(seed)

import tensorflow as tf
tf.random.set_seed(seed)

# --- SciPy for shortest paths (graph distances) -------------
from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import dijkstra

# ============================================================
# NEW Environment
# ============================================================
from environment import Environment  # must provide eligible_mask_grid, spawn_count_grid, spawn_prob_grid

# ============================================================
# Keras / plotting
# ============================================================
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Dense, Lambda, Reshape, Conv2D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber

from collections import deque
import warnings

import matplotlib
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib.font_manager")
matplotlib.rcParams["font.family"] = "DejaVu Sans"
matplotlib.rcParams["axes.unicode_minus"] = False

import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle


DATA_DIR = "./data"


# ============================================================
# 1. Distance structures (cached inside env)
# ============================================================

def _build_distance_structs(env):
    """
    Builds graph distances once and stores into env:
      env._dist_matrix
      env._dist_from_idx_grids
      env._dist_base_grid
      env._dist_ready
      env._mapping / env._coord_to_idx (variant 2)
    """
    if getattr(env, "_dist_ready", False):
        return

    variant = env.variant
    H, W = 5, 5

    if variant in (0, 1):
        neighbor_matrix = np.zeros((25, 25), dtype=np.int32)
        for i in range(25):
            ir, ic = divmod(i, 5)
            for j in range(i + 1, 25):
                jr, jc = divmod(j, 5)
                if (jr == ir and jc == ic + 1) or (jr == ir + 1 and jc == ic):
                    neighbor_matrix[i, j] = 1
                    neighbor_matrix[j, i] = 1

        graph = csr_matrix(neighbor_matrix)
        dist_matrix, _ = dijkstra(csgraph=graph, directed=False, return_predecessors=True, unweighted=True)

        env._mapping = None
        env._coord_to_idx = None
        env._dist_matrix = dist_matrix

        dist_grids = []
        for idx in range(25):
            grid = np.zeros((H, W), dtype=np.float32)
            for j in range(25):
                r, c = divmod(j, 5)
                grid[r, c] = dist_matrix[idx, j]
            dist_grids.append(grid)
        env._dist_from_idx_grids = dist_grids

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        idx_base = base_coord[0] * 5 + base_coord[1]
        env._dist_base_grid = dist_grids[idx_base]

    else:
        # Variant 2 graph mapping (only eligible cells)
        mapping = [
            (0, 0), (1, 0), (2, 0), (3, 0), (4, 0),
            (3, 1), (4, 1),
            (0, 2), (1, 2), (2, 2), (3, 2), (4, 2),
            (0, 3),
            (0, 4), (1, 4), (2, 4), (3, 4), (4, 4),
        ]
        env._mapping = mapping
        env._coord_to_idx = {coord: i for i, coord in enumerate(mapping)}

        neighbor_matrix = np.zeros((18, 18), dtype=np.int32)

        def link(i, j):
            neighbor_matrix[i, j] = 1
            neighbor_matrix[j, i] = 1

        # edges (from your earlier code)
        link(0, 1); link(1, 2); link(2, 3); link(3, 4)
        link(3, 5); link(4, 6); link(5, 6)
        link(5, 10); link(6, 11)
        link(7, 8); link(7, 12); link(8, 9); link(9, 10); link(10, 11)
        link(12, 13); link(13, 14); link(14, 15); link(15, 16); link(16, 17)

        graph = csr_matrix(neighbor_matrix)
        dist_matrix, _ = dijkstra(csgraph=graph, directed=False, return_predecessors=True, unweighted=True)
        env._dist_matrix = dist_matrix

        dist_grids = []
        for idx in range(len(mapping)):
            grid = np.zeros((H, W), dtype=np.float32)
            for j, (r, c) in enumerate(mapping):
                grid[r, c] = dist_matrix[idx, j]
            dist_grids.append(grid)
        env._dist_from_idx_grids = dist_grids

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        idx_base = env._coord_to_idx[base_coord]
        env._dist_base_grid = dist_grids[idx_base]

    env._dist_ready = True


def _coord_to_idx(env, coord):
    if env.variant in (0, 1):
        return coord[0] * 5 + coord[1]
    return env._coord_to_idx[coord]


# ============================================================
# 2. CNN observation encoding
# ============================================================

HEIGHT = 5
WIDTH = 5

# Channels (old 11 + new priority_ratio = 12):
# 0: Agent position (one-hot)
# 1: Item presence
# 2: Item age / max_response_time
# 3: Distanz Agent -> Zelle (normalisiert)
# 4: Distanz Base  -> Zelle (normalisiert)
# 5: Restzeit (max_response_time - age) / max_response_time
# 6: eligible_mask_grid (aus Environment)
# 7: step / episode_steps
# 8: load / agent_capacity
# 9: spawn_count_norm (aus Environment)
# 10: spawn_prob_grid (aus Environment)
# 11: priority_ratio_norm (NEU, on item cells)
CHANNELS = 12


def encode_obs(obs, env):
    step, agent_loc, agent_load, item_locs, item_times = obs

    _build_distance_structs(env)

    state = np.zeros((HEIGHT, WIDTH, CHANNELS), dtype=np.float32)

    # 6: eligible mask (directly from env)
    state[:, :, 6] = env.eligible_mask_grid.astype(np.float32)

    # 9,10: spawn stats (directly from env)
    spawn_count = env.spawn_count_grid.astype(np.float32)
    spawn_prob = env.spawn_prob_grid.astype(np.float32)

    max_count = float(np.max(spawn_count))
    if max_count > 0.0:
        state[:, :, 9] = spawn_count / max_count
    else:
        state[:, :, 9] = 0.0
    state[:, :, 10] = spawn_prob

    # 0: agent position
    ar, ac = agent_loc
    state[ar, ac, 0] = 1.0

    # Distance normalization factor
    finite = env._dist_matrix[np.isfinite(env._dist_matrix)]
    max_dist = float(np.max(finite)) if finite.size > 0 else 1.0
    if max_dist <= 0.0:
        max_dist = 1.0

    # 3: dist agent->cell (normalized)
    agent_idx = _coord_to_idx(env, agent_loc)
    dist_agent_grid = env._dist_from_idx_grids[agent_idx]
    state[:, :, 3] = dist_agent_grid / max_dist

    # 4: dist base->cell (normalized)
    dist_base_grid = env._dist_base_grid
    state[:, :, 4] = dist_base_grid / max_dist

    # 7: step norm
    state[:, :, 7] = float(step) / float(env.episode_steps)

    # 8: load norm
    state[:, :, 8] = float(agent_load) / float(env.agent_capacity)

    # Items: channels 1,2,5 + NEW channel 11 priority ratio
    mrt = float(env.max_response_time)
    for (loc, age) in zip(item_locs, item_times):
        r, c = loc
        age_f = float(age)
        rem = max(0.0, mrt - age_f)

        # 1,2,5 (old)
        state[r, c, 1] = 1.0
        state[r, c, 2] = age_f / mrt
        state[r, c, 5] = rem / mrt

        # 11: priority ratio (new)
        d_agent = float(dist_agent_grid[r, c])  # steps (graph)
        d_target = float(dist_base_grid[r, c])  # steps (graph)
        t_i = max(1.0, rem)  # avoid div by 0

        pr = (1.0 / (1.0 + d_agent) + 1.0 / (1.0 + d_target)) * (1.0 / t_i)
        pr_norm = np.clip(pr / 2.0, 0.0, 1.0)
        state[r, c, 11] = pr_norm

    return state.reshape(-1)


# ============================================================
# 3. Minimal renderer (optional)
# ============================================================

class PrettyRenderer:
    def __init__(self, env, fps=8, render_every=2):
        self.env = env
        self.pause = 1.0 / max(1e-3, fps)
        self.render_every = max(1, int(render_every))
        self.fig = None
        self.ax = None
        self.agent_artist = None
        self.target_artist = None
        self.item_artists = []

    def _setup_canvas(self):
        if self.fig is not None:
            return
        plt.ion()
        self.fig, self.ax = plt.subplots(figsize=(5.6, 5.6))
        ax = self.ax
        ax.set_aspect("equal")
        H, W = self.env.vertical_cell_count, self.env.horizontal_cell_count
        ax.set_xlim(-0.5, W - 0.5)
        ax.set_ylim(H - 0.5, -0.5)
        ax.set_xticks(np.arange(-0.5, W, 1), minor=True)
        ax.set_yticks(np.arange(-0.5, H, 1), minor=True)
        ax.grid(which="minor", linestyle="-", linewidth=0.6, alpha=0.28)
        ax.set_xticks([])
        ax.set_yticks([])

        for r in range(H):
            for c in range(W):
                is_hole = (self.env.variant == 2) and ((r, c) not in self.env.eligible_cells)
                col = (0.965, 0.965, 1.0) if not is_hole else (0.85, 0.85, 0.88)
                ax.add_patch(Rectangle((c - 0.5, r - 0.5), 1, 1, facecolor=col, edgecolor="none", zorder=0))

        tr, tc = self.env.target_loc
        self.target_artist = ax.text(tc, tr, "T", ha="center", va="center",
                                     fontsize=18, fontweight="bold", zorder=3)
        ar, ac = self.env.agent_loc
        self.agent_artist = ax.text(ac, ar, "A", ha="center", va="center",
                                    fontsize=20, color="blue", fontweight="bold", zorder=4)

    def draw(self, step, ep_return, action=None):
        if step % self.render_every != 0:
            return
        self._setup_canvas()
        ax = self.ax

        for it in self.item_artists:
            it.remove()
        self.item_artists.clear()

        for (ir, ic) in self.env.item_locs:
            txt = ax.text(ic, ir, "*", ha="center", va="center", fontsize=18, color="orange", zorder=3)
            self.item_artists.append(txt)

        ar, ac = self.env.agent_loc
        self.agent_artist.set_position((ac, ar))
        names = {0: "idle", 1: "up", 2: "right", 3: "down", 4: "left"}
        ax.set_title(f"Step {step} | Return: {ep_return:.2f} | Action: {names.get(action, '-')}",
                     fontsize=11, pad=8)
        plt.pause(self.pause)

    def close(self):
        plt.ioff()
        if self.fig is not None:
            plt.show()


# ============================================================
# 4. Action masking
# ============================================================

def compute_valid_actions(env):
    valid = [0]  # idle always allowed
    r, c = env.agent_loc
    H, W = env.vertical_cell_count, env.horizontal_cell_count

    moves = [
        (1, -1, 0),  # up
        (2, 0, 1),   # right
        (3, 1, 0),   # down
        (4, 0, -1),  # left
    ]
    for act, dr, dc in moves:
        nr, nc = r + dr, c + dc
        if 0 <= nr < H and 0 <= nc < W and (nr, nc) in env.eligible_cells:
            valid.append(act)

    return np.array(valid, dtype=np.int32)


# ============================================================
# 5. GymEnvironment wrapper
# ============================================================

class GymEnvironment:
    def __init__(self, env_id, save_path, render=False, variant=0, data_dir="./data",
                 pretty_render=False, fps=8, render_every=2):
        self.env = Environment(variant=variant, data_dir=data_dir)
        self.variant = variant
        self.max_timesteps = int(getattr(self.env, "episode_steps", 200))
        self.save_path = save_path
        self.pretty_render = pretty_render
        self.fps = fps
        self.render_every = render_every

    def trainDQN(self, agent, no_episodes):
        rew = self.runDQN(agent, no_episodes, training=True, evaluation=False)
        agent.model.save_weights(
            os.path.join(self.save_path, f"dueling_double_dqn_variant{self.variant}.weights.h5"),
            overwrite=True
        )
        return rew

    def runDQN(self, agent, no_episodes, training=False, evaluation=False):
        rew = np.zeros(no_episodes, dtype=np.float32)
        viz = PrettyRenderer(self.env, fps=self.fps, render_every=self.render_every) \
            if (self.pretty_render and training) else None

        for episode in range(no_episodes):
            if training:
                if episode < 400:
                    update_every = 8
                    per_update_steps = 1
                else:
                    update_every = 4
                    per_update_steps = 1
            else:
                update_every = 4
                per_update_steps = 1

            mode = "training" if training else "validation"
            obs = self.env.reset(mode=mode)
            state_vec = encode_obs(obs, self.env).reshape(1, -1)

            done = 0
            rwd = 0.0
            t = 0
            if viz:
                viz.draw(step=t, ep_return=rwd, action=None)

            step_since_update = 0

            while not done and t < self.max_timesteps:
                valid_actions = compute_valid_actions(self.env)
                action = agent.select_action(state_vec, valid_actions=valid_actions, training=training)

                reward, next_obs, done = self.env.step(action)
                next_state_vec = encode_obs(next_obs, self.env).reshape(1, -1)
                rwd += float(reward)

                truncated = (t + 1 >= self.max_timesteps)
                terminal_flag = float(done or truncated)

                if training and not evaluation:
                    agent.record(state_vec, action, reward, next_state_vec, terminal_flag)
                    step_since_update += 1
                    if step_since_update % update_every == 0:
                        for _ in range(per_update_steps):
                            agent.update_weights()

                state_vec = next_state_vec
                t += 1
                if viz:
                    viz.draw(step=t, ep_return=rwd, action=action)
                if truncated:
                    break

            rew[episode] = rwd

            if training:
                if (episode + 1) % 25 == 0:
                    start = max(0, episode + 1 - 25)
                    avg25 = float(np.mean(rew[start:episode + 1]))
                    print(f"[TRAIN] Variant {self.variant} Episode {episode + 1}/{no_episodes}  "
                          f"Return: {rwd:.1f}  |  Avg last 25: {avg25:.1f}")
                agent.update_epsilon()
            else:
                print(f"[VAL]   Variant {self.variant} Episode {episode + 1}/{no_episodes}  Return: {rwd:.1f}")

        if viz:
            viz.close()
        return rew


# ============================================================
# 6. Replay buffers (PER fixed)
# ============================================================

class UniformReplayBuffer:
    def __init__(self, state_dim, capacity=50_000):
        self.capacity = int(capacity)
        self.ptr = 0
        self.n = 0
        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity,), dtype=np.int32)
        self.rewards = np.zeros((capacity,), dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros((capacity,), dtype=np.float32)

    def __len__(self):
        return self.n

    def add(self, s, a, r, ns, d):
        i = self.ptr
        self.states[i] = s
        self.actions[i] = a
        self.rewards[i] = r
        self.next_states[i] = ns
        self.dones[i] = d
        self.ptr = (self.ptr + 1) % self.capacity
        self.n = min(self.n + 1, self.capacity)

    def sample(self, batch_size):
        idxs = np.random.randint(self.n, size=batch_size)
        weights = np.ones((batch_size,), dtype=np.float32)
        return (
            self.states[idxs],
            self.actions[idxs],
            self.rewards[idxs],
            self.next_states[idxs],
            self.dones[idxs],
            idxs,
            weights
        )

    def update_priorities(self, idxs, td_errors):
        pass


class SumTree:
    def __init__(self, capacity: int):
        self.capacity = 1
        while self.capacity < capacity:
            self.capacity <<= 1
        self.tree = np.zeros(2 * self.capacity, dtype=np.float32)

    def total(self):
        return float(self.tree[1])

    def add(self, p: float, idx: int):
        i = idx + self.capacity
        delta = p - self.tree[i]
        self.tree[i] = p
        i //= 2
        while i >= 1:
            self.tree[i] += delta
            i //= 2

    def find_prefixsum_idx(self, mass: float) -> int:
        i = 1
        while i < self.capacity:
            left = 2 * i
            if self.tree[left] >= mass:
                i = left
            else:
                mass -= self.tree[left]
                i = left + 1
        return i - self.capacity


class PrioritizedReplayBuffer:
    def __init__(self, state_dim, capacity=50_000, alpha=0.6, beta0=0.4,
                 beta_final=1.0, beta_anneal_steps=200_000, eps=1e-6):
        self.capacity = int(capacity)
        self.alpha = float(alpha)
        self.eps = float(eps)
        self.beta0 = float(beta0)
        self.beta_final = float(beta_final)
        self.beta_anneal_steps = max(1, int(beta_anneal_steps))
        self.ptr = 0
        self.n = 0

        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity,), dtype=np.int32)
        self.rewards = np.zeros((capacity,), dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros((capacity,), dtype=np.float32)

        self.sumtree = SumTree(capacity)
        self.max_priority = 1.0
        self._beta_updates = 0

    @property
    def beta(self):
        t = min(1.0, self._beta_updates / float(self.beta_anneal_steps))
        return (1.0 - t) * self.beta0 + t * self.beta_final

    def __len__(self):
        return self.n

    def add(self, s, a, r, ns, d):
        i = self.ptr
        self.states[i] = s
        self.actions[i] = a
        self.rewards[i] = r
        self.next_states[i] = ns
        self.dones[i] = d

        p = (self.max_priority + self.eps) ** self.alpha
        self.sumtree.add(float(p), i)

        self.ptr = (self.ptr + 1) % self.capacity
        self.n = min(self.n + 1, self.capacity)

    def sample(self, batch_size):
        total = self.sumtree.total()
        seg = total / max(1, batch_size)

        idxs = np.empty((batch_size,), dtype=np.int32)
        for i in range(batch_size):
            mass = np.random.uniform(i * seg, (i + 1) * seg)
            idx = self.sumtree.find_prefixsum_idx(mass)
            if idx >= self.n:
                idx = np.random.randint(self.n)
            idxs[i] = idx

        leaf_start = self.sumtree.capacity
        p = self.sumtree.tree[leaf_start + idxs]
        p = np.clip(p, 1e-12, None)

        P = p / max(1e-12, total)
        self._beta_updates += 1
        w = (self.n * P) ** (-self.beta)
        w = w / (w.max() + 1e-12)

        return (
            self.states[idxs],
            self.actions[idxs],
            self.rewards[idxs],
            self.next_states[idxs],
            self.dones[idxs],
            idxs,
            w.astype(np.float32)
        )

    def update_priorities(self, idxs, td_errors):
        prios = (np.abs(td_errors) + self.eps) ** self.alpha
        self.max_priority = max(self.max_priority, float(np.max(prios)))
        for i, p in zip(idxs, prios):
            self.sumtree.add(float(p), int(i))


# ============================================================
# 7. Noisy layer
# ============================================================

class NoisyDense(tf.keras.layers.Layer):
    def __init__(self, units, activation=None, sigma0=0.5, **kwargs):
        super().__init__(**kwargs)
        self.units = int(units)
        self.activation = tf.keras.activations.get(activation)
        self.sigma0 = float(sigma0)

    def build(self, input_shape):
        in_dim = int(input_shape[-1])
        mu_range = 1.0 / np.sqrt(in_dim)

        self.mu_w = self.add_weight(
            name="mu_w",
            shape=(in_dim, self.units),
            initializer=tf.keras.initializers.RandomUniform(-mu_range, mu_range),
            trainable=True,
        )
        self.sigma_w = self.add_weight(
            name="sigma_w",
            shape=(in_dim, self.units),
            initializer=tf.keras.initializers.Constant(self.sigma0 / np.sqrt(in_dim)),
            trainable=True,
        )
        self.mu_b = self.add_weight(
            name="mu_b",
            shape=(self.units,),
            initializer=tf.keras.initializers.RandomUniform(-mu_range, mu_range),
            trainable=True,
        )
        self.sigma_b = self.add_weight(
            name="sigma_b",
            shape=(self.units,),
            initializer=tf.keras.initializers.Constant(self.sigma0 / np.sqrt(in_dim)),
            trainable=True,
        )
        super().build(input_shape)

    def call(self, inputs):
        def f(x):
            return tf.sign(x) * tf.sqrt(tf.abs(x) + 1e-12)

        in_dim = tf.shape(self.mu_w)[0]
        eps_in = f(tf.random.normal((in_dim,)))
        eps_out = f(tf.random.normal((self.units,)))

        eps_w = tf.tensordot(eps_in, eps_out, axes=0)
        w = self.mu_w + self.sigma_w * eps_w
        b = self.mu_b + self.sigma_b * eps_out

        out = tf.matmul(inputs, w) + b
        if self.activation is not None:
            out = self.activation(out)
        return out


# ============================================================
# 8. DQN Agent (dueling + double, noisy, PER, optional dist)
# ============================================================

class DQN_Agent:
    def __init__(self, state_size, no_of_actions, agent_hyperparameters=None, old_model_path=""):
        if agent_hyperparameters is None:
            agent_hyperparameters = {}
        self.state_size = int(state_size)
        self.action_size = int(no_of_actions)

        # base hparams
        self.gamma = float(agent_hyperparameters.get("gamma", 0.99))
        self.epsilon = float(agent_hyperparameters.get("epsilon", 1.0))
        self.batch_size = int(agent_hyperparameters.get("batch_size", 32))
        self.epsilon_min = float(agent_hyperparameters.get("epsilon_min", 0.05))
        self.epsilon_decay = float(agent_hyperparameters.get("epsilon_decay", 0.995))
        self.units = int(agent_hyperparameters.get("units", 64))

        self.use_per = bool(agent_hyperparameters.get("use_per", True))

        per_alpha = float(agent_hyperparameters.get("per_alpha", 0.6))
        per_beta0 = float(agent_hyperparameters.get("per_beta0", 0.4))
        per_beta1 = float(agent_hyperparameters.get("per_beta1", 1.0))
        per_eps = float(agent_hyperparameters.get("per_eps", 1e-6))
        per_anneal = int(agent_hyperparameters.get("per_beta_anneal_steps", 200_000))

        # Rainbow bits
        self.n_step = int(agent_hyperparameters.get("n_step", 1))
        self.use_noisy = bool(agent_hyperparameters.get("use_noisy", False))
        self.noisy_sigma0 = float(agent_hyperparameters.get("noisy_sigma0", 0.5))

        self.use_distributional = bool(agent_hyperparameters.get("use_distributional", False))
        self.num_atoms = int(agent_hyperparameters.get("num_atoms", 51))
        self.vmin = float(agent_hyperparameters.get("vmin", -200.0))
        self.vmax = float(agent_hyperparameters.get("vmax", 200.0))
        if self.use_distributional:
            self.z = np.linspace(self.vmin, self.vmax, self.num_atoms).astype(np.float32)
        else:
            self.z = None

        self.gamma_n = self.gamma ** self.n_step

        # soft target update
        self.tau = float(agent_hyperparameters.get("tau", 0.001))

        # networks
        if self.use_distributional:
            self.model = self._build_distributional_model(self.state_size, self.action_size, self.units, old_model_path)
            self.target_model = self._build_distributional_model(self.state_size, self.action_size, self.units)
        else:
            self.model = self._build_model(self.state_size, self.action_size, self.units, old_model_path)
            self.target_model = self._build_model(self.state_size, self.action_size, self.units)

        self.target_model.set_weights(self.model.get_weights())

        # replay buffer
        if self.use_per:
            self.memory = PrioritizedReplayBuffer(
                state_dim=self.state_size,
                capacity=50_000,
                alpha=per_alpha,
                beta0=per_beta0,
                beta_final=per_beta1,
                beta_anneal_steps=per_anneal,
                eps=per_eps
            )
        else:
            self.memory = UniformReplayBuffer(state_dim=self.state_size, capacity=50_000)

        self.learn_start = max(5000, 5 * self.batch_size)
        self.total_updates = 0

        # n-step buffer
        self._n_step_buffer = deque(maxlen=self.n_step)

    def _dense_layer(self, units, activation=None):
        if self.use_noisy:
            return NoisyDense(units, activation=activation, sigma0=self.noisy_sigma0)
        return Dense(units, activation=activation)

    def _build_model(self, state_size, action_size, units, old_model_path=""):
        DenseLayer = self._dense_layer

        inp = Input(shape=(state_size,))
        x = Reshape((HEIGHT, WIDTH, CHANNELS))(inp)

        x = Conv2D(32, kernel_size=3, padding="same", activation="relu")(x)
        x = Conv2D(64, kernel_size=3, padding="same", activation="relu")(x)
        x = Flatten()(x)
        x = DenseLayer(units, activation="relu")(x)

        # dueling heads
        adv = DenseLayer(units, activation="relu")(x)
        adv = DenseLayer(action_size)(adv)

        val = DenseLayer(units, activation="relu")(x)
        val = DenseLayer(1)(val)

        def combine_streams(tensors):
            v, a = tensors
            a_mean = tf.reduce_mean(a, axis=1, keepdims=True)
            return v + (a - a_mean)

        q = Lambda(combine_streams, name="dueling_combine")([val, adv])

        model = Model(inputs=inp, outputs=q)
        model.compile(
            optimizer=Adam(learning_rate=3e-4, clipnorm=10.0),
            loss=Huber(delta=1.0),
            run_eagerly=False
        )
        if old_model_path:
            model.load_weights(old_model_path)
        return model

    def _build_distributional_model(self, state_size, action_size, units, old_model_path=""):
        DenseLayer = self._dense_layer

        inp = Input(shape=(state_size,))
        x = Reshape((HEIGHT, WIDTH, CHANNELS))(inp)

        x = Conv2D(32, kernel_size=3, padding="same", activation="relu")(x)
        x = Conv2D(64, kernel_size=3, padding="same", activation="relu")(x)
        x = Flatten()(x)
        x = DenseLayer(units, activation="relu")(x)

        v = DenseLayer(units, activation="relu")(x)
        v = DenseLayer(self.num_atoms)(v)

        a = DenseLayer(units, activation="relu")(x)
        a = DenseLayer(action_size * self.num_atoms)(a)

        def dist_dueling(tensors):
            v_, a_ = tensors
            batch = tf.shape(v_)[0]
            v_ = tf.reshape(v_, (batch, 1, self.num_atoms))
            a_ = tf.reshape(a_, (batch, action_size, self.num_atoms))
            a_mean = tf.reduce_mean(a_, axis=1, keepdims=True)
            q_logits = v_ + (a_ - a_mean)
            return tf.nn.softmax(q_logits, axis=-1)

        out = Lambda(dist_dueling, name="dist_dueling")([v, a])

        model = Model(inputs=inp, outputs=out)
        model.compile(
            optimizer=Adam(learning_rate=3e-4, clipnorm=10.0),
            loss="categorical_crossentropy",
            run_eagerly=False
        )
        if old_model_path:
            model.load_weights(old_model_path)
        return model

    def select_action(self, state, valid_actions=None, training=True):
        if valid_actions is None:
            valid_actions = np.arange(self.action_size, dtype=np.int32)

        if training and (np.random.rand() < self.epsilon):
            return int(np.random.choice(valid_actions))

        if self.use_distributional:
            dist = self.model(state, training=False).numpy()[0]  # (A, atoms)
            q = np.sum(self.z * dist, axis=1)
        else:
            q = self.model(state, training=False).numpy()[0]

        masked_q = np.full_like(q, -1e9, dtype=np.float32)
        masked_q[valid_actions] = q[valid_actions]
        return int(np.argmax(masked_q))

    def _add_single_transition(self, state, action, reward, next_state, done):
        self.memory.add(state.reshape(-1), int(action), float(reward),
                        next_state.reshape(-1), float(done))

    def record(self, state, action, reward, next_state, done):
        if self.n_step <= 1:
            self._add_single_transition(state, action, reward, next_state, done)
            return

        self._n_step_buffer.append((state, action, reward, next_state, done))

        if len(self._n_step_buffer) == self.n_step and not done:
            self._flush_n_step()
            self._n_step_buffer.popleft()

        if done:
            while len(self._n_step_buffer) > 0:
                self._flush_n_step()
                self._n_step_buffer.popleft()

    def _flush_n_step(self):
        R = 0.0
        gamma = 1.0
        for (_, _, r, _, d) in self._n_step_buffer:
            R += gamma * float(r)
            gamma *= self.gamma
            if d:
                break

        state_0, action_0, _, _, _ = self._n_step_buffer[0]
        _, _, _, next_state_last, done_last = self._n_step_buffer[-1]

        self._add_single_transition(state_0, action_0, R, next_state_last, done_last)

    def update_epsilon(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def _soft_update_target(self):
        w = self.model.get_weights()
        tw = self.target_model.get_weights()
        self.target_model.set_weights([
            self.tau * wi + (1.0 - self.tau) * twi
            for wi, twi in zip(w, tw)
        ])

    def _valid_action_mask_from_states(self, states):
        batch = states.shape[0]
        grid = states.reshape(batch, HEIGHT, WIDTH, CHANNELS)

        mask = np.zeros((batch, self.action_size), dtype=bool)
        mask[:, 0] = True

        agent_layer = grid[:, :, :, 0]
        eligible_layer = grid[:, :, :, 6] > 0.5

        moves = [
            (1, -1, 0),
            (2, 0, 1),
            (3, 1, 0),
            (4, 0, -1),
        ]

        for b in range(batch):
            pos = np.argwhere(agent_layer[b] > 0.5)
            if pos.size == 0:
                continue
            ar, ac = pos[0]
            for act, dr, dc in moves:
                nr, nc = ar + dr, ac + dc
                if 0 <= nr < HEIGHT and 0 <= nc < WIDTH and eligible_layer[b, nr, nc]:
                    mask[b, act] = True

        return mask

    def update_weights(self):
        if len(self.memory) < self.learn_start:
            return

        S, A, R, NS, D, idxs, is_w = self.memory.sample(self.batch_size)
        valid_next_mask = self._valid_action_mask_from_states(NS)
        batch_idx = np.arange(self.batch_size, dtype=np.int32)

        if not self.use_distributional:
            q_next_online = self.model(NS, training=False).numpy()
            q_next_online[~valid_next_mask] = -1e9
            best_next_actions = np.argmax(q_next_online, axis=1)

            q_next_target = self.target_model(NS, training=False).numpy()
            q_next_target[~valid_next_mask] = -1e9
            best_next_q = q_next_target[batch_idx, best_next_actions]

            Tgt = R + self.gamma_n * best_next_q * (1.0 - D)

            q_curr = self.model(S, training=False).numpy()
            q_chosen = q_curr[batch_idx, A]
            td_errors = Tgt - q_chosen

            q_curr[batch_idx, A] = Tgt

            # apply PER importance weights
            self.model.train_on_batch(S, q_curr, sample_weight=is_w)
            self.memory.update_priorities(idxs, np.abs(td_errors))

        else:
            dist_next_online = self.model(NS, training=False).numpy()
            dist_next_target = self.target_model(NS, training=False).numpy()

            q_next_online = np.sum(self.z[None, None, :] * dist_next_online, axis=2)
            q_next_online[~valid_next_mask] = -1e9
            best_next_actions = np.argmax(q_next_online, axis=1)

            next_dist = dist_next_target[batch_idx, best_next_actions, :]

            Tz = R[:, None] + (self.gamma_n * (1.0 - D))[:, None] * self.z[None, :]
            Tz = np.clip(Tz, self.vmin, self.vmax)

            b = (Tz - self.vmin) / (self.vmax - self.vmin) * (self.num_atoms - 1)
            l = np.floor(b).astype(np.int64)
            u = np.ceil(b).astype(np.int64)
            l = np.clip(l, 0, self.num_atoms - 1)
            u = np.clip(u, 0, self.num_atoms - 1)

            m = np.zeros((self.batch_size, self.num_atoms), dtype=np.float32)
            for i in range(self.batch_size):
                for j in range(self.num_atoms):
                    lj, uj, bj = l[i, j], u[i, j], b[i, j]
                    p = next_dist[i, j]
                    if lj == uj:
                        m[i, lj] += p
                    else:
                        m[i, lj] += p * (uj - bj)
                        m[i, uj] += p * (bj - lj)

            dist_curr = self.model(S, training=False).numpy()
            dist_a = dist_curr[batch_idx, A, :]

            eps = 1e-8
            m_safe = np.clip(m, eps, 1.0)
            dist_a_safe = np.clip(dist_a, eps, 1.0)
            kl = np.sum(m_safe * (np.log(m_safe) - np.log(dist_a_safe)), axis=1)

            y = dist_curr.copy()
            y[batch_idx, A, :] = m

            self.model.train_on_batch(S, y, sample_weight=is_w)
            self.memory.update_priorities(idxs, kl)

        self._soft_update_target()
        self.total_updates += 1


# ============================================================
# 9. Evaluation
# ============================================================

def evaluate_average_reward(agent, variant, data_dir, episodes=100, mode="testing"):
    env = Environment(variant=variant, data_dir=data_dir)
    max_steps = min(int(getattr(env, "episode_steps", 200)), 200)

    if mode == "testing":
        max_eps = min(episodes, len(env.test_episodes))
    elif mode == "validation":
        max_eps = min(episodes, len(env.validation_episodes))
    else:
        max_eps = episodes

    total = 0.0
    for _ in range(max_eps):
        obs = env.reset(mode)
        s_vec = encode_obs(obs, env).reshape(1, -1)
        done = 0
        ep_ret = 0.0
        steps = 0

        while not done and steps < max_steps:
            valid_actions = compute_valid_actions(env)
            a = agent.select_action(s_vec, valid_actions=valid_actions, training=False)
            r, nxt, done = env.step(a)
            s_vec = encode_obs(nxt, env).reshape(1, -1)
            ep_ret += float(r)
            steps += 1

        total += ep_ret

    return total / float(max_eps)


# ============================================================
# 10. Main: Train (800) + Val/Test (100/100) + Plot
# ============================================================

if __name__ == "__main__":
    # ---- Focus variant 2 (barriers) ----
    VARIANT = 2

    TRAIN_EPISODES_FULL = 800
    VAL_EPISODES_FINAL = 100
    TEST_EPISODES_FINAL = 100
    NO_OF_ACTIONS = 5

    # Greedy benchmark (set your known value here if you have it)
    GREEDY_BENCHMARK = None  # e.g. 240.0

    # ---- Your BO-best "Combo 2" (from your earlier message) ----
    CONFIG_COMBO2 = dict(
        gamma=0.9389065584049,
        epsilon=1.0,
        epsilon_decay=0.99,
        epsilon_min=0.079088807071618797,
        batch_size=32,
        units=128,
        tau=0.0010323839507959762,
        use_per=True,
        per_alpha=0.4,
        per_beta0=0.2,
        per_beta1=1.0,
        per_eps=1e-6,
        per_beta_anneal_steps=200_000,
        n_step=1,
        use_noisy=True,
        noisy_sigma0=0.12546649741233012,
        use_distributional=False,
        num_atoms=51,
        vmin=-200.0,
        vmax=200.0,
        variant_name="Combo 2",
    )

    wd = os.getcwd()
    save_folder = os.path.join(wd, "save_folder_variant2_priority_ratio")
    os.makedirs(save_folder, exist_ok=True)

    # ---- Determine state size once ----
    env_tmp = Environment(variant=VARIANT, data_dir=DATA_DIR)
    obs0 = env_tmp.reset("training")
    state_sample = encode_obs(obs0, env_tmp)
    state_size = int(state_sample.shape[0])

    print(f"Variant {VARIANT}: inferred state_size = {state_size} (=5*5*{CHANNELS}), action_size = {NO_OF_ACTIONS}")

    # ---- Train ----
    agent = DQN_Agent(state_size, NO_OF_ACTIONS, CONFIG_COMBO2)

    env_train = GymEnvironment(
        env_id="env_combo2_priority_ratio",
        save_path=save_folder,
        render=False,
        variant=VARIANT,
        data_dir=DATA_DIR,
        pretty_render=False,
        fps=8,
        render_every=2
    )

    rew_train = env_train.trainDQN(agent, TRAIN_EPISODES_FULL)

    # ---- Validation & Test ----
    avg_val = evaluate_average_reward(agent, VARIANT, DATA_DIR, episodes=VAL_EPISODES_FINAL, mode="validation")
    avg_test = evaluate_average_reward(agent, VARIANT, DATA_DIR, episodes=TEST_EPISODES_FINAL, mode="testing")

    print(f"[Combo 2] Validation avg over {VAL_EPISODES_FINAL} episodes: {avg_val:.3f}")
    print(f"[Combo 2] Test       avg over {TEST_EPISODES_FINAL} episodes: {avg_test:.3f}")

    # ---- Plot ----
    plt.figure(figsize=(14, 6))
    window = 25
    kernel = np.ones(window, dtype=np.float32) / float(window)

    r = np.array(rew_train, dtype=np.float32)
    x = np.arange(1, len(r) + 1)

    raw_line, = plt.plot(x, r, alpha=0.15, label="Combo 2 – Train (raw)")

    if len(r) >= window:
        mov = np.convolve(r, kernel, mode="valid")
        xm = np.arange(window, len(r) + 1)
        plt.plot(xm, mov, label="Combo 2 – Train (25-ep avg)", color=raw_line.get_color())

    plt.axhline(y=avg_val, linestyle=":", color=raw_line.get_color(), alpha=0.9,
                label=f"Combo 2 – Val avg ({avg_val:.1f})")
    plt.axhline(y=avg_test, linestyle="--", color=raw_line.get_color(), alpha=0.9,
                label=f"Combo 2 – Test avg ({avg_test:.1f})")

    if GREEDY_BENCHMARK is not None:
        plt.axhline(y=float(GREEDY_BENCHMARK), linestyle="-.", color="black", alpha=0.8,
                    label=f"Greedy benchmark ({float(GREEDY_BENCHMARK):.2f})")

    plt.title(f"Variant {VARIANT} – Combo 2 + Priority Ratio Feature\n"
              f"{TRAIN_EPISODES_FULL} Training, {VAL_EPISODES_FINAL} Validation, {TEST_EPISODES_FINAL} Testing")
    plt.xlabel("Training episode")
    plt.ylabel("Reward")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()

    out_plot = os.path.join(save_folder, f"variant{VARIANT}_combo2_priority_ratio.png")
    plt.savefig(out_plot, dpi=150)
    print(f"\nSaved plot to: {out_plot}")
    plt.show()
