# ============================================================
# Noisy Dueling Double-DQN with CNN (variant 0)
# Encoder-Vergleich:
#   - jede Feature-Variante: 300 Training, 100 Validation
#   - beste Variante: 800 Training, 100 Validation, 100 Testing
# ============================================================

# ------------- Reproducibility / Quiet logs -----------------
seed = 2
import os
os.environ["PYTHONHASHSEED"] = str(seed)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"   # silence TF INFO/WARN

import random
random.seed(seed)

import numpy as np
np.random.seed(seed)

import tensorflow as tf
tf.random.set_seed(seed)

# --- SciPy for shortest paths (like greedy baseline) --------
from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import dijkstra

# ============================================================
# Environment (your provided implementation)
# ============================================================
import pandas as pd
from copy import deepcopy
from itertools import compress

class Environment(object):
    # actions: 0 (nothing), 1 (up), 2 (right), 3 (down), 4 (left)
    # grid coords: (0,0) is top-left; (row, col) increases down/right
    # note: if an item appears on the same step you arrive/stay, it cannot be picked that step

    def __init__(self, variant, data_dir):
        self.variant = variant
        self.vertical_cell_count = 5
        self.horizontal_cell_count = 5
        self.vertical_idx_target = 2
        self.horizontal_idx_target = 0
        self.target_loc = (self.vertical_idx_target, self.horizontal_idx_target)
        self.episode_steps = 200
        self.max_response_time = 15 if self.variant == 2 else 10
        self.reward = 25 if self.variant == 2 else 15

        # Make data_dir absolute relative to this file if needed
        if not os.path.isabs(data_dir):
            data_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), data_dir))
        self.data_dir = data_dir

        # -------- variant-aware base path --------
        self._base = os.path.join(self.data_dir, f'variant_{self.variant}')

        # Episode id lists
        self.training_episodes = pd.read_csv(
            os.path.join(self._base, 'training_episodes.csv')
        ).training_episodes.tolist()
        self.validation_episodes = pd.read_csv(
            os.path.join(self._base, 'validation_episodes.csv')
        ).validation_episodes.tolist()
        self.test_episodes = pd.read_csv(
            os.path.join(self._base, 'test_episodes.csv')
        ).test_episodes.tolist()

        self.remaining_training_episodes = deepcopy(self.training_episodes)
        self.validation_episode_counter = 0

        # Capacity
        self.agent_capacity = 1 if self.variant in (0, 2) else 3

        # Eligible cells (holes in variant 2)
        if self.variant in (0, 1):
            self.eligible_cells = [(r, c) for r in range(5) for c in range(5)]
        else:
            self.eligible_cells = [
                (0,0),        (0,2), (0,3), (0,4),
                (1,0),        (1,2),        (1,4),
                (2,0),        (2,2),        (2,4),
                (3,0), (3,1), (3,2),        (3,4),
                (4,0), (4,1), (4,2),        (4,4)
            ]

        # -------- discover available episode files and offset --------
        ep_dir = os.path.join(self._base, 'episode_data')
        files = [f for f in os.listdir(ep_dir) if f.startswith('episode_') and f.endswith('.csv')]
        if not files:
            raise FileNotFoundError(f"No episode files found in {ep_dir}")

        def _parse_num(fname: str) -> int:
            # supports episode_000.csv and episode_100.csv etc.
            core = fname[len('episode_'):-4]
            return int(core)

        self._available_ids = sorted(_parse_num(f) for f in files)
        self._available_set = set(self._available_ids)
        self._ep_min = min(self._available_ids)  # e.g., 0 or 100

    # initialize a new episode
    def reset(self, mode):
        modes = ['training', 'validation', 'testing']
        if mode not in modes:
            raise ValueError('Invalid mode. Expected one of: %s' % modes)

        self.step_count = 0
        self.agent_loc = (self.vertical_idx_target, self.horizontal_idx_target)
        self.agent_load = 0
        self.item_locs = []
        self.item_times = []

        # choose episode id from the lists
        if mode == "testing":
            episode = self.test_episodes[0]
            self.test_episodes.remove(episode)
        elif mode == "validation":
            episode = self.validation_episodes[self.validation_episode_counter]
            self.validation_episode_counter = (self.validation_episode_counter + 1) % max(1, len(self.validation_episodes))
        else:  # training
            if not self.remaining_training_episodes:
                self.remaining_training_episodes = deepcopy(self.training_episodes)
            episode = random.choice(self.remaining_training_episodes)
            self.remaining_training_episodes.remove(episode)

        # Map the list episode id to an actual filename id:
        # - if the id exists directly, use it
        # - else, add the discovered offset (min available id)
        file_id = episode if episode in self._available_set else episode + self._ep_min

        # Build a path that works with/without zero padding
        ep_dir = os.path.join(self._base, 'episode_data')
        cand1 = os.path.join(ep_dir, f'episode_{file_id}.csv')
        cand2 = os.path.join(ep_dir, f'episode_{file_id:03d}.csv')
        if os.path.exists(cand1):
            ep_path = cand1
        elif os.path.exists(cand2):
            ep_path = cand2
        else:
            raise FileNotFoundError(
                f"Episode file not found for id {file_id}. Tried:\n  {cand1}\n  {cand2}\n"
                f"Available ids (min..max): {self._available_ids[0]}..{self._available_ids[-1]}"
            )

        # Load the per-step data
        self.data = pd.read_csv(ep_path, index_col=0)

        return self.get_obs()

    # take one environment step
    def step(self, act):
        self.step_count += 1
        rew = 0

        # episode done?
        done = 1 if self.step_count == self.episode_steps else 0

        # agent movement (only valid moves cost -1 and change position)
        if act != 0:
            if act == 1:      # up
                new_loc = (self.agent_loc[0] - 1, self.agent_loc[1])
            elif act == 2:    # right
                new_loc = (self.agent_loc[0], self.agent_loc[1] + 1)
            elif act == 3:    # down
                new_loc = (self.agent_loc[0] + 1, self.agent_loc[1])
            elif act == 4:    # left
                new_loc = (self.agent_loc[0], self.agent_loc[1] - 1)
            else:
                new_loc = self.agent_loc  # unknown action -> no-op

            if new_loc in self.eligible_cells:
                self.agent_loc = new_loc
                rew += -1  # movement cost on valid move

        # item pick-up
        if (self.agent_load < self.agent_capacity) and (self.agent_loc in self.item_locs):
            self.agent_load += 1
            idx = self.item_locs.index(self.agent_loc)
            self.item_locs.pop(idx)
            self.item_times.pop(idx)
            rew += self.reward / 2

        # item drop-off at target
        if self.agent_loc == self.target_loc and self.agent_load > 0:
            rew += self.agent_load * self.reward / 2
            self.agent_load = 0

        # advance item ages and remove expired
        self.item_times = [t + 1 for t in self.item_times]
        mask = [t < self.max_response_time for t in self.item_times]
        self.item_locs  = list(compress(self.item_locs, mask))
        self.item_times = list(compress(self.item_times, mask))

        # add items that appear this step (cannot stack two items in same cell)
        new_items_df = self.data[self.data.step == self.step_count]
        new_items = list(zip(new_items_df.vertical_idx, new_items_df.horizontal_idx))
        new_items = [loc for loc in new_items if loc not in self.item_locs]
        self.item_locs += new_items
        self.item_times += [0] * len(new_items)

        next_obs = self.get_obs()
        return rew, next_obs, done

    # minimal observation
    def get_obs(self):
        return (
            self.step_count,
            self.agent_loc,
            self.agent_load,
            tuple(self.item_locs),
            tuple(self.item_times),
        )

# ============================================================
# TF / Keras and plotting
# ============================================================
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Dense, Lambda, Reshape, Conv2D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber

from collections import deque
import warnings

# --- Matplotlib: quiet + safe font --------------------------
import matplotlib
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib.font_manager")
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
matplotlib.rcParams['axes.unicode_minus'] = False

import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

DATA_DIR = "./data"

# ============================================================
# 1. Distance structures
# ============================================================

def _build_distance_structs(env):
    if hasattr(env, "_dist_ready") and env._dist_ready:
        return

    variant = env.variant
    H, W = 5, 5

    if variant in (0, 1):
        neighbor_matrix = np.zeros((25, 25), dtype=int)
        for i in range(25):
            for j in range(i + 1, 25):
                i_vert = i // 5
                i_hori = i % 5
                j_vert = j // 5
                j_hori = j % 5
                dist_vert = j_vert - i_vert
                dist_hori = j_hori - i_hori
                if (dist_vert == 0 and dist_hori == 1) or (dist_vert == 1 and dist_hori == 0):
                    neighbor_matrix[i, j] = 1
                    neighbor_matrix[j, i] = 1

        graph = csr_matrix(neighbor_matrix)
        dist_matrix, _ = dijkstra(csgraph=graph,
                                  directed=False,
                                  return_predecessors=True,
                                  unweighted=True)

        env._mapping = None
        env._coord_to_idx = None
        env._dist_matrix = dist_matrix

        dist_grids = []
        for idx in range(25):
            grid = np.zeros((H, W), dtype=np.float32)
            for j in range(25):
                r = j // 5
                c = j % 5
                grid[r, c] = dist_matrix[idx, j]
            dist_grids.append(grid)
        env._dist_from_idx_grids = dist_grids

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        idx_base = base_coord[0] * 5 + base_coord[1]
        env._dist_base_grid = dist_grids[idx_base]

    else:
        mapping = [
            (0, 0), (1, 0), (2, 0), (3, 0), (4, 0),
            (3, 1), (4, 1),
            (0, 2), (1, 2), (2, 2), (3, 2), (4, 2),
            (0, 3),
            (0, 4), (1, 4), (2, 4), (3, 4), (4, 4)
        ]
        env._mapping = mapping
        env._coord_to_idx = {coord: i for i, coord in enumerate(mapping)}

        neighbor_matrix = np.zeros((18, 18), dtype=int)

        def link(i, j):
            neighbor_matrix[i, j] = 1
            neighbor_matrix[j, i] = 1

        link(0, 1)
        link(1, 2)
        link(2, 3)
        link(3, 4)
        link(3, 5)
        link(4, 6)
        link(5, 6)
        link(5, 10)
        link(6, 11)
        link(7, 8)
        link(7, 12)
        link(8, 9)
        link(9, 10)
        link(10, 11)
        link(12, 13)
        link(13, 14)
        link(14, 15)
        link(15, 16)
        link(16, 17)

        graph = csr_matrix(neighbor_matrix)
        dist_matrix, _ = dijkstra(csgraph=graph,
                                  directed=False,
                                  return_predecessors=True,
                                  unweighted=True)
        env._dist_matrix = dist_matrix

        dist_grids = []
        for idx in range(len(mapping)):
            grid = np.zeros((H, W), dtype=np.float32)
            for j, coord in enumerate(mapping):
                r, c = coord
                grid[r, c] = dist_matrix[idx, j]
            dist_grids.append(grid)
        env._dist_from_idx_grids = dist_grids

        base_coord = (env.vertical_idx_target, env.horizontal_idx_target)
        idx_base = env._coord_to_idx[base_coord]
        env._dist_base_grid = dist_grids[idx_base]

    env._dist_ready = True


def _coord_to_idx(env, coord):
    if env.variant in (0, 1):
        return coord[0] * 5 + coord[1]
    else:
        return env._coord_to_idx[coord]


# ============================================================
# 2. CNN observation encoding (14 channels)
# ============================================================

HEIGHT = 5
WIDTH = 5
CHANNELS = 14  # we now use 14 channels for all variants


def encode_obs_14_base(obs, env):
    """
    Full 14-feature encoder. Andere Varianten werden davon abgeleitet,
    indem bestimmte Kanäle auf 0 gesetzt werden.
    """
    step, agent_loc, agent_load, item_locs, item_times = obs

    _build_distance_structs(env)

    H, W, C = HEIGHT, WIDTH, CHANNELS
    S = np.zeros((H, W, C), dtype=np.float32)

    # 6: eligible mask
    eligible_set = set(env.eligible_cells)
    for r in range(H):
        for c in range(W):
            S[r, c, 6] = 1.0 if (r, c) in eligible_set else 0.0

    # 0: agent position
    ar, ac = agent_loc
    S[ar, ac, 0] = 1.0

    # 1,2,5: items
    for (loc, age) in zip(item_locs, item_times):
        r, c = loc
        S[r, c, 1] = 1.0
        S[r, c, 2] = age / env.max_response_time
        S[r, c, 5] = (env.max_response_time - age) / env.max_response_time

    # 3: distance agent->cell (normalized)
    agent_idx = _coord_to_idx(env, agent_loc)
    dist_agent_grid = env._dist_from_idx_grids[agent_idx]
    finite = env._dist_matrix[env._dist_matrix < np.inf]
    max_dist = np.max(finite) if finite.size > 0 else 1.0
    if max_dist <= 0:
        max_dist = 1.0
    S[:, :, 3] = dist_agent_grid / max_dist

    # 4: distance base->cell (normalized with same max_dist)
    dist_base_grid = env._dist_base_grid
    S[:, :, 4] = dist_base_grid / max_dist

    # 7: step norm
    S[:, :, 7] = step / float(env.episode_steps)

    # 8: load norm
    S[:, :, 8] = agent_load / float(env.agent_capacity)

    # -------- extra global features (9–13) --------
    # 9: number of items / capacity
    S[:, :, 9] = len(item_locs) / float(max(1, env.agent_capacity))

    # 10: item density
    S[:, :, 10] = len(item_locs) / 25.0

    # 11: distance to nearest item (already normalized), or 1 if none
    if item_locs:
        d_nearest = min(dist_agent_grid[r, c] for (r, c) in item_locs)
        S[:, :, 11] = d_nearest
    else:
        S[:, :, 11] = 1.0

    # 12: min remaining time over all items normalised, 1 if none
    if item_times:
        min_rem = min(env.max_response_time - t for t in item_times) / env.max_response_time
    else:
        min_rem = 1.0
    S[:, :, 12] = float(min_rem)

    # 13: average age normalized
    if item_times:
        avg_age = (sum(item_times) / len(item_times)) / env.max_response_time
    else:
        avg_age = 0.0
    S[:, :, 13] = float(avg_age)

    return S.reshape(-1)


def _mask_channels(base_vec, keep_channels):
    grid = base_vec.reshape(HEIGHT, WIDTH, CHANNELS)
    mask = np.zeros(CHANNELS, dtype=bool)
    for ch in keep_channels:
        mask[ch] = True
    for ch in range(CHANNELS):
        if not mask[ch]:
            grid[:, :, ch] = 0.0
    return grid.reshape(-1)

# ------------------------------------------------------------
# Encoder-Varianten
# ------------------------------------------------------------

def encode_obs_spatial_only(obs, env):
    s = encode_obs_14_base(obs, env)
    keep = [0, 1, 3, 4, 6]
    return _mask_channels(s, keep)

def encode_obs_spatial_temporal(obs, env):
    s = encode_obs_14_base(obs, env)
    keep = [0, 1, 2, 3, 4, 5, 6]
    return _mask_channels(s, keep)

def encode_obs_spatial_step(obs, env):
    s = encode_obs_14_base(obs, env)
    keep = [0, 1, 3, 4, 6, 7]
    return _mask_channels(s, keep)

def encode_obs_spatial_load(obs, env):
    s = encode_obs_14_base(obs, env)
    keep = [0, 1, 3, 4, 6, 8]
    return _mask_channels(s, keep)

def encode_obs_globals_only(obs, env):
    s = encode_obs_14_base(obs, env)
    keep = [9, 10, 11, 12, 13]
    return _mask_channels(s, keep)

def encode_obs_all_channels(obs, env):
    s = encode_obs_14_base(obs, env)
    keep = list(range(14))
    return _mask_channels(s, keep)

def encode_obs_first9_plus_11_12_13(obs, env):
    s = encode_obs_14_base(obs, env)
    keep = list(range(0, 9)) + [11, 12, 13]
    return _mask_channels(s, keep)

def encode_obs_first9_without_agent(obs, env):
    s = encode_obs_14_base(obs, env)
    keep = [1, 2, 3, 4, 5, 6, 7, 8, 9]
    return _mask_channels(s, keep)

def encode_obs_first9_without_items(obs, env):
    s = encode_obs_14_base(obs, env)
    keep = [0, 3, 4, 6, 7, 8, 9]
    return _mask_channels(s, keep)

def encode_obs_items_and_globals(obs, env):
    s = encode_obs_14_base(obs, env)
    keep = [1, 2, 5, 9, 10, 11, 12, 13]
    return _mask_channels(s, keep)

# Optional noch vorhanden, aber nicht in der Kandidatenliste:
def encode_obs_first9_plus_10(obs, env):
    """
    Erste 9 Features + Feature 10.
    Kanäle: 0–8 und 10.
    """
    s = encode_obs_14_base(obs, env)
    keep = list(range(0, 9)) + [10]
    return _mask_channels(s, keep)


# ============================================================
# 3. Minimal renderer (unused if render=False)
# ============================================================

class PrettyRenderer:
    def __init__(self, env, fps=8, render_every=2):
        self.env = env
        self.pause = 1.0 / max(1e-3, fps)
        self.render_every = max(1, int(render_every))
        self.fig = None
        self.ax = None
        self.agent_artist = None
        self.target_artist = None
        self.item_artists = []

    def _setup_canvas(self):
        if self.fig is not None:
            return
        plt.ion()
        self.fig, self.ax = plt.subplots(figsize=(5.6, 5.6))
        ax = self.ax
        ax.set_aspect('equal')
        H, W = self.env.vertical_cell_count, self.env.horizontal_cell_count
        ax.set_xlim(-0.5, W - 0.5)
        ax.set_ylim(H - 0.5, -0.5)
        ax.set_xticks(np.arange(-.5, W, 1), minor=True)
        ax.set_yticks(np.arange(-.5, H, 1), minor=True)
        ax.grid(which='minor', linestyle='-', linewidth=0.6, alpha=0.28)
        ax.set_xticks([])
        ax.set_yticks([])
        for r in range(H):
            for c in range(W):
                is_hole = (self.env.variant == 2) and ((r, c) not in self.env.eligible_cells)
                col = (0.965, 0.965, 1.0) if not is_hole else (0.85, 0.85, 0.88)
                ax.add_patch(Rectangle((c - 0.5, r - 0.5), 1, 1,
                                       facecolor=col, edgecolor='none', zorder=0))
        tr, tc = self.env.target_loc
        self.target_artist = ax.text(tc, tr, "T", ha='center', va='center',
                                     fontsize=18, fontweight='bold', zorder=3)
        ar, ac = self.env.agent_loc
        self.agent_artist = ax.text(ac, ar, "A", ha='center', va='center',
                                    fontsize=20, color='blue', fontweight='bold', zorder=4)

    def draw(self, step, ep_return, action=None):
        if step % self.render_every != 0:
            return
        self._setup_canvas()
        ax = self.ax
        for it in self.item_artists:
            it.remove()
        self.item_artists.clear()
        for (ir, ic) in self.env.item_locs:
            txt = ax.text(ic, ir, "*", ha='center', va='center', fontsize=18,
                          color='orange', zorder=3)
            self.item_artists.append(txt)
        ar, ac = self.env.agent_loc
        self.agent_artist.set_position((ac, ar))
        names = {0: "idle", 1: "up", 2: "right", 3: "down", 4: "left"}
        ax.set_title(f"Step {step} | Return: {ep_return:.2f} | Action: {names.get(action, '-')}",
                     fontsize=11, pad=8)
        plt.pause(self.pause)

    def close(self):
        plt.ioff()
        if self.fig is not None:
            plt.show()


# 4. Action masking helper

def compute_valid_actions(env):
    valid = [0]  # idle is always allowed
    r, c = env.agent_loc
    H, W = env.vertical_cell_count, env.horizontal_cell_count

    moves = [
        (1, -1, 0),  # up
        (2, 0, 1),   # right
        (3, 1, 0),   # down
        (4, 0, -1),  # left
    ]
    for act, dr, dc in moves:
        nr, nc = r + dr, c + dc
        if 0 <= nr < H and 0 <= nc < W and (nr, nc) in env.eligible_cells:
            valid.append(act)

    return np.array(valid, dtype=np.int32)


# ============================================================
# 5. GymEnvironment wrapper
# ============================================================

class GymEnvironment:
    def __init__(self, env_id, save_path, encoder_fn,
                 render=False, variant=0, data_dir="./data",
                 pretty_render=False, fps=8, render_every=2):
        self.env = Environment(variant=variant, data_dir=data_dir)
        self.variant = variant
        self.max_timesteps = getattr(self.env, "episode_steps", 200)
        self.save_path = save_path
        self.pretty_render = pretty_render
        self.fps = fps
        self.render_every = render_every
        self.encoder_fn = encoder_fn
        self.env_id = env_id

    def trainDQN(self, agent, no_episodes):
        rew = self.runDQN(agent, no_episodes, training=True, evaluation=False)
        agent.model.save_weights(
            os.path.join(self.save_path, f"{self.env_id}_variant{self.variant}.weights.h5"),
            overwrite=True
        )
        return rew

    def runDQN(self, agent, no_episodes, training=False, evaluation=False):
        rew = np.zeros(no_episodes, dtype=np.float32)
        viz = PrettyRenderer(self.env, fps=self.fps, render_every=self.render_every) \
            if (self.pretty_render and training) else None

        for episode in range(no_episodes):
            if training:
                if episode < 400:
                    update_every = 8
                    per_update_steps = 1
                else:
                    update_every = 4
                    per_update_steps = 1
            else:
                update_every = 4
                per_update_steps = 1

            mode = "training" if training else "validation"
            obs = self.env.reset(mode=mode)
            state_vec = self.encoder_fn(obs, self.env).reshape(1, -1)
            done = 0
            rwd = 0.0
            t = 0
            if viz:
                viz.draw(step=t, ep_return=rwd, action=None)
            step_since_update = 0

            while not done and t < self.max_timesteps:
                valid_actions = compute_valid_actions(self.env)
                action = agent.select_action(state_vec, valid_actions=valid_actions, training=training)

                reward, next_obs, done = self.env.step(action)
                next_state_vec = self.encoder_fn(next_obs, self.env).reshape(1, -1)
                rwd += reward

                truncated = (t + 1 >= self.max_timesteps)
                terminal_flag = float(done or truncated)

                if training and not evaluation:
                    agent.record(state_vec, action, reward, next_state_vec, terminal_flag)
                    step_since_update += 1
                    if step_since_update % update_every == 0:
                        for _ in range(per_update_steps):
                            agent.update_weights()

                state_vec = next_state_vec
                t += 1
                if viz:
                    viz.draw(step=t, ep_return=rwd, action=action)
                if truncated:
                    break

            rew[episode] = rwd

            if training:
                if (episode + 1) % 25 == 0:
                    start = max(0, episode + 1 - 25)
                    avg25 = np.mean(rew[start:episode + 1])
                    print(f"[TRAIN] {self.env_id} Episode {episode + 1}/{no_episodes}  "
                          f"Return: {rwd:.1f}  |  Avg last 25: {avg25:.1f}")
            else:
                print(f"[VAL]   {self.env_id} Episode {episode + 1}/{no_episodes}  Return: {rwd:.1f}")

            if training:
                agent.update_epsilon()

        if viz:
            viz.close()
        return rew


# ============================================================
# 6. Replay buffers
# ============================================================

class UniformReplayBuffer:
    def __init__(self, state_dim, capacity=50_000):
        self.capacity = capacity
        self.ptr = 0
        self.n = 0
        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity,), dtype=np.int32)
        self.rewards = np.zeros((capacity,), dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros((capacity,), dtype=np.float32)

    def __len__(self):
        return self.n

    def add(self, s, a, r, ns, d):
        i = self.ptr
        self.states[i] = s
        self.actions[i] = a
        self.rewards[i] = r
        self.next_states[i] = ns
        self.dones[i] = d
        self.ptr = (self.ptr + 1) % self.capacity
        self.n = min(self.n + 1, self.capacity)

    def sample(self, batch_size):
        idxs = np.random.randint(self.n, size=batch_size)
        weights = np.ones((batch_size,), dtype=np.float32)
        return (
            self.states[idxs],
            self.actions[idxs],
            self.rewards[idxs],
            self.next_states[idxs],
            self.dones[idxs],
            idxs,
            weights
        )

    def update_priorities(self, idxs, td_errors):
        pass


class SumTree:
    def __init__(self, capacity: int):
        self.capacity = 1
        while self.capacity < capacity:
            self.capacity <<= 1
        self.tree = np.zeros(2 * self.capacity, dtype=np.float32)

    def total(self):
        return self.tree[1]

    def add(self, p: float, idx: int):
        i = idx + self.capacity
        delta = p - self.tree[i]
        self.tree[i] = p
        i //= 2
        while i >= 1:
            self.tree[i] += delta
            i //= 2

    def find_prefixsum_idx(self, mass: float) -> int:
        i = 1
        while i < self.capacity:
            left = 2 * i
            if self.tree[left] >= mass:
                i = left
            else:
                mass -= self.tree[left]
                i = left + 1
        return i - self.capacity


class PrioritizedReplayBuffer:
    def __init__(self, state_dim, capacity=50_000, alpha=0.6, beta0=0.4,
                 beta_final=1.0, beta_anneal_steps=200_000, eps=1e-6):
        self.capacity = capacity
        self.alpha = alpha
        self.eps = eps
        self.beta0 = beta0
        self.beta_final = beta_final
        self.beta_anneal_steps = max(1, beta_anneal_steps)
        self.ptr = 0
        self.n = 0

        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity,), dtype=np.int32)
        self.rewards = np.zeros((capacity,), dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros((capacity,), dtype=np.float32)

        self.sumtree = SumTree(capacity)
        self.max_priority = 1.0
        self._beta_updates = 0

    @property
    def beta(self):
        t = min(1.0, self._beta_updates / float(self.beta_anneal_steps))
        return (1.0 - t) * self.beta0 + t * self.beta_final

    def __len__(self):
        return self.n

    def add(self, s, a, r, ns, d):
        i = self.ptr
        self.states[i] = s
        self.actions[i] = a
        self.rewards[i] = r
        self.next_states[i] = ns
        self.dones[i] = d
        p = (self.max_priority + self.eps) ** self.alpha
        self.sumtree.add(p, i)
        self.ptr = (self.ptr + 1) % self.capacity
        self.n = min(self.n + 1, self.capacity)

    def sample(self, batch_size):
        total = self.sumtree.total()
        seg = total / max(1, batch_size)
        idxs = []
        for i in range(batch_size):
            mass = np.random.uniform(i * seg, (i + 1) * seg)
            idx = self.sumtree.find_prefixsum_idx(mass)
            if idx >= self.n:
                idx = np.random.randint(self.n)
            idxs.append(idx)
        idxs = np.array(idxs, dtype=np.int32)

        p = self.sumtree.tree[self.sumtree.capacity:self.sumtree.capacity + self.sumtree.capacity][idxs]
        p = np.clip(p, 1e-12, None)
        P = p / total
        self._beta_updates += 1
        w = (self.n * P) ** (-self.beta)
        w = w / (w.max() + 1e-12)

        return (
            self.states[idxs],
            self.actions[idxs],
            self.rewards[idxs],
            self.next_states[idxs],
            self.dones[idxs],
            idxs,
            w.astype(np.float32)
        )

    def update_priorities(self, idxs, td_errors):
        prios = (np.abs(td_errors) + self.eps) ** self.alpha
        self.max_priority = max(self.max_priority, float(np.max(prios)))
        for i, p in zip(idxs, prios):
            self.sumtree.add(float(p), int(i))


# ============================================================
# 7. Noisy layer
# ============================================================

class NoisyDense(tf.keras.layers.Layer):
    def __init__(self, units, activation=None, sigma0=0.5, **kwargs):
        super(NoisyDense, self).__init__(**kwargs)
        self.units = units
        self.activation = tf.keras.activations.get(activation)
        self.sigma0 = float(sigma0)

    def build(self, input_shape):
        in_dim = int(input_shape[-1])
        mu_range = 1.0 / np.sqrt(in_dim)

        self.mu_w = self.add_weight(
            name="mu_w",
            shape=(in_dim, self.units),
            initializer=tf.keras.initializers.RandomUniform(-mu_range, mu_range),
            trainable=True,
        )
        self.sigma_w = self.add_weight(
            name="sigma_w",
            shape=(in_dim, self.units),
            initializer=tf.keras.initializers.Constant(self.sigma0 / np.sqrt(in_dim)),
            trainable=True,
        )
        self.mu_b = self.add_weight(
            name="mu_b",
            shape=(self.units,),
            initializer=tf.keras.initializers.RandomUniform(-mu_range, mu_range),
            trainable=True,
        )
        self.sigma_b = self.add_weight(
            name="sigma_b",
            shape=(self.units,),
            initializer=tf.keras.initializers.Constant(self.sigma0 / np.sqrt(in_dim)),
            trainable=True,
        )
        super(NoisyDense, self).build(input_shape)

    def call(self, inputs):
        def f(x):
            return tf.sign(x) * tf.sqrt(tf.abs(x))

        in_dim = tf.shape(self.mu_w)[0]
        eps_in = f(tf.random.normal((in_dim,)))
        eps_out = f(tf.random.normal((self.units,)))

        eps_w = tf.tensordot(eps_in, eps_out, axes=0)  # (in_dim, units)
        eps_b = eps_out

        w = self.mu_w + self.sigma_w * eps_w
        b = self.mu_b + self.sigma_b * eps_b

        out = tf.matmul(inputs, w) + b
        if self.activation is not None:
            out = self.activation(out)
        return out


# ============================================================
# 8. DQN_Agent (dueling + double, noisy support, dist optional)
# ============================================================

class DQN_Agent:
    def __init__(self, state_size, no_of_actions, agent_hyperparameters=None, old_model_path=''):
        if agent_hyperparameters is None:
            agent_hyperparameters = {}
        self.state_size = int(state_size)
        self.action_size = int(no_of_actions)

        # base hparams
        self.gamma = agent_hyperparameters.get('gamma', 0.99)
        self.epsilon = agent_hyperparameters.get('epsilon', 1.0)
        self.batch_size = agent_hyperparameters.get('batch_size', 32)
        self.epsilon_min = agent_hyperparameters.get('epsilon_min', 0.05)
        self.epsilon_decay = agent_hyperparameters.get('epsilon_decay', 0.995)
        self.units = agent_hyperparameters.get('units', 64)

        self.use_per = agent_hyperparameters.get('use_per', True)

        per_alpha = agent_hyperparameters.get('per_alpha', 0.6)
        per_beta0 = agent_hyperparameters.get('per_beta0', 0.4)
        per_beta1 = agent_hyperparameters.get('per_beta1', 1.0)
        per_eps = agent_hyperparameters.get('per_eps', 1e-6)
        per_anneal = agent_hyperparameters.get('per_beta_anneal_steps', 200_000)

        # Rainbow bits
        self.n_step = int(agent_hyperparameters.get('n_step', 1))
        self.use_noisy = agent_hyperparameters.get('use_noisy', False)
        self.noisy_sigma0 = agent_hyperparameters.get('noisy_sigma0', 0.5)

        self.use_distributional = agent_hyperparameters.get('use_distributional', False)
        self.num_atoms = int(agent_hyperparameters.get('num_atoms', 51))
        self.vmin = float(agent_hyperparameters.get('vmin', -200.0))
        self.vmax = float(agent_hyperparameters.get('vmax', 200.0))
        if self.use_distributional:
            self.z = np.linspace(self.vmin, self.vmax, self.num_atoms).astype(np.float32)
        else:
            self.z = None

        self.gamma_n = self.gamma ** self.n_step

        # slower soft target update
        self.tau = agent_hyperparameters.get('tau', 0.001)

        # networks
        if self.use_distributional:
            self.model = self._build_distributional_model(self.state_size, self.action_size, self.units, old_model_path)
            self.target_model = self._build_distributional_model(self.state_size, self.action_size, self.units)
        else:
            self.model = self._build_model(self.state_size, self.action_size, self.units, old_model_path)
            self.target_model = self._build_model(self.state_size, self.action_size, self.units)

        self.target_model.set_weights(self.model.get_weights())

        # replay buffer
        if self.use_per:
            self.memory = PrioritizedReplayBuffer(
                state_dim=self.state_size,
                capacity=50_000,
                alpha=per_alpha,
                beta0=per_beta0,
                beta_final=per_beta1,
                beta_anneal_steps=per_anneal,
                eps=per_eps
            )
        else:
            self.memory = UniformReplayBuffer(state_dim=self.state_size, capacity=50_000)

        self.learn_start = max(5000, 5 * self.batch_size)
        self.total_updates = 0

        # n-step buffer
        self._n_step_buffer = deque(maxlen=self.n_step)

    def _dense_layer(self, units, activation=None):
        if self.use_noisy:
            return NoisyDense(units, activation=activation, sigma0=self.noisy_sigma0)
        else:
            return Dense(units, activation=activation)

    def _build_model(self, state_size, action_size, units, old_model_path=''):
        DenseLayer = self._dense_layer

        inp = Input(shape=(state_size,))
        x = Reshape((HEIGHT, WIDTH, CHANNELS))(inp)

        x = Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
        x = Conv2D(64, kernel_size=3, padding='same', activation='relu')(x)
        x = Flatten()(x)
        x = DenseLayer(units, activation='relu')(x)

        # Dueling heads
        adv = DenseLayer(units, activation='relu')(x)
        adv = DenseLayer(action_size)(adv)

        val = DenseLayer(units, activation='relu')(x)
        val = DenseLayer(1)(val)

        def combine_streams(tensors):
            v, a = tensors
            a_mean = tf.reduce_mean(a, axis=1, keepdims=True)
            return v + (a - a_mean)

        q = Lambda(combine_streams, name="dueling_combine")([val, adv])

        model = Model(inputs=inp, outputs=q)
        model.compile(
            optimizer=Adam(learning_rate=3e-4, clipnorm=10.0),
            loss=Huber(delta=1.0),
            run_eagerly=False
        )
        if old_model_path:
            model.load_weights(old_model_path)
        return model

    def _build_distributional_model(self, state_size, action_size, units, old_model_path=''):
        DenseLayer = self._dense_layer

        inp = Input(shape=(state_size,))
        x = Reshape((HEIGHT, WIDTH, CHANNELS))(inp)

        x = Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)
        x = Conv2D(64, kernel_size=3, padding='same', activation='relu')(x)
        x = Flatten()(x)
        x = DenseLayer(units, activation='relu')(x)

        # Value stream -> num_atoms
        v = DenseLayer(units, activation='relu')(x)
        v = DenseLayer(self.num_atoms)(v)

        # Advantage stream -> action_size * num_atoms
        a = DenseLayer(units, activation='relu')(x)
        a = DenseLayer(action_size * self.num_atoms)(a)

        def dist_dueling(tensors):
            v_, a_ = tensors
            batch = tf.shape(v_)[0]
            v_ = tf.reshape(v_, (batch, 1, self.num_atoms))
            a_ = tf.reshape(a_, (batch, action_size, self.num_atoms))
            a_mean = tf.reduce_mean(a_, axis=1, keepdims=True)
            q_logits = v_ + (a_ - a_mean)
            return tf.nn.softmax(q_logits, axis=-1)

        out = Lambda(dist_dueling, name="dist_dueling")([v, a])

        model = Model(inputs=inp, outputs=out)
        model.compile(
            optimizer=Adam(learning_rate=3e-4, clipnorm=10.0),
            loss='categorical_crossentropy',
            run_eagerly=False
        )
        if old_model_path:
            model.load_weights(old_model_path)
        return model

    def select_action(self, state, valid_actions=None, training=True):
        if valid_actions is None:
            valid_actions = np.arange(self.action_size, dtype=np.int32)

        if training and np.random.rand() < self.epsilon:
            return int(np.random.choice(valid_actions))

        if self.use_distributional:
            dist = self.model.predict(state, verbose=0)[0]  # (A, num_atoms)
            q = np.sum(self.z * dist, axis=1)
        else:
            q = self.model.predict(state, verbose=0)[0]

        masked_q = np.full_like(q, -1e9, dtype=np.float32)
        masked_q[valid_actions] = q[valid_actions]
        return int(np.argmax(masked_q))

    def _add_single_transition(self, state, action, reward, next_state, done):
        self.memory.add(state.reshape(-1), int(action), float(reward),
                        next_state.reshape(-1), float(done))

    def record(self, state, action, reward, next_state, done):
        if self.n_step <= 1:
            self._add_single_transition(state, action, reward, next_state, done)
            return

        self._n_step_buffer.append((state, action, reward, next_state, done))

        if len(self._n_step_buffer) == self.n_step and not done:
            self._flush_n_step()
            self._n_step_buffer.popleft()

        if done:
            while len(self._n_step_buffer) > 0:
                self._flush_n_step()
                self._n_step_buffer.popleft()

    def _flush_n_step(self):
        R = 0.0
        gamma = 1.0
        for (s, a, r, ns, d) in self._n_step_buffer:
            R += gamma * float(r)
            gamma *= self.gamma
            if d:
                break

        state_0, action_0, _, _, _ = self._n_step_buffer[0]
        _, _, _, next_state_last, done_last = self._n_step_buffer[-1]

        self._add_single_transition(state_0, action_0, R, next_state_last, done_last)

    def update_epsilon(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def _soft_update_target(self):
        w = self.model.get_weights()
        tw = self.target_model.get_weights()
        self.target_model.set_weights([
            self.tau * wi + (1.0 - self.tau) * twi
            for wi, twi in zip(w, tw)
        ])

    def _valid_action_mask_from_states(self, states):
        batch = states.shape[0]
        grid = states.reshape(batch, HEIGHT, WIDTH, CHANNELS)

        mask = np.zeros((batch, self.action_size), dtype=bool)
        mask[:, 0] = True

        agent_layer = grid[:, :, :, 0]
        eligible_layer = grid[:, :, :, 6] > 0.5

        for b in range(batch):
            pos = np.argwhere(agent_layer[b] > 0.5)
            if pos.size == 0:
                continue
            ar, ac = pos[0]
            H, W = HEIGHT, WIDTH
            moves = [
                (1, -1, 0),
                (2, 0, 1),
                (3, 1, 0),
                (4, 0, -1),
            ]
            for act, dr, dc in moves:
                nr, nc = ar + dr, ac + dc
                if 0 <= nr < H and 0 <= nc < W and eligible_layer[b, nr, nc]:
                    mask[b, act] = True

        return mask

    def update_weights(self):
        if len(self.memory) < self.learn_start:
            return

        S, A, R, NS, D, idxs, is_w = self.memory.sample(self.batch_size)
        valid_next_mask = self._valid_action_mask_from_states(NS)
        batch_idx = np.arange(self.batch_size, dtype=np.int32)

        if not self.use_distributional:
            q_next_online = self.model.predict(NS, verbose=0)
            q_next_online[~valid_next_mask] = -1e9
            best_next_actions = np.argmax(q_next_online, axis=1)

            q_next_target = self.target_model.predict(NS, verbose=0)
            q_next_target[~valid_next_mask] = -1e9
            best_next_q = q_next_target[np.arange(self.batch_size), best_next_actions]

            Tgt = R + self.gamma_n * best_next_q * (1.0 - D)

            q_curr = self.model.predict(S, verbose=0)
            q_chosen = q_curr[np.arange(self.batch_size), A]
            td_errors = Tgt - q_chosen

            q_curr[np.arange(self.batch_size), A] = Tgt

            self.model.train_on_batch(S, q_curr)

            self.memory.update_priorities(idxs, np.abs(td_errors))

        else:
            dist_next_online = self.model.predict(NS, verbose=0)
            dist_next_target = self.target_model.predict(NS, verbose=0)

            q_next_online = np.sum(self.z[None, None, :] * dist_next_online, axis=2)
            q_next_online[~valid_next_mask] = -1e9
            best_next_actions = np.argmax(q_next_online, axis=1)

            next_dist = dist_next_target[batch_idx, best_next_actions, :]

            Tz = R[:, None] + (self.gamma_n * (1.0 - D))[:, None] * self.z[None, :]
            Tz = np.clip(Tz, self.vmin, self.vmax)

            b = (Tz - self.vmin) / (self.vmax - self.vmin) * (self.num_atoms - 1)
            l = np.floor(b).astype(np.int64)
            u = np.ceil(b).astype(np.int64)

            l = np.clip(l, 0, self.num_atoms - 1)
            u = np.clip(u, 0, self.num_atoms - 1)

            m = np.zeros((self.batch_size, self.num_atoms), dtype=np.float32)

            for i in range(self.batch_size):
                for j in range(self.num_atoms):
                    lj, uj, bj = l[i, j], u[i, j], b[i, j]
                    p = next_dist[i, j]
                    if lj == uj:
                        m[i, lj] += p
                    else:
                        m[i, lj] += p * (uj - bj)
                        m[i, uj] += p * (bj - lj)

            dist_curr = self.model.predict(S, verbose=0)
            dist_a = dist_curr[batch_idx, A, :]

            eps = 1e-8
            m_safe = np.clip(m, eps, 1.0)
            dist_a_safe = np.clip(dist_a, eps, 1.0)
            kl = np.sum(m_safe * (np.log(m_safe) - np.log(dist_a_safe)), axis=1)

            y = dist_curr.copy()
            y[batch_idx, A, :] = m

            self.model.train_on_batch(S, y)

            self.memory.update_priorities(idxs, kl)

        self._soft_update_target()
        self.total_updates += 1


# ============================================================
# 9. Evaluation
# ============================================================

def evaluate_average_reward(agent, variant, data_dir, encoder_fn,
                            episodes=100, mode="testing"):
    env = Environment(variant, data_dir)
    max_steps = min(getattr(env, "episode_steps", 200), 200)

    if mode == "testing":
        max_eps = min(episodes, len(env.test_episodes))
    elif mode == "validation":
        max_eps = min(episodes, len(env.validation_episodes))
    else:
        max_eps = episodes

    total = 0.0
    for _ in range(max_eps):
        obs = env.reset(mode)
        s_vec = encoder_fn(obs, env).reshape(1, -1)
        done = 0
        ep_ret = 0.0
        steps = 0

        while not done and steps < max_steps:
            valid_actions = compute_valid_actions(env)
            a = agent.select_action(s_vec, valid_actions=valid_actions, training=False)
            r, nxt, done = env.step(a)
            s_vec = encoder_fn(nxt, env).reshape(1, -1)
            ep_ret += r
            steps += 1

        total += ep_ret

    return total / float(max_eps)


# ============================================================
# 10. Encoder-Vergleich + Main
# ============================================================

if __name__ == "__main__":
    VARIANT = 0
    NO_OF_ACTIONS = 5

    # Kurze Runs für Encoder-Vergleich
    TRAIN_EPISODES_SHORT = 300
    VAL_EPISODES_SHORT = 100

    # Finaler Run mit dem besten Encoder
    TRAIN_EPISODES_FULL = 800
    VAL_EPISODES_FINAL = 100
    TEST_EPISODES_FINAL = 100

    # Greedy-Benchmark
    GREEDY_BENCHMARK_V0 = 221.77

    # ---- Basis-Hyperparameter (wie vorher) ----
    BASE_CONFIG = dict(
        gamma=0.9389065584049,
        epsilon=1.0,
        batch_size=32,
        epsilon_min=0.079088807071618797,
        epsilon_decay=0.99,
        units=128,
        tau=0.0010323839507959762,
        use_per=True,
        per_alpha=0.4,
        per_beta0=0.2,
        per_beta1=1.0,
        per_eps=1e-6,
        per_beta_anneal_steps=200_000,
        n_step=1,
        use_noisy=True,
        noisy_sigma0=0.12546649741233012,
        use_distributional=False,
        num_atoms=51,
        vmin=-200.0,
        vmax=200.0,
    )

    # Alle Encoder-Varianten, die du testen willst
    ENCODER_CANDIDATES = {
        "spatial_only":           encode_obs_spatial_only,
        "spatial_temporal":       encode_obs_spatial_temporal,
        "spatial_step":           encode_obs_spatial_step,
        "spatial_load":           encode_obs_spatial_load,
        "globals_only":           encode_obs_globals_only,
        "all_channels":           encode_obs_all_channels,
        "first9_plus_11_12_13":   encode_obs_first9_plus_11_12_13,
        "first9_without_agent":   encode_obs_first9_without_agent,
        "first9_without_items":   encode_obs_first9_without_items,
        "items_and_globals":      encode_obs_items_and_globals,
    }

    # ---- State-Size bestimmen (vom ersten Encoder) ----
    env_tmp = Environment(VARIANT, DATA_DIR)
    first_name, first_fn = next(iter(ENCODER_CANDIDATES.items()))
    obs0 = env_tmp.reset("training")
    state_sample = first_fn(obs0, env_tmp)
    state_size = state_sample.shape[0]
    print(f"State size (vom Encoder '{first_name}'): {state_size}")

    # Speicherordner
    save_folder = os.path.join(os.path.expanduser("~"), "save_feature_variants")
    os.makedirs(save_folder, exist_ok=True)

    # Ergebnisse pro Encoder
    encoder_results = {}

    # =====================================================
    # 1. Alle Encoder-Varianten: 300 Train + 100 Val
    # =====================================================
    for enc_name, enc_fn in ENCODER_CANDIDATES.items():
        print("\n" + "=" * 60)
        print(f"Training Encoder-Variante: {enc_name}")
        print("=" * 60)

        # Seeds pro Encoder resetten für mehr Vergleichbarkeit
        random.seed(seed)
        np.random.seed(seed)
        tf.random.set_seed(seed)

        # Agent + Environment
        agent = DQN_Agent(state_size, NO_OF_ACTIONS, BASE_CONFIG)

        env_train = GymEnvironment(
            env_id=f'env_{enc_name}',
            save_path=save_folder,
            encoder_fn=enc_fn,
            render=False,
            variant=VARIANT,
            data_dir=DATA_DIR,
            pretty_render=False,
            fps=8,
            render_every=2
        )

        # Training (300 Episoden)
        rew_train = env_train.trainDQN(agent, TRAIN_EPISODES_SHORT)
        if len(rew_train) >= 25:
            avg_train_last25 = float(np.mean(rew_train[-25:]))
        else:
            avg_train_last25 = float(np.mean(rew_train))
        print(f"[{enc_name}] Train avg letzte 25: {avg_train_last25:.2f}")

        # Validation (100 Episoden)
        avg_val = evaluate_average_reward(
            agent, VARIANT, DATA_DIR,
            encoder_fn=enc_fn,
            episodes=VAL_EPISODES_SHORT,
            mode="validation"
        )
        print(f"[{enc_name}] Validation avg über {VAL_EPISODES_SHORT} Episoden: {avg_val:.2f}")

        # Speichern
        encoder_results[enc_name] = {
            "encoder_fn": enc_fn,
            "avg_val": avg_val,
            "rew_train": rew_train,
            "avg_train_last25": avg_train_last25,
        }

    # =====================================================
    # 2. Besten Encoder anhand der Validation bestimmen
    # =====================================================
    best_name = max(encoder_results.keys(), key=lambda n: encoder_results[n]["avg_val"])
    best_info = encoder_results[best_name]
    best_encoder_fn = best_info["encoder_fn"]

    print("\n" + "=" * 60)
    print(f"BESTER ENCODER: {best_name} (Val-Avg = {best_info['avg_val']:.3f})")
    print("=" * 60)

    # =====================================================
    # 3. Finales Training mit bestem Encoder
    # =====================================================

    # Seeds nochmal resetten
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

    agent_best = DQN_Agent(state_size, NO_OF_ACTIONS, BASE_CONFIG)

    env_train_final = GymEnvironment(
        env_id=f'env_best_{best_name}',
        save_path=save_folder,
        encoder_fn=best_encoder_fn,
        render=False,
        variant=VARIANT,
        data_dir=DATA_DIR,
        pretty_render=False,
        fps=8,
        render_every=2
    )

    # 800 Trainingsepisoden
    rew_train_full = env_train_final.trainDQN(agent_best, TRAIN_EPISODES_FULL)

    # Validation + Test mit je 100 Episoden
    avg_val_final = evaluate_average_reward(
        agent_best, VARIANT, DATA_DIR,
        encoder_fn=best_encoder_fn,
        episodes=VAL_EPISODES_FINAL,
        mode="validation"
    )
    avg_test_final = evaluate_average_reward(
        agent_best, VARIANT, DATA_DIR,
        encoder_fn=best_encoder_fn,
        episodes=TEST_EPISODES_FINAL,
        mode="testing"
    )

    print(f"\n[FINAL] Bester Encoder: {best_name}")
    print(f"[FINAL] Validation avg über {VAL_EPISODES_FINAL} Episoden: {avg_val_final:.3f}")
    print(f"[FINAL] Test       avg über {TEST_EPISODES_FINAL} Episoden: {avg_test_final:.3f}")
    print(f"[FINAL] Greedy benchmark: {GREEDY_BENCHMARK_V0:.2f}")

    # =====================================================
    # 4. Plot der Performance des besten Encoders
    # =====================================================
    plt.figure(figsize=(10, 5))
    x = np.arange(1, len(rew_train_full) + 1)
    plt.plot(x, rew_train_full, alpha=0.25, label="Train (raw)")

    window = 25
    if len(rew_train_full) >= window:
        kernel = np.ones(window, dtype=np.float32) / float(window)
        mov = np.convolve(rew_train_full, kernel, mode="valid")
        xm = np.arange(window, len(rew_train_full) + 1)
        plt.plot(xm, mov, label=f"Train (25-ep avg)")

    plt.axhline(
        y=avg_val_final,
        linestyle=":",
        color="green",
        alpha=0.9,
        label=f"Val avg ({avg_val_final:.1f})"
    )
    plt.axhline(
        y=avg_test_final,
        linestyle="--",
        color="red",
        alpha=0.9,
        label=f"Test avg ({avg_test_final:.1f})"
    )
    plt.axhline(
        y=GREEDY_BENCHMARK_V0,
        linestyle="-.",
        color="black",
        alpha=0.8,
        label=f"Greedy benchmark ({GREEDY_BENCHMARK_V0:.2f})"
    )

    plt.title(f"Variant 0 – Bester Encoder: {best_name}")
    plt.xlabel("Trainingsepisode")
    plt.ylabel("Reward")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()

    out_plot = os.path.join(save_folder, f"variant0_best_encoder_{best_name}.png")
    plt.savefig(out_plot, dpi=150)
    print(f"\nSaved plot to: {out_plot}")
    # plt.show()  # optional
